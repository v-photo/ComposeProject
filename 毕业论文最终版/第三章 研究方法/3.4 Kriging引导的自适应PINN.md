# 3.4 Kriging引导的自适应PINN

针对物理信息神经网络（PINN）在处理复杂辐射场时存在的“采样盲目性”与“局部收敛困难”问题，本研究提出了一种基于 Kriging 残差引导的自适应进化机制。该机制巧妙地结合了统计插值算法的计算优势与深度学习的物理表征能力，通过“残差评估-模型更新-自适应采样”的闭环反馈，实现了对高误差区域的自动锁定与精准修复。

### 3.4.1 物理模型构建与损失函数

作为自适应机制的基础，本节首先明确物理信息神经网络所需满足的控制方程及其损失函数的构建方式。

**1. 原始控制方程**
在各向同性的均匀介质中，辐射场强度的空间分布通常遵循亥姆霍兹方程（Helmholtz Equation）或其扩散近似形式：
$$ (\nabla^2 - \mu^2) \phi(\mathbf{x}) = -S(\mathbf{x}) $$
其中，$\phi(\mathbf{x})$ 为空间位置 $\mathbf{x}=(x,y,z)$ 处的辐射通量（或剂量率），$\mu$ 为介质的线性衰减系数，$S(\mathbf{x})$ 为源项分布。

**2. 对数变换与数值稳定性**
由于辐射剂量率在空间上往往跨越多个数量级（从源项附近的 $10^5$ 量级衰减至远处的本底水平），直接求解原始方程 $\phi$ 容易导致神经网络训练时的数值不稳定（梯度爆炸或消失）。为此，本研究引入对数变换 $u(\mathbf{x}) = \ln(\phi(\mathbf{x}))$。

将变换代入原始方程，可推导出关于对数剂量 $u$ 的非线性偏微分方程：
$$ \nabla^2 u + |\nabla u|^2 - \mu^2 = -S e^{-u} $$
在无源区域（$S=0$），该方程简化为 Eikonal 方程的一种变形：
$$ \mathcal{F}(u) \equiv \nabla^2 u + |\nabla u|^2 - \mu^2 = 0 $$
这就是本框架中 PINN 所需满足的核心物理约束 $\mathcal{F}$。

**3. 边界条件与初值**
由于辐射场重建通常属于稳态反问题，无需考虑时间维度的初值条件。对于空间边界条件，本研究采用 **Dirichlet 边界条件** 的软约束形式。即利用稀疏的现场观测数据 $\{(\mathbf{x}_i, \phi_i)\}_{i=1}^{N_{obs}}$ 作为已知点，要求网络在这些坐标位置的输出逼近观测值。这种处理方式避免了对复杂几何边界（如房间墙壁）的显式建模，使算法能够适应开放或半开放的复杂环境。

**4. 损失函数构建**
网络的总损失函数 $\mathcal{L}$ 由两部分组成：
$$ \mathcal{L} = \mathcal{L}_{data} + \lambda \cdot \mathcal{L}_{physics} $$
*   **数据损失 $\mathcal{L}_{data}$**：衡量网络预测值与稀疏观测点之间的均方误差（MSE）。
    $$ \mathcal{L}_{data} = \frac{1}{N_{obs}} \sum_{i=1}^{N_{obs}} |u_{pred}(\mathbf{x}_i) - u_{true}(\mathbf{x}_i)|^2 $$
*   **物理损失 $\mathcal{L}_{physics}$**：衡量网络输出在全域内对控制方程 $\mathcal{F}(u)=0$ 的违反程度。
    $$ \mathcal{L}_{physics} = \frac{1}{N_{col}} \sum_{j=1}^{N_{col}} |\mathcal{F}(u_{pred}(\mathbf{x}_j))|^2 $$
其中，$\lambda$ 为平衡系数，用于调节物理约束的强弱。

### 3.4.2 核心思想：残差代理与自适应采样

传统的 PINN 训练通常采用固定的随机采样点（Collocation Points）来计算偏微分方程（PDE）的残差损失。然而，在三维非均匀介质或多源辐射场中，误差往往集中在源项附近、几何边界处或梯度剧烈变化的区域。均匀随机采样难以捕捉这些局部特征，导致网络在这些关键区域**采样覆盖不足**，训练效率低下。

本研究设计的**“残差代理” (Residual Surrogate)** 机制，是将训练中的 PINN 视为一个待评估的物理模型，利用 Kriging 算法作为**“残差预测器”**来构建全域的误差分布模型。

需要特别说明的是，此处的**“残差”**特指**PDE残差**（PDE Residual），即物理方程的代数违背度，而非与真实观测值的误差。根据前文定义的控制方程 $\mathcal{F}(u)=0$，对于计算域内任意一点 $\mathbf{x}$，无论该点是否存在真实观测数据，均可利用当前神经网络的输出 $u_{pred}(\mathbf{x})$ 及其通过自动微分计算的导数项，直接计算出该点对控制方程的违反程度 $|\mathcal{F}(u_{pred}(\mathbf{x}))|$。这使得我们能够在没有密集观测数据的区域，依然能够依据物理方程的约束来评估模型的局部准确性。

*   **残差评估 (Residual Assessment)**：在全域随机撒布大量“侦察点”（Scout Points），利用当前 PINN 网络计算每个点的 PDE 残差绝对值。
*   **代理 (Surrogate)**：利用 Kriging 模块对这些离散的 PDE 残差值进行空间插值，构建出一个连续的“残差场”。
*   **引导 (Guidance)**：基于该残差场生成新的训练配点（Collocation Points），使配点密度与预测的残差大小成正比，从而在下一轮训练中加强对高物理违背区域的约束。

这种方法避免了在全域进行高密度的 PDE 计算（极其耗时），而是用极低的计算成本（Kriging 插值）获得了全域误差的近似分布，从而指导下一轮训练的“注意力”分配。

### 3.4.3 循环进化机制

为了实现模型性能的持续提升，本研究将训练过程构建为一个**多周期循环进化 (Multi-Cycle Evolutionary)** 系统。每个周期包含以下关键步骤：

1.  **物理约束训练**：运行标准的 PINN 训练流程，优化网络参数以最小化当前的物理损失和数据损失。
2.  **全域残差评估**：周期结束时，系统冻结网络参数，在计算域内生成高密度的测试点阵列，计算控制方程残差 $|f(x)|$。
3.  **残差场建模与重采样**：调用 Kriging 模块构建残差分布模型 $R(x)$。依据 $R(x)$ 进行重要性采样，生成下一周期的配点集合 $S_{new}$。
4.  **物理信息注入（可选）**：在特定周期，从储备数据池中抽取新的观测数据注入训练集。这不仅是样本数量的增加，更是对物理场真实信息的补充，有助于打破模型可能陷入的局部极值。

### 3.4.4 关键优化策略

为了保证自适应过程的收敛性与稳定性，本框架引入了三项关键策略。每项策略均针对特定的优化难题，并具备明确的触发条件与执行逻辑。

1.  **探索率衰减 (Exploration Decay)**
    *   **策略目的**：解决自适应采样中“全局探索”与“局部精修”的平衡问题。若始终全盘信赖残差引导，可能导致模型过度关注已知的高误差区而忽略潜在的其他问题区域（过拟合局部）；若始终随机采样，则无法发挥自适应优势。
    *   **实施方式**：引入探索率参数 $\epsilon$，控制采样点中均匀随机分布的比例。该参数随训练周期 $k$ 逐步降低，计算公式为：
        $$ \epsilon_k = \max(\epsilon_{min}, \epsilon_{initial} - k \cdot \Delta\epsilon) $$
    *   **参数设定**：典型配置为初始探索率 $\epsilon_{initial}=0.2$，最终探索率 $\epsilon_{min}=0.05$，每周期衰减步长 $\Delta\epsilon=0.02$。这意味着训练初期保留 20% 的随机点进行广域搜索，随着模型趋于成熟，逐步降至 5%，将 95% 的计算资源集中用于修复 Kriging 锁定的高残差区域。
    *   **预期效果**：确保模型在训练初期具备足够的全局视野，避免陷入局部极值；在训练后期能够高效收敛，实现精细化重建。

2.  **快速改善早停 (Rapid Improvement Early Stopping)**
    *   **策略目的**：避免在模型已经获得显著突破时继续进行低效的微调计算，从而节省计算资源并加快迭代节奏。
    *   **实施方式**：在每个训练周期内，系统以固定间隔（如每 100 Epochs）计算当前模型在验证集上的平均相对误差（MRE）。计算当前误差相对于周期起始误差的改善比例 $I_{imp}$：
        $$ I_{imp} = \frac{\text{MRE}_{start} - \text{MRE}_{current}}{\text{MRE}_{start}} $$
    *   **触发条件**：若 $I_{imp}$ 超过预设阈值 $T_{stop}$（例如 10%），则判定本周期的优化潜力已主要释放。
    *   **执行动作**：立即终止当前周期的剩余训练轮数，保存当前模型参数，并直接触发下一轮的“残差评估-重采样”流程。
    *   **风险控制**：为防止因误差波动导致的误判，要求改善必须是连续两次检测均满足条件才触发。

3.  **停滞回退保护 (Stagnation Rollback)**
    *   **策略目的**：防止因自适应采样过于激进或数据注入带来的扰动，导致模型性能不升反降（即发散或震荡）。这是自适应算法在复杂非凸优化中常见的风险。
    *   **实施方式**：系统在每个周期开始时自动保存一份模型参数快照（Checkpoint）。在周期内的每次检测点，若发现当前 MRE 优于历史最佳，则更新快照。
    *   **触发条件**：在周期结束时，对比最终模型的 MRE 与周期内的最佳 MRE。若最终 MRE 显著高于最佳值（例如恶化超过 5%），则判定训练陷入停滞或震荡。
    *   **执行动作**：
        1.  丢弃当前周期的最终参数，**强制回滚**至该周期内的最佳快照状态。
        2.  在日志中记录“Rollback”事件。
        3.  保持当前采样分布不变进入下一周期，或在极端情况下重置为均匀采样（取决于连续回退次数）。
    *   **预期效果**：为自适应进化提供一道“安全阀”，确保模型的演进方向总体上是单调向好的，即使某个周期的尝试失败，也不会破坏已有的训练成果。
