## 4.2 GPU加速Kriging算法性能评估

为了独立且深入地评估本研究中实现的高性能Kriging模块的性能，我们基于4.1节构建的两个模拟场景，对其重建精度、计算效率以及加速比的变化规律进行了详细测试。

### 4.2.1 实验平台与设置

- **硬件平台**：所有实验均在同一平台上进行，CPU为 `Intel Core i5-12490F`，GPU为 `NVIDIA GeForce RTX 2080Ti`（11GB显存），系统内存为32GB。
- **Kriging模型**：算法采用指数半变异函数模型。
- **评估场景**：分别在“单源无屏蔽”（对应均匀采样）和“多源带屏蔽”（对应聚集采样）两个场景下进行测试。
- **核心评估指标**：
    1.  **平均相对误差（MRE）**：评估重建精度。
    2.  **计算耗时（秒）**：评估绝对计算效率。
    3.  **加速比（Speedup）**：定义为 `CPU耗时 / GPU耗时`，评估并行化带来的相对性能提升。

### 4.2.2 精度验证：GPU与CPU计算结果的一致性

首先，关键在于验证GPU并行计算是否会引入精度损失。我们在两个场景下，使用完全相同的输入数据（已知点和待预测点），分别通过传统CPU实现的Kriging算法和本研究的GPU加速Kriging算法进行计算。

**表4-2-1：GPU与CPU实现的Kriging算法精度对比**

| 数据分布 | 采样方式 | CPU计算MRE | GPU计算MRE | 精度差异 | 结论 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 场景一 | 均匀随机采样 | 7.6765% | 7.6760% | <0.01% | 几乎无差异 |
| 场景二 | 聚集性采样 | 14.0551% | 14.0556% | <0.01% | 几乎无差异 |

如 `表4-2-1` 所示（详细数据见附录），在所有测试工况中，GPU与CPU计算结果的差异极小（误差小于0.01%），完全在数值计算的浮点误差范围内。**这充分证明了本研究的GPU加速实现是精确无损的，它在提升速度的同时，完美地保持了原始Kriging算法的插值精度。**

### 4.2.3 性能分析

#### 4.2.3.1 数据分布对Kriging性能的影响

如下表所示，Kriging算法的性能表现出对数据分布的强烈依赖性。

**表4-2-2：GPU-Kriging在不同数据分布下的性能表现**

| 数据分布 | 已知点数 | 预测点数 | 平均相对误差 (MRE) | GPU计算耗时 (秒) | CPU计算耗时 (秒) | 加速比 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 场景一：单源无屏蔽 | 343 | 32,768 | 7.68% | 0.07 | 1.61 | 23.0× |
| 场景一：单源无屏蔽 | 2,744 | 110,592 | 3.36% | 0.58 | 20.84 | 35.7× |
| 场景二：多源带屏蔽 | 1,000 | 32,768 | 14.06% | 0.17 | 5.71 | 33.6× |
| 场景二：多源带屏蔽 | 8,000 | 110,592 | 9.83% | 1.32 | 82.5 | 62.5× |

分析如下：
1.  **场景一（均匀数据）**：Kriging算法展现了出色的性能，取得了较低的MRE和极短的计算耗时。这验证了其处理空间相关性良好、信息覆盖全面的数据时的有效性和高效率。
2.  **场景二（聚集数据）**：当数据分布变得复杂且非均匀时，Kriging算法的精度急剧下降，MRE显著增大。这是因为在广阔的未采样区域，模型失去了有效的插值依据，暴露了纯数据驱动方法在面临数据严重“偏科”时的固有局限性。

#### 4.2.3.2 加速比与计算规模的关系

为了探究GPU加速的潜力，我们固定其中一个变量，观察加速比与另一个变量的关系。
- **与已知点数的关系**：当我们将待预测点数量固定（例如11万个点），并逐渐增加已知点（训练点）的数量时，我们观察到CPU和GPU的计算耗时均增加，但CPU耗时的增长率远高于GPU。**随着已知点数从数百增加到数千，加速比也从约20倍显著提升至超过60倍。**
- **与预测点数的关系**：当我们将已知点数量固定（例如512个点），并逐渐增加待预测点的数量时，也观察到了类似的趋势。**随着待预测点数从十万级增加到千万级，加速比可以从约10倍提升至超过90倍。**

这表明，本研究的GPU加速策略尤其擅长处理大规模计算任务，计算规模越大，其并行计算的优势越明显，加速效果越好。

#### 4.2.3.3 数据分块大小（Block Size）对性能的影响

本研究实现的一个关键优化是**数据分块策略**，以解决大规模数据可能超出单次显存容量的问题。实验发现，分块大小是影响计算效率的一个重要超参数。
- **过小的分块**：会导致CPU与GPU之间过于频繁的数据传输，此时的性能瓶颈在于数据拷贝的开销，而非计算本身。
- **过大的分块**：虽然减少了传输次数，但单次计算任务过重，可能导致GPU线程等待时间变长，或因超出显存而失败。

实验结果表明，存在一个**最优的分块大小区间（在本次实验平台和数据规模下，约为6万至9万之间）**，能够最好地平衡数据传输开销和并行计算效率，从而达到最短的计算耗时。

### 4.2.4 本节结论

综上所述，本研究实现的GPU加速Kriging模块具备以下特性：
1.  **精度无损**：与传统CPU实现相比，精度完全一致。
2.  **性能卓越**：在处理大规模数据时，可获得高达数十倍甚至近百倍的加速比。
3.  **场景依赖性**：它是一个“特长生”，在处理均匀分布数据时表现高效且准确，但在处理复杂、非均匀数据时则精度严重下降。这一特性也正是构建自适应混合框架的必要前提。

