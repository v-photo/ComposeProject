# 2.1 相关理论框架

本章旨在系统性地阐述支撑“双层自适应PINN-Kriging混合计算框架”的三大核心理论与技术。我们将依次介绍作为经典空间插值基准的克里金（Kriging）方法、作为科学机器学习前沿的物理信息神经网络（PINN），以及作为高性能计算实现手段的GPU并行计算技术。

### 2.1.1 克里金（Kriging）插值理论

克里金法，又称空间局部插值法，是源于地质统计学的一种最优内插方法。与反距离加权等确定性插值方法不同，克里金法是一种基于随机过程理论的估计方法，其核心优势在于它不仅能给出待估点位的最优线性无偏估计（Best Linear Unbiased Estimator, BLUE），还能提供估计的方差，即对预测结果不确定性的量化。

#### 2.1.1.1 核心思想与变异函数

克里金方法的基本假设是，待插值的空间属性（如辐射剂量率、温度等）可以被视为一个随机场，并且场内任意两点之间的依赖关系取决于它们之间的距离和方向，而非它们的绝对位置。这种空间自相关性（Spatial Autocorrelation）是克里金方法得以成立的基石。

为了定量描述这种空间自相关性，克里金方法引入了**半变异函数**（Semivariogram），通常简称为**变异函数**。它被定义为随机场中相距为 **h** 的两点 `z(x)` 和 `z(x+h)` 之差的方差的一半：

\[\gamma(h) = \frac{1}{2} E[(Z(x) - Z(x+h))^2]\]

通过计算样本数据点对的距离与值的差异，可以得到实验变异函数。随后，通过拟合一个理论模型（如球状模型、指数模型或高斯模型）到实验变异函数上，可以得到三个关键参数：

*   **基台值（Sill）**：当距离 h 趋于无穷时，变异函数达到的平台值，等于区域化变量的方差。
*   **变程（Range）**：变异函数达到基台值时的距离。当两点间距小于变程时，它们存在空间自相关性；反之则不相关。
*   **块金值（Nugget）**：当距离 h=0 时，变异函数的值。它代表了由测量误差和微观尺度变异引起的随机性。

#### 2.1.1.2 最优线性无偏估计

克里金插值的目标是对待预测点 ($x_0$) 的值 ($Z^*(x_0)$) 进行估计。该估计值被表示为已知 ($N$) 个样本点 ($x_i$) 的观测值 ($Z(x_i)$) 的线性加权和：

\[Z^*(x_0) = \sum_{i=1}^{N} \lambda_i Z(x_i)\]

其中 ($\lambda_i$) 是待求的权重。为了实现“最优线性无偏估计”，权重 ($\lambda_i$) 的求解需要满足两个条件：

1.  **无偏性**：估计值的期望等于真实值的期望，即 $E[Z^*(x_0) - Z(x_0)] = 0$。这可以推导出权重之和为1，即 ($\sum_{i=1}^{N} \lambda_i = 1$)。
2.  **最优性**：估计方差 ($\sigma_E^2 = E[(Z^*(x_0) - Z(x_0))^2]$) 最小。

通过拉格朗日乘子法，将最小化估计方差问题转化为求解一个线性方程组，其矩阵形式如下：

\[
\begin{pmatrix}
\gamma(x_1, x_1) & \cdots & \gamma(x_1, x_N) & 1 \\
\vdots & \ddots & \vdots & \vdots \\
\gamma(x_N, x_1) & \cdots & \gamma(x_N, x_N) & 1 \\
1 & \cdots & 1 & 0
\end{pmatrix}
\begin{pmatrix}
\lambda_1 \\
\vdots \\
\lambda_N \\
\mu
\end{pmatrix}
=
\begin{pmatrix}
\gamma(x_1, x_0) \\
\vdots \\
\gamma(x_N, x_0) \\
1
\end{pmatrix}
\]

其中 ($\gamma(x_i, x_j)$) 是已知点之间的变异函数值，($\mu$) 是拉格朗日乘子。

#### 2.1.1.3 计算瓶颈

从上述方程组可以看出，为了求解权重，需要构建并求解一个 ($(N+1) \times (N+1)$) 的矩阵。其中，矩阵的求逆是计算量最大的步骤，其计算复杂度为 ($O(N^3)$)。这意味着当样本点数量 ($N$) 较大时（例如数千或数万个点），计算耗时会急剧增加，成为传统克里金方法应用于大规模数据集时的主要性能瓶颈。

### 2.1.2 物理信息神经网络（PINN）

物理信息神经网络（Physics-Informed Neural Networks, PINNs）是近年来在科学计算领域迅速崛起的一种深度学习框架，它将物理学基本定律（以偏微分方程形式表达）作为强先验信息，融入到神经网络的训练过程中。

#### 2.1.2.1 核心思想：复合损失函数

与传统的、纯粹由数据驱动的神经网络不同，PINN的独特之处在于其精心设计的**复合损失函数**。一个典型的PINN损失函数 ($L_{total}$) 由多个部分加权构成：

\[L_{total} = w_{data} L_{data} + w_{pde} L_{pde} + w_{bc} L_{bc}\]

其中：
*   ($L_{data}$) 是**数据损失**，通常是均方误差（MSE），用于衡量网络预测值与已知观测数据点（训练数据）之间的差异。这与标准的监督学习完全相同。
($L_{data} = \frac{1}{N_{data}} \sum_{i=1}^{N_{data}} |u(x_i) - u_{true}(x_i)|^2$)

*   ($L_{pde}$) 是**物理损失**或**PDE残差损失**。为了计算这一项，首先在求解域内部随机撒下大量的“配置点”（Collocation Points）。对于每一个配置点，利用神经网络的**自动微分**（Automatic Differentiation, AD）特性，计算出网络输出 ($u(x)$) 关于输入 ($x$) 的各阶导数（如 ($\frac{\partial u}{\partial x_1}, \frac{\partial^2 u}{\partial x_1^2}$) 等），并将它们代入到待求解的偏微分方程中。PDE的残差即为方程未能被满足的程度。($L_{pde}$) 就是所有配置点上PDE残差的均方误差。
($\mathcal{F}(u(x)) = 0 \implies L_{pde} = \frac{1}{N_{pde}} \sum_{j=1}^{N_{pde}} |\mathcal{F}(u(x_j))|^2$)

*   ($L_{bc}$) 是**边界条件损失**，用于衡量网络预测是否满足问题的边界条件（如狄利克雷边界、诺伊曼边界等）。

($w_{data}, w_{pde}, w_{bc}$) 是各项损失的权重系数，用于平衡数据拟合与物理规律的满足程度。

#### 2.1.2.2 自动微分（AD）

自动微分是实现PINN的关键技术。与数值微分（有精度误差）和符号微分（有表达式爆炸问题）不同，自动微分能够精确且高效地计算出复杂函数（如神经网络）的任意阶导数值。主流的深度学习框架（如TensorFlow, PyTorch）都内置了强大的自动微分引擎，这使得计算PDE残差变得轻而易举。

通过最小化上述复合损失函数，神经网络的权重和偏置在训练过程中被不断优化，最终收敛到一个既能拟合稀疏观测数据，又能在整个求解域内（包括没有数据的区域）普遍满足物理规律的函数，即偏微分方程的解。

### 2.1.3 基于GPU的并行计算

无论是求解克里金法的矩阵方程，还是训练PINN中的海量张量运算，都对计算性能提出了极高的要求。基于图形处理单元（GPU）的并行计算是解决此类计算密集型问题的关键技术。

#### 2.1.3.1 CPU与GPU的架构差异

*   **中央处理器（CPU）**：其设计目标是低延迟和高通用性。它拥有少量（数个至数十个）强大且复杂的计算核心，擅长处理逻辑判断、分支跳转等复杂的串行任务。
*   **图形处理单元（GPU）**：其设计目标是高吞吐量。它拥有数千个结构相对简单的计算核心，能够同时对大规模数据执行相同的指令。这种“单指令多数据流”（SIMD）的并行架构，使其在处理矩阵乘法、向量运算等具有高度并行性的任务时，表现出远超CPU的计算能力。

#### 2.1.3.2 CuPy：GPU上的NumPy

为了利用GPU的强大算力，开发者通常需要使用专门的并行编程模型，如NVIDIA的CUDA或开放标准的OpenCL。然而，这些底层API的学习曲线陡峭，开发效率较低。

**CuPy** 是一个开源的Python库，它旨在弥合这一差距。CuPy提供了与著名科学计算库 **NumPy** 高度兼容的API接口。这意味着，开发者可以用几乎与NumPy完全相同的语法编写代码，但这些代码实际上会在NVIDIA GPU上并行执行。例如，一个在NumPy中创建和操作数组的代码，只需将 `import numpy as np` 替换为 `import cupy as cp`，即可无缝迁移到GPU上运行，从而获得数十倍甚至上百倍的性能提升。

在本研究中，CuPy被专门用于加速克里金算法中的大规模矩阵构建、求逆和乘法运算，从而攻克其 ($O(N^3)$) 的性能瓶颈。而对于PINN的训练，其所依赖的底层深度学习框架（如PyTorch）本身就深度集成了GPU并行计算能力。
