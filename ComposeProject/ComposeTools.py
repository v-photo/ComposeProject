"""
GPU-Accelerated Block Kriging Ã— PINN è€¦åˆé‡å»ºå·¥å…·æ¨¡å—
GPU-Accelerated Block Kriging Ã— PINN Coupling Reconstruction Tools

åŠŸèƒ½æ¦‚è¿° (Functionality Overview):
- é€šç”¨å·¥å…· (Common Tools): æ•°æ®æ ‡å‡†åŒ–ã€è¯¯å·®ç»Ÿè®¡ã€å¯è§†åŒ–
- æ–¹æ¡ˆ1ä¸“ç”¨ (Mode 1 Specific): PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ
- æ–¹æ¡ˆ2ä¸“ç”¨ (Mode 2 Specific): Kriging ROIæ ·æœ¬æ‰©å…… â†’ PINNé‡è®­ç»ƒ

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
import time
from dataclasses import dataclass
from pathlib import Path

# ==================== ä»æ–°ç‰ˆPINNé¡¹ç›®è¿ç§»çš„æ•°æ®å¤„ç†å·¥å…· ====================

class RadiationDataProcessor:
    """
    Enhanced data processor for radiation field data
    Supports multiple input formats including {z: DataFrame[y, x]} from tool.py
    """
    
    def __init__(self, space_dims=None, world_bounds=None):
        """
        Initialize the data processor
        
        Args:
            space_dims: Physical dimensions [x, y, z] in meters
            world_bounds: Dict with 'min' and 'max' arrays, or None for auto-detection
        """
        self.space_dims = np.array(space_dims) if space_dims is not None else None
        self.world_bounds = world_bounds
        self.dose_data = None
        
    def load_from_dict(self, data_dict: Dict, space_dims=None, world_bounds=None):
        """
        Load radiation data from dictionary format {z: DataFrame[y, x]} or {z: numpy_array}
        Compatible with tool.py RadiationDataset format
        
        Args:
            data_dict: Dictionary where keys are z-coordinates and values are 2D data (DataFrame or numpy array)
            space_dims: Physical dimensions [x, y, z] in meters
            world_bounds: Physical bounds dict with 'min' and 'max' keys
            
        Returns:
            dict: Standardized dose_data format for PINN usage
        """
        print("Loading radiation data from dictionary format...")
        
        z_coords = sorted(data_dict.keys())
        first_layer = data_dict[z_coords[0]]
        
        if hasattr(first_layer, 'values'):
            first_array = first_layer.values
        else:
            first_array = np.array(first_layer)
        
        y_size, x_size = first_array.shape
        z_size = len(z_coords)
        
        dose_grid = np.zeros((x_size, y_size, z_size), dtype=np.float32)
        
        for z_idx, z_coord in enumerate(z_coords):
            layer_data = data_dict[z_coord]
            layer_array = layer_data.values if hasattr(layer_data, 'values') else np.array(layer_data)
            dose_grid[:, :, z_idx] = layer_array.T.astype(np.float32)
        
        if space_dims is not None:
            self.space_dims = np.array(space_dims, dtype=np.float32)
        elif self.space_dims is None:
            self.space_dims = np.array([20.0, 10.0, 10.0], dtype=np.float32)
        
        if world_bounds is not None:
            self.world_bounds = world_bounds
            world_min = np.array(world_bounds['min'], dtype=np.float32)
            world_max = np.array(world_bounds['max'], dtype=np.float32)
        elif self.world_bounds is not None:
            world_min = np.array(self.world_bounds['min'], dtype=np.float32)
            world_max = np.array(self.world_bounds['max'], dtype=np.float32)
        else:
            world_min = -self.space_dims / 2.0
            world_max = self.space_dims / 2.0
        
        grid_shape = np.array([x_size, y_size, z_size])
        voxel_size = (world_max - world_min) / grid_shape
        
        self.dose_data = {
            'dose_grid': dose_grid, 'world_min': world_min, 'world_max': world_max,
            'voxel_size': voxel_size, 'grid_shape': grid_shape, 'space_dims': self.space_dims,
            'z_coords': np.array(z_coords, dtype=np.float32), 'original_data_dict': data_dict
        }
        return self.dose_data

    def load_from_numpy(self, dose_array, space_dims, world_bounds=None):
        """
        Load radiation data from 3D numpy array
        """
        if dose_array.ndim != 3:
            raise ValueError(f"Expected 3D array, got {dose_array.ndim}D")
        
        self.space_dims = np.array(space_dims, dtype=np.float32)
        grid_shape = np.array(dose_array.shape)
        
        if world_bounds is not None:
            world_min = np.array(world_bounds['min'], dtype=np.float32)
            world_max = np.array(world_bounds['max'], dtype=np.float32)
        else:
            world_min = -self.space_dims / 2.0
            world_max = self.space_dims / 2.0
        
        voxel_size = (world_max - world_min) / grid_shape
        
        self.dose_data = {
            'dose_grid': dose_array.astype(np.float32), 'world_min': world_min, 'world_max': world_max,
            'voxel_size': voxel_size, 'grid_shape': grid_shape, 'space_dims': self.space_dims
        }
        return self.dose_data

    def get_dose_data(self):
        if self.dose_data is None:
            raise ValueError("No data loaded.")
        return self.dose_data

class DataLoader:
    @staticmethod
    def load_dose_from_dict(data_dict: Dict, space_dims=None, world_bounds=None):
        processor = RadiationDataProcessor()
        return processor.load_from_dict(data_dict, space_dims, world_bounds)

    @staticmethod
    def load_dose_from_numpy(dose_array, space_dims, world_bounds=None):
        processor = RadiationDataProcessor()
        return processor.load_from_numpy(dose_array, space_dims, world_bounds)

# ==================== è€¦åˆé¡¹ç›®åŸæœ‰å·¥å…·å’Œæ¨¡å—å¯¼å…¥ ====================

# å°è¯•å¯¼å…¥æ‰€éœ€çš„ç¬¬ä¸‰æ–¹åº“
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    warnings.warn("PyTorchä¸å¯ç”¨ï¼Œéƒ¨åˆ†GPUåŠ é€ŸåŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import cupy as cp
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    warnings.warn("CuPyä¸å¯ç”¨ï¼ŒGPUåŠ é€ŸåŠŸèƒ½å°†è¢«ç¦ç”¨")

# å¯¼å…¥ç°æœ‰æ¨¡å— - éœ€è¦ç¡®ä¿è·¯å¾„æ­£ç¡®
current_dir = Path(__file__).parent
project_root = current_dir.parent

# æ·»åŠ Krigingå’Œæ–°çš„PINNæ¨¡å—è·¯å¾„
sys.path.insert(0, str(project_root / "Kriging"))
sys.path.insert(0, str(project_root.parent / "PINN_claude"))

try:
    # å¯¼å…¥Krigingæ¨¡å—
    from myKriging import training as kriging_training, testing as kriging_testing
    from myPyKriging3D import MyOrdinaryKriging3D
    KRIGING_AVAILABLE = True
    print("âœ… Krigingæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    KRIGING_AVAILABLE = False
    warnings.warn(f"Krigingæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

try:
    # å¯¼å…¥æ–°çš„PINNæ¨¡å—
    from pinn_core import PINNTrainer, setup_deepxde_backend
    # ç«‹å³è®¾ç½®DeepXDEåç«¯
    setup_deepxde_backend()
    PINN_AVAILABLE = True
    print("âœ… æ–°ç‰ˆPINNæ¨¡å— (pinn_core) å¯¼å…¥æˆåŠŸ")
    print("âœ… DeepXDEåç«¯å·²è®¾ç½®ä¸ºPyTorch")
except ImportError as e:
    PINN_AVAILABLE = False
    warnings.warn(f"æ–°ç‰ˆPINNæ¨¡å— (pinn_core) å¯¼å…¥å¤±è´¥: {e}")

# ==================== å…¨å±€å¸¸é‡ä¸é…ç½® ====================
# Global Constants and Configuration

EPSILON = 1e-30  # æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
DEFAULT_METRICS = ['MAE', 'RMSE', 'MAPE', 'R2']

@dataclass
class ComposeConfig:
    """
    è€¦åˆç³»ç»Ÿå…¨å±€é…ç½®ç±»
    Global configuration for the coupling system
    """
    # é€šç”¨é…ç½® Common settings
    gpu_enabled: bool = True
    verbose: bool = True
    random_seed: int = 42
    
    # Krigingé…ç½® Kriging settings
    kriging_variogram_model: str = "linear"
    kriging_block_size: int = 10000
    kriging_enable_uncertainty: bool = True  # æ³¨æ„ï¼šå½“å‰å®ç°å¯èƒ½ä¸å®Œå…¨æ”¯æŒ
    
    # PINNé…ç½® PINN settings (å¯¹é½PINNå­é¡¹ç›®é…ç½®)
    pinn_epochs: int = 10000  # ä¸PINNå­é¡¹ç›®å¯¹é½ï¼š10000è½®è®­ç»ƒ
    pinn_learning_rate: float = 1e-3
    pinn_network_layers: List[int] = None
    pinn_use_lbfgs: bool = True  # å¯ç”¨L-BFGSï¼Œä¸PINNå­é¡¹ç›®å¯¹é½
    pinn_loss_weights: List[float] = None  # lossæƒé‡ï¼Œä¸PINNå­é¡¹ç›®å¯¹é½
    pinn_sampling_strategy: str = 'positive_only'  # é‡‡æ ·ç­–ç•¥ï¼Œä¸PINNå­é¡¹ç›®å¯¹é½
    pinn_include_source: bool = False # æ˜¯å¦åœ¨PINNæ¨¡å‹ä¸­åŒ…å«æºé¡¹å‚æ•°åŒ–
    
    # è€¦åˆé…ç½® Coupling settings
    fusion_weight: float = 0.5  # æ–¹æ¡ˆ1ä¸­çš„æƒé‡Ï‰
    roi_detection_strategy: str = 'high_density'  # æ–¹æ¡ˆ2ä¸­çš„ROIæ£€æµ‹ç­–ç•¥
    sample_augment_factor: float = 2.0  # æ–¹æ¡ˆ2ä¸­çš„æ ·æœ¬æ‰©å……å€æ•°
    
    def __post_init__(self):
        if self.pinn_network_layers is None:
            # ä¸PINNå­é¡¹ç›®å¯¹é½ï¼šä½¿ç”¨æ— æºPINNçš„ç½‘ç»œé…ç½® [3, 32, 32, 32, 32, 1]
            self.pinn_network_layers = [3, 32, 32, 32, 32, 1]
        
        if self.pinn_loss_weights is None:
            # ä¸PINNå­é¡¹ç›®å¯¹é½ï¼šä½¿ç”¨æ— æºPINNçš„lossæƒé‡ [1, 100]
            self.pinn_loss_weights = [1, 100]

# ==================== é€šç”¨å·¥å…· (Common Tools) ====================

@dataclass
class FieldTensor:
    """
    æ ‡å‡†åŒ–çš„åœºæ•°æ®ç»“æ„
    Standardized field data structure
    """
    coordinates: np.ndarray  # (N, 3) - xyzåæ ‡
    values: np.ndarray      # (N,) - åœºå€¼
    uncertainties: Optional[np.ndarray] = None  # (N,) - ä¸ç¡®å®šåº¦
    metadata: Optional[Dict[str, Any]] = None   # å…ƒæ•°æ®
    
    def __post_init__(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§ Validate data consistency"""
        if self.coordinates.shape[0] != self.values.shape[0]:
            raise ValueError("åæ ‡å’Œæ•°å€¼çš„æ•°é‡ä¸åŒ¹é…")
        if self.coordinates.shape[1] != 3:
            raise ValueError("åæ ‡å¿…é¡»æ˜¯3ç»´ (x, y, z)")
        if self.uncertainties is not None and self.uncertainties.shape[0] != self.values.shape[0]:
            raise ValueError("ä¸ç¡®å®šåº¦å’Œæ•°å€¼çš„æ•°é‡ä¸åŒ¹é…")

@dataclass 
class ProbeSet:
    """
    æ ‡å‡†åŒ–çš„æµ‹ç‚¹æ•°æ®ç»“æ„
    Standardized probe data structure
    """
    positions: np.ndarray   # (N, 3) - æµ‹ç‚¹åæ ‡
    measurements: np.ndarray # (N,) - æµ‹é‡å€¼
    weights: Optional[np.ndarray] = None  # (N,) - æƒé‡
    metadata: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        if self.positions.shape[0] != self.measurements.shape[0]:
            raise ValueError("æµ‹ç‚¹ä½ç½®å’Œæµ‹é‡å€¼çš„æ•°é‡ä¸åŒ¹é…")
        if self.positions.shape[1] != 3:
            raise ValueError("æµ‹ç‚¹ä½ç½®å¿…é¡»æ˜¯3ç»´ (x, y, z)")
        if self.weights is not None and self.weights.shape[0] != self.measurements.shape[0]:
            raise ValueError("æƒé‡å’Œæµ‹é‡å€¼çš„æ•°é‡ä¸åŒ¹é…")

class DataNormalizer:
    """
    æ•°æ®å½’ä¸€åŒ–å·¥å…·
    Data normalization utilities
    """
    
    @staticmethod
    def normalize_tensor_to_grid(field_tensor: FieldTensor, 
                               grid_shape: Tuple[int, int, int],
                               world_bounds: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        å°†å¼ é‡æ•°æ®è½¬æ¢ä¸ºç½‘æ ¼æ ¼å¼
        Convert tensor data to grid format
        
        Args:
            field_tensor: è¾“å…¥çš„åœºæ•°æ®å¼ é‡
            grid_shape: ç›®æ ‡ç½‘æ ¼å½¢çŠ¶ (nx, ny, nz)
            world_bounds: ä¸–ç•Œåæ ‡è¾¹ç•Œ {'min': [x,y,z], 'max': [x,y,z]}
            
        Returns:
            DictåŒ…å« 'grid', 'coordinates', 'bounds'
        """
        coordinates = field_tensor.coordinates
        values = field_tensor.values
        
        world_min = world_bounds['min']
        world_max = world_bounds['max']
        
        # åˆ›å»ºè§„åˆ™ç½‘æ ¼
        x_grid = np.linspace(world_min[0], world_max[0], grid_shape[0])
        y_grid = np.linspace(world_min[1], world_max[1], grid_shape[1])  
        z_grid = np.linspace(world_min[2], world_max[2], grid_shape[2])
        
        X, Y, Z = np.meshgrid(x_grid, y_grid, z_grid, indexing='ij')
        grid_coords = np.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)
        
        # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼å°†ä¸è§„åˆ™æ•°æ®æ˜ å°„åˆ°ç½‘æ ¼
        from scipy.spatial import cKDTree
        tree = cKDTree(coordinates)
        distances, indices = tree.query(grid_coords)
        
        grid_values = values[indices].reshape(grid_shape)
        
        return {
            'grid': grid_values,
            'coordinates': grid_coords.reshape((*grid_shape, 3)),
            'bounds': world_bounds,
            'interpolation_distances': distances.reshape(grid_shape)
        }
    
    @staticmethod
    def robust_normalize(data: np.ndarray, 
                        quantile_range: Tuple[float, float] = (0.01, 0.99)) -> Tuple[np.ndarray, Dict]:
        """
        é²æ£’å½’ä¸€åŒ– (åŸºäºåˆ†ä½æ•°)
        Robust normalization based on quantiles
        """
        q_low, q_high = quantile_range
        low_val = np.quantile(data, q_low)
        high_val = np.quantile(data, q_high)
        
        normalized = np.clip((data - low_val) / (high_val - low_val + EPSILON), 0, 1)
        
        normalization_info = {
            'method': 'robust',
            'low_val': low_val,
            'high_val': high_val,
            'quantile_range': quantile_range
        }
        
        return normalized, normalization_info

class MetricsCalculator:
    """
    è¯¯å·®ç»Ÿè®¡è®¡ç®—å™¨
    Error metrics calculator
    """
    
    @staticmethod
    def compute_metrics(true_values: np.ndarray, 
                       pred_values: np.ndarray,
                       metrics: List[str] = None) -> Dict[str, float]:
        """
        è®¡ç®—é¢„æµ‹è¯¯å·®æŒ‡æ ‡
        Compute prediction error metrics
        
        Args:
            true_values: çœŸå®å€¼
            pred_values: é¢„æµ‹å€¼  
            metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨
            
        Returns:
            Dict[metric_name, metric_value]
        """
        if metrics is None:
            metrics = DEFAULT_METRICS
            
        # ç¡®ä¿è¾“å…¥ä¸ºnumpyæ•°ç»„
        true_values = np.asarray(true_values).flatten()
        pred_values = np.asarray(pred_values).flatten()
        
        if len(true_values) != len(pred_values):
            raise ValueError("çœŸå®å€¼å’Œé¢„æµ‹å€¼çš„é•¿åº¦ä¸åŒ¹é…")
        
        results = {}
        
        # è®¡ç®—æ®‹å·®
        residuals = pred_values - true_values
        
        # å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute Error
        if 'MAE' in metrics:
            results['MAE'] = np.mean(np.abs(residuals))
        
        # å‡æ–¹æ ¹è¯¯å·® Root Mean Square Error  
        if 'RMSE' in metrics:
            results['RMSE'] = np.sqrt(np.mean(residuals**2))
        
        # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® Mean Absolute Percentage Error
        # åªåœ¨éé›¶çœŸå€¼å¤„è®¡ç®—
        if 'MAPE' in metrics:
            nonzero_mask = np.abs(true_values) > EPSILON
            if np.any(nonzero_mask):
                mape_values = np.abs(residuals[nonzero_mask] / true_values[nonzero_mask])
                results['MAPE'] = np.mean(mape_values) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            else:
                results['MAPE'] = float('inf')
        
        # å†³å®šç³»æ•° R-squared
        if 'R2' in metrics:
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((true_values - np.mean(true_values))**2)
            results['R2'] = 1 - (ss_res / (ss_tot + EPSILON))
        
        # ç›¸å…³ç³»æ•° Pearson correlation
        if 'CORR' in metrics:
            correlation_matrix = np.corrcoef(true_values, pred_values)
            results['CORR'] = correlation_matrix[0, 1] if not np.isnan(correlation_matrix[0, 1]) else 0.0
        
        return results
    
    @staticmethod
    def compute_relative_error_stats(true_values: np.ndarray, 
                                   pred_values: np.ndarray,
                                   percentiles: List[float] = None) -> Dict[str, float]:
        """
        è®¡ç®—ç›¸å¯¹è¯¯å·®çš„ç»Ÿè®¡åˆ†å¸ƒ
        Compute relative error statistics
        """
        if percentiles is None:
            percentiles = [10, 25, 50, 75, 90, 95, 99]
        
        # åªåœ¨éé›¶çœŸå€¼å¤„è®¡ç®—ç›¸å¯¹è¯¯å·®
        nonzero_mask = np.abs(true_values) > EPSILON
        if not np.any(nonzero_mask):
            return {f'P{p}': float('inf') for p in percentiles}
        
        relative_errors = np.abs((pred_values[nonzero_mask] - true_values[nonzero_mask]) 
                                / true_values[nonzero_mask]) * 100
        
        stats = {}
        for p in percentiles:
            stats[f'P{p}'] = np.percentile(relative_errors, p)
        
        stats['mean_rel_error'] = np.mean(relative_errors)
        stats['std_rel_error'] = np.std(relative_errors)
        
        return stats

class VisualizationTools:
    """
    å¯è§†åŒ–å·¥å…·é›†
    Visualization utilities
    """
    
    @staticmethod
    def plot_comparison_2d_slice(true_field: np.ndarray,
                               pred_field: np.ndarray, 
                               slice_axis: int = 2,
                               slice_idx: Optional[int] = None,
                               uncertainty_field: Optional[np.ndarray] = None,
                               save_path: Optional[str] = None,
                               title_prefix: str = "") -> plt.Figure:
        """
        ç»˜åˆ¶2Dåˆ‡ç‰‡å¯¹æ¯”å›¾
        Plot 2D slice comparison
        
        Args:
            true_field: çœŸå®åœº (nx, ny, nz)
            pred_field: é¢„æµ‹åœº (nx, ny, nz)
            slice_axis: åˆ‡ç‰‡è½´ (0=x, 1=y, 2=z)
            slice_idx: åˆ‡ç‰‡ç´¢å¼•ï¼ŒNoneåˆ™ä½¿ç”¨ä¸­é—´åˆ‡ç‰‡
            uncertainty_field: ä¸ç¡®å®šåº¦åœºï¼ˆå¯é€‰ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            title_prefix: æ ‡é¢˜å‰ç¼€
            
        Returns:
            matplotlib Figureå¯¹è±¡
        """
        if slice_idx is None:
            slice_idx = true_field.shape[slice_axis] // 2
        
        # æå–åˆ‡ç‰‡
        if slice_axis == 0:
            true_slice = true_field[slice_idx, :, :]
            pred_slice = pred_field[slice_idx, :, :]
            uncertainty_slice = uncertainty_field[slice_idx, :, :] if uncertainty_field is not None else None
        elif slice_axis == 1:
            true_slice = true_field[:, slice_idx, :]
            pred_slice = pred_field[:, slice_idx, :]
            uncertainty_slice = uncertainty_field[:, slice_idx, :] if uncertainty_field is not None else None
        else:  # slice_axis == 2
            true_slice = true_field[:, :, slice_idx]
            pred_slice = pred_field[:, :, slice_idx]
            uncertainty_slice = uncertainty_field[:, :, slice_idx] if uncertainty_field is not None else None
        
        # åˆ›å»ºå­å›¾å¸ƒå±€
        n_plots = 3 if uncertainty_slice is not None else 2
        fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))
        if n_plots == 2:
            axes = [axes[0], axes[1]]
        
        # ç»˜åˆ¶çœŸå®åœº
        im1 = axes[0].imshow(true_slice.T, origin='lower', aspect='auto', 
                           norm=LogNorm(vmin=max(true_slice.min(), EPSILON), vmax=true_slice.max()))
        axes[0].set_title(f'{title_prefix}çœŸå®åœº (è½´{slice_axis}, åˆ‡ç‰‡{slice_idx})')
        axes[0].set_xlabel('X' if slice_axis != 0 else ('Y' if slice_axis == 2 else 'Z'))
        axes[0].set_ylabel('Y' if slice_axis != 1 else ('X' if slice_axis == 2 else 'Z'))
        plt.colorbar(im1, ax=axes[0])
        
        # ç»˜åˆ¶é¢„æµ‹åœº
        im2 = axes[1].imshow(pred_slice.T, origin='lower', aspect='auto',
                           norm=LogNorm(vmin=max(pred_slice.min(), EPSILON), vmax=pred_slice.max()))
        axes[1].set_title(f'{title_prefix}é¢„æµ‹åœº')
        axes[1].set_xlabel('X' if slice_axis != 0 else ('Y' if slice_axis == 2 else 'Z'))
        axes[1].set_ylabel('Y' if slice_axis != 1 else ('X' if slice_axis == 2 else 'Z'))
        plt.colorbar(im2, ax=axes[1])
        
        # ç»˜åˆ¶ä¸ç¡®å®šåº¦åœºï¼ˆå¦‚æœæä¾›ï¼‰
        if uncertainty_slice is not None:
            im3 = axes[2].imshow(uncertainty_slice.T, origin='lower', aspect='auto')
            axes[2].set_title(f'{title_prefix}ä¸ç¡®å®šåº¦')
            axes[2].set_xlabel('X' if slice_axis != 0 else ('Y' if slice_axis == 2 else 'Z'))
            axes[2].set_ylabel('Y' if slice_axis != 1 else ('X' if slice_axis == 2 else 'Z'))
            plt.colorbar(im3, ax=axes[2])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            
        return fig
    
    @staticmethod
    def plot_residual_analysis(residuals: np.ndarray,
                             coordinates: Optional[np.ndarray] = None,
                             save_path: Optional[str] = None) -> plt.Figure:
        """
        æ®‹å·®åˆ†æå¯è§†åŒ–
        Residual analysis visualization
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æ®‹å·®ç›´æ–¹å›¾
        axes[0, 0].hist(residuals, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(np.mean(residuals), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(residuals):.2e}')
        axes[0, 0].axvline(np.median(residuals), color='orange', linestyle='--', label=f'ä¸­ä½æ•°: {np.median(residuals):.2e}')
        axes[0, 0].set_xlabel('æ®‹å·®å€¼')
        axes[0, 0].set_ylabel('é¢‘ç‡')
        axes[0, 0].set_title('æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Q-Qå›¾
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('æ®‹å·®Q-Qå›¾ (æ­£æ€æ€§æ£€éªŒ)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ®‹å·®ç»å¯¹å€¼vsé¢„æµ‹å€¼ï¼ˆå¦‚æœæœ‰åæ ‡ä¿¡æ¯ï¼‰
        if coordinates is not None and coordinates.shape[0] == len(residuals):
            # ä½¿ç”¨zåæ ‡ä½œä¸ºå‚è€ƒ
            z_coords = coordinates[:, 2]
            scatter = axes[1, 0].scatter(z_coords, np.abs(residuals), alpha=0.6, c=np.abs(residuals), cmap='viridis')
            axes[1, 0].set_xlabel('Zåæ ‡')
            axes[1, 0].set_ylabel('æ®‹å·®ç»å¯¹å€¼')
            axes[1, 0].set_title('æ®‹å·®ç»å¯¹å€¼ vs Zåæ ‡')
            plt.colorbar(scatter, ax=axes[1, 0])
        else:
            # æ®‹å·®ç»å¯¹å€¼vsç´¢å¼•
            axes[1, 0].plot(np.abs(residuals), 'o', alpha=0.6, markersize=3)
            axes[1, 0].set_xlabel('æ ·æœ¬ç´¢å¼•')
            axes[1, 0].set_ylabel('æ®‹å·®ç»å¯¹å€¼')
            axes[1, 0].set_title('æ®‹å·®ç»å¯¹å€¼åˆ†å¸ƒ')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ®‹å·®ç»Ÿè®¡æ‘˜è¦
        axes[1, 1].axis('off')
        stats_text = f"""
æ®‹å·®ç»Ÿè®¡æ‘˜è¦:

åŸºæœ¬ç»Ÿè®¡é‡:
â€¢ æ ·æœ¬æ•°é‡: {len(residuals)}
â€¢ å‡å€¼: {np.mean(residuals):.4e}
â€¢ æ ‡å‡†å·®: {np.std(residuals):.4e}
â€¢ æœ€å°å€¼: {np.min(residuals):.4e}
â€¢ æœ€å¤§å€¼: {np.max(residuals):.4e}

åˆ†ä½æ•°:
â€¢ 25%: {np.percentile(residuals, 25):.4e}
â€¢ 50%: {np.percentile(residuals, 50):.4e}
â€¢ 75%: {np.percentile(residuals, 75):.4e}

è´¨é‡æŒ‡æ ‡:
â€¢ MAE: {np.mean(np.abs(residuals)):.4e}
â€¢ RMSE: {np.sqrt(np.mean(residuals**2)):.4e}
â€¢ ååº¦: {stats.skew(residuals):.4f}
â€¢ å³°åº¦: {stats.kurtosis(residuals):.4f}
        """
        axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=10, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        
        if save_path:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            
        return fig

    @staticmethod
    def plot_pinn_error_analysis(train_errors: np.ndarray, 
                                train_points: np.ndarray,
                                pinn_predictions: np.ndarray,
                                true_values: np.ndarray,
                                save_path: Optional[str] = None) -> plt.Figure:
        """
        PINNè¯¯å·®æ·±åº¦åˆ†æå¯è§†åŒ–
        PINN error deep analysis visualization
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è¯¯å·®vsçœŸå®å€¼æ•£ç‚¹å›¾
        axes[0, 0].scatter(true_values, train_errors, alpha=0.6, c='blue', s=20)
        axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=1)
        axes[0, 0].set_xlabel('çœŸå®å€¼')
        axes[0, 0].set_ylabel('é¢„æµ‹è¯¯å·®')
        axes[0, 0].set_title('è¯¯å·® vs çœŸå®å€¼')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(true_values, train_errors, 1)
        p = np.poly1d(z)
        axes[0, 0].plot(true_values, p(true_values), "r--", alpha=0.8, linewidth=1)
        
        # 2. è¯¯å·®vsé¢„æµ‹å€¼æ•£ç‚¹å›¾
        axes[0, 1].scatter(pinn_predictions, train_errors, alpha=0.6, c='green', s=20)
        axes[0, 1].axhline(0, color='red', linestyle='--', linewidth=1)
        axes[0, 1].set_xlabel('PINNé¢„æµ‹å€¼')
        axes[0, 1].set_ylabel('é¢„æµ‹è¯¯å·®')
        axes[0, 1].set_title('è¯¯å·® vs PINNé¢„æµ‹å€¼')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. 3Dç©ºé—´è¯¯å·®åˆ†å¸ƒ
        ax_3d = fig.add_subplot(2, 3, 3, projection='3d')
        scatter = ax_3d.scatter(train_points[:, 0], train_points[:, 1], train_points[:, 2], 
                               c=np.abs(train_errors), cmap='hot', s=30, alpha=0.7)
        ax_3d.set_xlabel('X')
        ax_3d.set_ylabel('Y') 
        ax_3d.set_zlabel('Z')
        ax_3d.set_title('3Dç©ºé—´è¯¯å·®åˆ†å¸ƒ')
        plt.colorbar(scatter, ax=ax_3d, shrink=0.8)
        
        # 4. è¯¯å·®ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
        sorted_errors = np.sort(np.abs(train_errors))
        y_vals = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)
        axes[1, 0].plot(sorted_errors, y_vals, linewidth=2, color='purple')
        axes[1, 0].set_xlabel('è¯¯å·®ç»å¯¹å€¼')
        axes[1, 0].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
        axes[1, 0].set_title('è¯¯å·®ç´¯ç§¯åˆ†å¸ƒå‡½æ•°')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ å…³é”®ç™¾åˆ†ä½çº¿
        percentiles = [50, 80, 90, 95]
        colors = ['blue', 'orange', 'red', 'darkred']
        for p, color in zip(percentiles, colors):
            error_val = np.percentile(np.abs(train_errors), p)
            axes[1, 0].axvline(error_val, color=color, linestyle='--', alpha=0.7, 
                              label=f'{p}%: {error_val:.2e}')
        axes[1, 0].legend()
        
        # 5. é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾
        axes[1, 1].scatter(true_values, pinn_predictions, alpha=0.6, c='cyan', s=20)
        
        # å®Œç¾é¢„æµ‹çº¿
        min_val = min(np.min(true_values), np.min(pinn_predictions))
        max_val = max(np.max(true_values), np.max(pinn_predictions))
        axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
        
        axes[1, 1].set_xlabel('çœŸå®å€¼')
        axes[1, 1].set_ylabel('PINNé¢„æµ‹å€¼')
        axes[1, 1].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. è¯¯å·®ç»Ÿè®¡æ‘˜è¦è¡¨æ ¼
        axes[1, 2].axis('off')
        
        # è®¡ç®—ç›¸å…³æ€§
        from scipy.stats import pearsonr, spearmanr
        pearson_corr, _ = pearsonr(true_values, pinn_predictions)
        spearman_corr, _ = spearmanr(true_values, pinn_predictions)
        
        stats_text = f"""
PINNé¢„æµ‹æ€§èƒ½è¯¦ç»†åˆ†æ:

åŸºæœ¬è¯¯å·®ç»Ÿè®¡:
â€¢ MAE: {np.mean(np.abs(train_errors)):.4e}
â€¢ RMSE: {np.sqrt(np.mean(train_errors**2)):.4e}
â€¢ MAPE: {np.mean(np.abs(train_errors)/(np.abs(true_values)+1e-8))*100:.2f}%
â€¢ æœ€å¤§è¯¯å·®: {np.max(np.abs(train_errors)):.4e}

ç›¸å…³æ€§åˆ†æ:
â€¢ Pearsonç›¸å…³ç³»æ•°: {pearson_corr:.4f}
â€¢ Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f}
â€¢ RÂ²å†³å®šç³»æ•°: {1 - np.sum(train_errors**2)/np.sum((true_values-np.mean(true_values))**2):.4f}

è¯¯å·®åˆ†å¸ƒ:
â€¢ è¯¯å·®å‡å€¼: {np.mean(train_errors):.4e}
â€¢ è¯¯å·®æ ‡å‡†å·®: {np.std(train_errors):.4e}
â€¢ æ­£åè¯¯å·®æ¯”ä¾‹: {np.sum(train_errors>0)/len(train_errors)*100:.1f}%
â€¢ è´Ÿåè¯¯å·®æ¯”ä¾‹: {np.sum(train_errors<0)/len(train_errors)*100:.1f}%

æ•°æ®èŒƒå›´:
â€¢ çœŸå®å€¼èŒƒå›´: [{np.min(true_values):.2e}, {np.max(true_values):.2e}]
â€¢ é¢„æµ‹å€¼èŒƒå›´: [{np.min(pinn_predictions):.2e}, {np.max(pinn_predictions):.2e}]
        """
        
        axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes,
                        fontsize=9, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        plt.tight_layout()
        
        if save_path:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            
        return fig

# ==================== é€šç”¨æ¥å£é€‚é…å™¨ ====================

class KrigingAdapter:
    """
    Krigingæ¨¡å—çš„æ ‡å‡†åŒ–æ¥å£é€‚é…å™¨
    Standardized interface adapter for Kriging module
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.model = None
        self.is_fitted = False
        
    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> 'KrigingAdapter':
        """
        æ ‡å‡†åŒ–çš„fitæ¥å£
        Standardized fit interface
        
        Args:
            X: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            y: è®­ç»ƒç‚¹æ•°å€¼ (N,)
            **kwargs: é¢å¤–çš„krigingå‚æ•°
            
        Returns:
            self
        """
        if not KRIGING_AVAILABLE:
            raise RuntimeError("Krigingæ¨¡å—ä¸å¯ç”¨")
            
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼ˆå…¼å®¹ç°æœ‰æ¥å£ï¼‰
        df = pd.DataFrame({
            'x': X[:, 0],
            'y': X[:, 1], 
            'z': X[:, 2],
            'target': y
        })
        
        # ä½¿ç”¨ç°æœ‰çš„trainingå‡½æ•°
        variogram_model = kwargs.get('variogram_model', self.config.kriging_variogram_model)
        self.model = kriging_training(
            df=df,
            variogram_model=variogram_model,
            nlags=kwargs.get('nlags', 8),
            enable_plotting=kwargs.get('enable_plotting', False),
            weight=kwargs.get('weight', False),
            uk=kwargs.get('uk', False),
            cpu_on=not self.config.gpu_enabled
        )
        
        self.is_fitted = True
        return self
    
    def predict(self, X: np.ndarray, return_std: bool = False, **kwargs) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        æ ‡å‡†åŒ–çš„predictæ¥å£
        Standardized predict interface
        
        Args:
            X: é¢„æµ‹ç‚¹åæ ‡ (N, 3)
            return_std: æ˜¯å¦è¿”å›æ ‡å‡†å·®ï¼ˆä¸ç¡®å®šåº¦ï¼‰
            **kwargs: é¢å¤–çš„é¢„æµ‹å‚æ•°
            
        Returns:
            predictions æˆ– (predictions, std)
        """
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")
            
        if not KRIGING_AVAILABLE:
            raise RuntimeError("Krigingæ¨¡å—ä¸å¯ç”¨")
        
        # å°†é¢„æµ‹ç‚¹è½¬æ¢ä¸ºDataFrameæ ¼å¼
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æä¾›è™šæ‹Ÿçš„targetåˆ—ï¼Œä½†ä¸ä¼šè¢«ä½¿ç”¨
        df_pred = pd.DataFrame({
            'x': X[:, 0],
            'y': X[:, 1],
            'z': X[:, 2], 
            'target': np.zeros(X.shape[0])  # è™šæ‹Ÿå€¼
        })
        
        # ä½¿ç”¨ç°æœ‰çš„testingå‡½æ•°è¿›è¡Œé¢„æµ‹
        predictions, _ = kriging_testing(
            df=df_pred,
            model=self.model,
            block_size=kwargs.get('block_size', self.config.kriging_block_size),
            cpu_on=not self.config.gpu_enabled,
            style=kwargs.get('style', "gpu_b"),
            multi_process=kwargs.get('multi_process', False),
            print_time=kwargs.get('print_time', False),
            torch_ac=kwargs.get('torch_ac', False),
            compute_precision=False  # å…³é—­ç²¾åº¦è®¡ç®—é¿å…æ··æ·†
        )
        
        if return_std and self.config.kriging_enable_uncertainty:
            # æ³¨æ„ï¼šå½“å‰å®ç°å¯èƒ½ä¸å®Œå…¨æ”¯æŒå…¨å±€ÏƒÂ²è¾“å‡º
            # TODO: éœ€è¦ä¿®æ”¹ç°æœ‰Krigingä»£ç ä»¥æ­£ç¡®è¿”å›ä¸ç¡®å®šåº¦
            warnings.warn("å½“å‰Krigingå®ç°æš‚ä¸å®Œå…¨æ”¯æŒå…¨å±€ÏƒÂ²è¾“å‡ºï¼Œè¿”å›çš„ä¸ç¡®å®šåº¦å¯èƒ½ä¸å‡†ç¡®")
            
            # ä¸´æ—¶æ–¹æ¡ˆï¼šä½¿ç”¨executeæ–¹æ³•ç›´æ¥è·å–æ–¹å·®
            try:
                _, variances = self.model.execute(
                    style='points',
                    xpoints=X[:, 0],
                    ypoints=X[:, 1], 
                    zpoints=X[:, 2],
                    block_size=kwargs.get('block_size', self.config.kriging_block_size),
                    cpu_on=not self.config.gpu_enabled
                )
                std_values = np.sqrt(np.maximum(variances, 0))  # ç¡®ä¿éè´Ÿ
                return predictions, std_values
            except Exception as e:
                warnings.warn(f"è·å–ä¸ç¡®å®šåº¦å¤±è´¥: {e}ï¼Œè¿”å›é›¶ä¸ç¡®å®šåº¦")
                return predictions, np.zeros_like(predictions)
        else:
            return predictions

class PINNAdapter:
    """
    PINNæ¨¡å—çš„æ ‡å‡†åŒ–æ¥å£é€‚é…å™¨  
    Standardized interface adapter for PINN module
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.trainer = None
        self.dose_data = None
        self.is_fitted = False
        
    def fit(self, X: np.ndarray, y: np.ndarray, 
           dose_data: Optional[Dict] = None,
           space_dims: List[float] = None,
           world_bounds: Dict = None,
           **kwargs) -> 'PINNAdapter':
        """
        æ ¹æ®è¾“å…¥æ•°æ®å’Œé…ç½®ï¼Œè®­ç»ƒæˆ–é‡æ–°è®­ç»ƒPINNæ¨¡å‹ã€‚
        æ”¯æŒä»dose_dataè‡ªåŠ¨åˆå§‹åŒ–ï¼Œæˆ–ä»space_dims/world_boundsæ‰‹åŠ¨åˆå§‹åŒ–ã€‚
        """
        if not PINN_AVAILABLE:
            raise RuntimeError("æ–°ç‰ˆPINN (pinn_core) æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œfitæ“ä½œã€‚")
            
        # æ­¥éª¤ 1: ç”¨ç‰©ç†å‚æ•°åˆå§‹åŒ– PINNTrainer
        self.trainer = PINNTrainer(physical_params=kwargs.get('physical_params'))

        # æ­¥éª¤ 2: å‡†å¤‡å¹¶è°ƒç”¨ create_pinn_model
        X = X.astype(np.float32)
        y = y.astype(np.float32)
        sampled_log_doses_values = np.log(y + EPSILON)
        network_config = kwargs.get('network_config')
        
        self.trainer.create_pinn_model(
            dose_data=dose_data,
            sampled_points_xyz=X,
            sampled_log_doses_values=sampled_log_doses_values,
            include_source=False,
            network_config=network_config
        )

        # æ­¥éª¤ 3: å‡†å¤‡å¹¶è°ƒç”¨ train æ–¹æ³•
        train_params = {
            "epochs": self.config.pinn_epochs,
            "use_lbfgs": self.config.pinn_use_lbfgs,
            "loss_weights": self.config.pinn_loss_weights,
            "display_every": 500
        }
        train_params.update({k: v for k, v in kwargs.items() if k in ['epochs', 'use_lbfgs', 'loss_weights']})

        try:
            self.trainer.train(**train_params)
            self.trained = True
        except Exception as e:
            print(f"âŒ PINNè®­ç»ƒå¤±è´¥: {e}")
            raise e
            
        return self

    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:
        """
        æ ‡å‡†åŒ–çš„predictæ¥å£
        
        Args:
            X: é¢„æµ‹ç‚¹åæ ‡ (N, 3)
            **kwargs: é¢å¤–çš„é¢„æµ‹å‚æ•°
            
        Returns:
            predictions: é¢„æµ‹å€¼ (N,)
        """
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")
            
        if not PINN_AVAILABLE:
            raise RuntimeError("PINNæ¨¡å—ä¸å¯ç”¨")
        
        try:
            # ä½¿ç”¨PINNè¿›è¡Œé¢„æµ‹
            log_predictions = self.trainer.predict(X)
            
            # é˜²æ­¢expæº¢å‡ºçš„å®‰å…¨é¢„æµ‹
            log_predictions = np.clip(log_predictions, -30, 15)  # æ›´åˆç†çš„å¯¹æ•°å€¼èŒƒå›´
            predictions = np.exp(log_predictions) - EPSILON  # è½¬å›åŸå§‹å°ºåº¦
            
            # ç¡®ä¿é¢„æµ‹å€¼ä¸ºæ­£æ•°ä¸”åœ¨æ•°æ®èŒƒå›´å†…
            # åŠ¨æ€ç¡®å®šåˆç†çš„ä¸Šç•Œï¼ˆåŸºäºè®­ç»ƒæ•°æ®èŒƒå›´ï¼‰
            if hasattr(self, 'data_max_value'):
                max_pred = self.data_max_value * 10  # å…è®¸ä¸€å®šçš„å¤–æ¨
            else:
                max_pred = 1e6  # ä¿å®ˆçš„ä¸Šç•Œ
                
            predictions = np.clip(predictions, EPSILON, max_pred)
            
            if self.config.verbose:
                print(f"   ğŸ” PINNé¢„æµ‹ç»Ÿè®¡: èŒƒå›´[{np.min(predictions):.2e}, {np.max(predictions):.2e}]")
                print(f"   ğŸ“Š æœ‰æ•ˆé¢„æµ‹æ•°é‡: {len(predictions)}")
            
            return predictions
            
        except RuntimeError as e:
            if "CUDA" in str(e):
                if self.config.verbose:
                    print(f"âŒ CUDAé¢„æµ‹å¤±è´¥: {e}")
                    print("ğŸ”„ å°è¯•CPUæ¨¡å¼é¢„æµ‹...")
                
                # è®¾ç½®CPUæ¨¡å¼é‡æ–°é¢„æµ‹
                import torch
                torch.set_default_device('cpu')
                
                log_predictions = self.trainer.predict(X)
                predictions = np.exp(log_predictions) - EPSILON
                
                return predictions
            else:
                raise e

def validate_compose_environment() -> Dict[str, bool]:
    """
    éªŒè¯è€¦åˆç¯å¢ƒçš„å®Œæ•´æ€§
    Validate the coupling environment integrity
    
    Returns:
        Dict of availability status for each component
    """
    status = {
        'Kriging': KRIGING_AVAILABLE,
        'PINN': PINN_AVAILABLE, 
        'CuPy': CUPY_AVAILABLE,
        'PyTorch': TORCH_AVAILABLE
    }
    
    print("\n=== ç¯å¢ƒæ£€æŸ¥ç»“æœ Environment Check ===")
    for component, available in status.items():
        status_str = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
        print(f"{component}: {status_str}")
    
    return status 

# ==================== æ–¹æ¡ˆ1ä¸“ç”¨åŠŸèƒ½ (Mode 1 Specific) ====================
# PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ

class Mode1ResidualKriging:
    """
    æ–¹æ¡ˆ1: æ®‹å·®å…‹é‡Œé‡‘æ’å€¼ä¸“ç”¨å·¥å…·
    Mode 1: Residual Kriging specific tools
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.kriging_adapter = KrigingAdapter(config)
        
    def compute_residuals(self, 
                         train_points: np.ndarray,
                         train_values: np.ndarray, 
                         pinn_predictions: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—PINNé¢„æµ‹ä¸çœŸå®å€¼çš„æ®‹å·®
        Compute residuals between PINN predictions and true values
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: çœŸå®è®­ç»ƒå€¼ (N,)
            pinn_predictions: PINNåœ¨è®­ç»ƒç‚¹çš„é¢„æµ‹å€¼ (N,)
            
        Returns:
            residuals: æ®‹å·® = çœŸå®å€¼ - PINNé¢„æµ‹å€¼ (N,)
        """
        if len(train_values) != len(pinn_predictions):
            raise ValueError("çœŸå®å€¼å’ŒPINNé¢„æµ‹å€¼çš„é•¿åº¦ä¸åŒ¹é…")
            
        residuals = train_values - pinn_predictions
        
        # æ£€æŸ¥å¹¶ä¿®å¤å¼‚å¸¸å€¼
        valid_mask = np.isfinite(residuals)
        if not np.all(valid_mask):
            print(f"       âš ï¸ å‘ç° {np.sum(~valid_mask)} ä¸ªæ— æ•ˆæ®‹å·®å€¼ï¼Œå°†è¿›è¡Œä¿®å¤")
            residuals = residuals[valid_mask]
            train_points = train_points[valid_mask]
            train_values = train_values[valid_mask]
            pinn_predictions = pinn_predictions[valid_mask]
        
        # æ£€æŸ¥æ®‹å·®çš„æ•°å€¼ç‰¹æ€§
        residual_std = np.std(residuals)
        residual_range = np.max(residuals) - np.min(residuals)
        
        if residual_std < 1e-10 or residual_range < 1e-10:
            # å¦‚æœæ®‹å·®å˜åŒ–æå°ï¼Œè¯´æ˜PINNé¢„æµ‹è¿‡äºä¸€è‡´ï¼Œéœ€è¦æ·»åŠ ç©ºé—´ç»“æ„
            print("       ğŸ”§ æ£€æµ‹åˆ°æ®‹å·®ç©ºé—´å˜åŒ–è¿‡å°ï¼Œæ·»åŠ åŸºäºä½ç½®çš„å¾®æ‰°ä»¥æ”¹å–„Krigingå»ºæ¨¡")
            
            # åŸºäºç©ºé—´ä½ç½®æ·»åŠ å¾®æ‰°ï¼Œä¿æŒç©ºé—´ç›¸å…³æ€§
            spatial_weights = np.linalg.norm(train_points - np.mean(train_points, axis=0), axis=1)
            spatial_weights = (spatial_weights - np.min(spatial_weights)) / (np.max(spatial_weights) - np.min(spatial_weights) + 1e-10)
            
            # æ·»åŠ ä¸ç©ºé—´ä½ç½®ç›¸å…³çš„å¾®æ‰°
            base_residual = np.mean(residuals)
            perturbation_scale = max(abs(base_residual) * 0.05, np.std(train_values) * 0.01, 1e-3)
            
            # ä½¿ç”¨ç©ºé—´æƒé‡ç”Ÿæˆå…·æœ‰ç©ºé—´ç»“æ„çš„å¾®æ‰°
            spatial_perturbation = perturbation_scale * (spatial_weights - 0.5) * 2
            random_perturbation = np.random.normal(0, perturbation_scale * 0.1, len(residuals))
            
            residuals = residuals + spatial_perturbation + random_perturbation
        
        # å¯¹æ®‹å·®è¿›è¡Œåˆç†æ€§æ£€æŸ¥å’Œè£å‰ª
        residuals = np.clip(residuals, -1e6, 1e6)
        
        if self.config.verbose:
            print(f"       ğŸ“Š æ®‹å·®ç»Ÿè®¡: å‡å€¼={np.mean(residuals):.4e}, æ ‡å‡†å·®={np.std(residuals):.4e}")
            print(f"       ğŸ“ˆ æ®‹å·®èŒƒå›´: [{np.min(residuals):.4e}, {np.max(residuals):.4e}]")
            
        return residuals
        
    def residual_kriging(self,
                        train_points: np.ndarray,
                        train_values: np.ndarray,
                        pinn_predictions: np.ndarray,
                        prediction_points: np.ndarray,
                        return_uncertainty: bool = True,
                        **kriging_params) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        å¯¹æ®‹å·®è¿›è¡Œå…‹é‡Œé‡‘æ’å€¼
        Perform Kriging interpolation on residuals
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: çœŸå®è®­ç»ƒå€¼ (N,)
            pinn_predictions: PINNåœ¨è®­ç»ƒç‚¹çš„é¢„æµ‹å€¼ (N,)
            prediction_points: é¢„æµ‹ç‚¹åæ ‡ (M, 3)
            return_uncertainty: æ˜¯å¦è¿”å›ä¸ç¡®å®šåº¦
            **kriging_params: å…‹é‡Œé‡‘å‚æ•°
            
        Returns:
            residual_predictions: æ®‹å·®é¢„æµ‹ (M,)
            å¦‚æœreturn_uncertainty=True: (residual_predictions, residual_std)
        """
        # è®¡ç®—æ®‹å·®
        print(f"       ğŸ§® è®¡ç®—æ®‹å·® = çœŸå®å€¼ - PINNé¢„æµ‹å€¼...")
        residuals = self.compute_residuals(train_points, train_values, pinn_predictions)
        
        # è®­ç»ƒæ®‹å·®å…‹é‡Œé‡‘æ¨¡å‹
        print(f"       ğŸ—ï¸ è®­ç»ƒæ®‹å·®å…‹é‡Œé‡‘æ¨¡å‹ (å˜å¼‚å‡½æ•°: {kriging_params.get('variogram_model', 'linear')})...")
        self.kriging_adapter.fit(train_points, residuals, **kriging_params)
        print(f"       âœ… æ®‹å·®å…‹é‡Œé‡‘æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # é¢„æµ‹æ®‹å·®
        if return_uncertainty and self.config.kriging_enable_uncertainty:
            residual_pred, residual_std = self.kriging_adapter.predict(
                prediction_points, return_std=True
            )
            return residual_pred, residual_std
        else:
            residual_pred = self.kriging_adapter.predict(prediction_points, return_std=False)
            if return_uncertainty:
                # å¦‚æœè¯·æ±‚ä¸ç¡®å®šåº¦ä½†ä¸å¯ç”¨ï¼Œè¿”å›é›¶ä¸ç¡®å®šåº¦
                return residual_pred, np.zeros_like(residual_pred)
            else:
                return residual_pred

class Mode1Fusion:
    """
    æ–¹æ¡ˆ1: åŠ æƒèåˆå·¥å…·
    Mode 1: Weighted fusion tools
    """
    
    @staticmethod
    def fuse_residual(pinn_pred: np.ndarray,
                     kriging_residual: np.ndarray, 
                     weight: float = 0.5,
                     uncertainty: Optional[np.ndarray] = None) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        åŠ æƒèåˆPINNé¢„æµ‹å’Œæ®‹å·®é¢„æµ‹
        Weighted fusion of PINN predictions and residual predictions
        
        Args:
            pinn_pred: PINNé¢„æµ‹å€¼ (N,)
            kriging_residual: Krigingæ®‹å·®é¢„æµ‹å€¼ (N,)  
            weight: æ®‹å·®æƒé‡ Ï‰ âˆˆ (0,1), æœ€ç»ˆé¢„æµ‹ = PINN + Ï‰Ã—æ®‹å·®
            uncertainty: Krigingæ®‹å·®çš„ä¸ç¡®å®šåº¦ (N,) [å¯é€‰]
            
        Returns:
            fused_prediction: èåˆé¢„æµ‹ (N,)
            å¦‚æœæä¾›uncertainty: (fused_prediction, confidence_bounds)
        """
        if len(pinn_pred) != len(kriging_residual):
            raise ValueError("PINNé¢„æµ‹å’Œæ®‹å·®é¢„æµ‹çš„é•¿åº¦ä¸åŒ¹é…")
            
        if not 0 < weight < 1:
            warnings.warn(f"æƒé‡ {weight} ä¸åœ¨æ¨èèŒƒå›´ (0,1) å†…")
        
        # åŠ æƒèåˆ
        fused_pred = pinn_pred + weight * kriging_residual
        
        if uncertainty is not None:
            # è®¡ç®—ç½®ä¿¡ç•Œ (å‡è®¾PINNæ— ä¸ç¡®å®šåº¦ï¼Œåªè€ƒè™‘Krigingæ®‹å·®çš„ä¸ç¡®å®šåº¦)
            # 95%ç½®ä¿¡ç•Œ â‰ˆ Â±1.96Ïƒ  
            confidence_bounds = weight * 1.96 * uncertainty
            return fused_pred, confidence_bounds
        else:
            return fused_pred
    
    @staticmethod
    def adaptive_weight_strategy(residuals: np.ndarray,
                               kriging_std: Optional[np.ndarray] = None,
                               strategy: str = 'variance_based') -> np.ndarray:
        """
        è‡ªé€‚åº”æƒé‡ç­–ç•¥
        Adaptive weighting strategy
        
        Args:
            residuals: æ®‹å·®å€¼ (N,)
            kriging_std: Krigingæ ‡å‡†å·® (N,) [å¯é€‰]
            strategy: æƒé‡ç­–ç•¥ ('variance_based', 'magnitude_based', 'uniform')
            
        Returns:
            weights: è‡ªé€‚åº”æƒé‡ (N,)
        """
        n_points = len(residuals)
        
        if strategy == 'uniform':
            return np.full(n_points, 0.5)
        
        elif strategy == 'magnitude_based':
            # åŸºäºæ®‹å·®å¹…åº¦ï¼šæ®‹å·®è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜
            abs_residuals = np.abs(residuals)
            max_residual = np.max(abs_residuals) 
            weights = 0.1 + 0.8 * (abs_residuals / (max_residual + EPSILON))
            return np.clip(weights, 0.1, 0.9)
        
        elif strategy == 'variance_based' and kriging_std is not None:
            # åŸºäºKrigingä¸ç¡®å®šåº¦ï¼šä¸ç¡®å®šåº¦è¶Šå°ï¼Œæƒé‡è¶Šé«˜
            normalized_std = kriging_std / (np.max(kriging_std) + EPSILON)
            weights = 0.1 + 0.8 * (1 - normalized_std)  # åæ¯”å…³ç³»
            return np.clip(weights, 0.1, 0.9)
        
        else:
            warnings.warn(f"ä¸æ”¯æŒçš„æƒé‡ç­–ç•¥ '{strategy}' æˆ–ç¼ºå°‘å¿…è¦æ•°æ®ï¼Œä½¿ç”¨å‡åŒ€æƒé‡")
            return np.full(n_points, 0.5)

# ==================== æ–¹æ¡ˆ2ä¸“ç”¨åŠŸèƒ½ (Mode 2 Specific) ====================  
# Krigingåœ¨ROIç”Ÿæˆæ–°æ ·æœ¬ â†’ æ‰©å……æ•°æ® â†’ é‡æ–°è®­ç»ƒPINN

class Mode2ROIDetector:
    """
    æ–¹æ¡ˆ2: æ„Ÿå…´è¶£åŒºåŸŸ(ROI)æ£€æµ‹å™¨
    Mode 2: Region of Interest (ROI) detector
    """
    
    @staticmethod
    def detect_roi(train_points: np.ndarray,
                  train_values: np.ndarray,
                  roi_strategy: str = 'high_density',
                  **strategy_params) -> Dict[str, np.ndarray]:
        """
        æ£€æµ‹ç›¸å…³åŒºåŸŸ (Region of Interest)
        Detect region of interest for sample augmentation
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: è®­ç»ƒç‚¹æ•°å€¼ (N,)
            roi_strategy: ROIæ£€æµ‹ç­–ç•¥
            **strategy_params: ç­–ç•¥ç›¸å…³å‚æ•°
            
        Returns:
            roi_bounds: ROIè¾¹ç•Œä¿¡æ¯ {'min': [x,y,z], 'max': [x,y,z], 'mask': bool_array}
        """
        if roi_strategy == 'high_density':
            return Mode2ROIDetector._detect_high_density_roi(
                train_points, train_values, **strategy_params
            )
        elif roi_strategy == 'high_value':
            return Mode2ROIDetector._detect_high_value_roi(
                train_points, train_values, **strategy_params
            )
        elif roi_strategy == 'bounding_box':
            return Mode2ROIDetector._detect_bounding_box_roi(
                train_points, train_values, **strategy_params
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ROIç­–ç•¥: {roi_strategy}")
    
    @staticmethod
    def _detect_high_density_roi(train_points: np.ndarray,
                               train_values: np.ndarray,
                               density_percentile: float = 75,
                               expansion_factor: float = 1.2) -> Dict[str, np.ndarray]:
        """é«˜å¯†åº¦åŒºåŸŸæ£€æµ‹ç­–ç•¥"""
        from scipy.spatial import cKDTree
        
        # è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦
        tree = cKDTree(train_points)
        # è®¡ç®—åˆ°ç¬¬5è¿‘é‚»çš„è·ç¦»ä½œä¸ºå¯†åº¦çš„é€†æŒ‡æ ‡
        k = min(5, len(train_points) - 1)
        distances, _ = tree.query(train_points, k=k+1)  # k+1å› ä¸ºåŒ…å«è‡ªèº«
        local_density = 1 / (np.mean(distances[:, 1:], axis=1) + EPSILON)  # æ’é™¤è‡ªèº«
        
        # é€‰æ‹©é«˜å¯†åº¦ç‚¹
        density_threshold = np.percentile(local_density, density_percentile)
        high_density_mask = local_density >= density_threshold
        
        if not np.any(high_density_mask):
            # å¦‚æœæ²¡æœ‰é«˜å¯†åº¦ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰ç‚¹
            high_density_mask = np.ones(len(train_points), dtype=bool)
        
        roi_points = train_points[high_density_mask]
        
        # è®¡ç®—ROIè¾¹ç•Œ
        roi_min = np.min(roi_points, axis=0)
        roi_max = np.max(roi_points, axis=0)
        
        # æ‰©å±•è¾¹ç•Œ
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        return {
            'min': roi_min,
            'max': roi_max, 
            'mask': high_density_mask,
            'density_scores': local_density
        }
    
    @staticmethod
    def _detect_high_value_roi(train_points: np.ndarray,
                             train_values: np.ndarray,
                             value_percentile: float = 80,
                             expansion_factor: float = 1.5) -> Dict[str, np.ndarray]:
        """é«˜æ•°å€¼åŒºåŸŸæ£€æµ‹ç­–ç•¥"""
        # é€‰æ‹©é«˜æ•°å€¼ç‚¹
        value_threshold = np.percentile(train_values, value_percentile)
        high_value_mask = train_values >= value_threshold
        
        if not np.any(high_value_mask):
            # å¦‚æœæ²¡æœ‰é«˜æ•°å€¼ç‚¹ï¼Œä½¿ç”¨æ•°å€¼å¤§äº0çš„ç‚¹
            high_value_mask = train_values > 0
            
        if not np.any(high_value_mask):
            # å¦‚æœä»ç„¶æ²¡æœ‰ï¼Œä½¿ç”¨æ‰€æœ‰ç‚¹
            high_value_mask = np.ones(len(train_points), dtype=bool)
        
        roi_points = train_points[high_value_mask]
        
        # è®¡ç®—ROIè¾¹ç•Œå¹¶æ‰©å±•
        roi_min = np.min(roi_points, axis=0)
        roi_max = np.max(roi_points, axis=0)
        
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        return {
            'min': roi_min,
            'max': roi_max,
            'mask': high_value_mask,
            'value_scores': train_values
        }
    
    @staticmethod
    def _detect_bounding_box_roi(train_points: np.ndarray,
                               train_values: np.ndarray,
                               expansion_factor: float = 1.1) -> Dict[str, np.ndarray]:
        """åŒ…å›´ç›’ROIæ£€æµ‹ç­–ç•¥"""
        # ä½¿ç”¨æ‰€æœ‰è®­ç»ƒç‚¹çš„åŒ…å›´ç›’
        roi_min = np.min(train_points, axis=0)
        roi_max = np.max(train_points, axis=0)
        
        # è½»å¾®æ‰©å±•
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        # æ‰€æœ‰ç‚¹éƒ½åœ¨ROIå†…
        all_points_mask = np.ones(len(train_points), dtype=bool)
        
        return {
            'min': roi_min,
            'max': roi_max,
            'mask': all_points_mask,
            'bounding_box': True
        }

class Mode2SampleAugmentor:
    """
    æ–¹æ¡ˆ2: æ ·æœ¬æ‰©å……å™¨  
    Mode 2: Sample augmentor using Kriging
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.kriging_adapter = KrigingAdapter(config)
        
    def augment_by_kriging(self,
                          train_points: np.ndarray,
                          train_values: np.ndarray,
                          roi_bounds: Dict[str, np.ndarray],
                          augment_factor: float = 2.0,
                          sampling_strategy: str = 'grid',
                          **kriging_params) -> Tuple[np.ndarray, np.ndarray]:
        """
        åœ¨ROIå†…ç”¨Krigingç”Ÿæˆæ–°æ ·æœ¬
        Generate new samples in ROI using Kriging
        
        Args:
            train_points: åŸå§‹è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: åŸå§‹è®­ç»ƒå€¼ (N,)
            roi_bounds: ROIè¾¹ç•Œä¿¡æ¯
            augment_factor: æ‰©å……å€æ•° (æ–°æ ·æœ¬æ•° = åŸæ ·æœ¬æ•° Ã— (augment_factor - 1))
            sampling_strategy: é‡‡æ ·ç­–ç•¥ ('grid', 'random', 'adaptive')
            **kriging_params: Krigingå‚æ•°
            
        Returns:
            augmented_points: æ‰©å……åçš„åæ ‡ (N+M, 3)
            augmented_values: æ‰©å……åçš„æ•°å€¼ (N+M,)
        """
        # è®­ç»ƒKrigingæ¨¡å‹
        self.kriging_adapter.fit(train_points, train_values, **kriging_params)
        
        # ç”ŸæˆROIå†…çš„æ–°é‡‡æ ·ç‚¹
        n_original = len(train_points) 
        n_new = int(n_original * (augment_factor - 1.0))
        
        if n_new <= 0:
            warnings.warn("æ‰©å……å€æ•°å¤ªå°ï¼Œæ²¡æœ‰ç”Ÿæˆæ–°æ ·æœ¬")
            return train_points, train_values
        
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆæ–°é‡‡æ ·ç‚¹
        new_points = self._generate_sampling_points(
            roi_bounds, n_new, sampling_strategy, train_points
        )
        
        # ä½¿ç”¨Krigingé¢„æµ‹æ–°ç‚¹çš„æ•°å€¼
        new_values = self.kriging_adapter.predict(new_points, return_std=False)
        
        # åˆå¹¶åŸå§‹å’Œæ–°ç”Ÿæˆçš„æ ·æœ¬
        augmented_points = np.vstack([train_points, new_points])
        augmented_values = np.concatenate([train_values, new_values])
        
        if self.config.verbose:
            print(f"æ ·æœ¬æ‰©å……å®Œæˆ: {n_original} â†’ {len(augmented_points)} ä¸ªæ ·æœ¬")
            print(f"æ–°æ ·æœ¬æ•°å€¼èŒƒå›´: [{np.min(new_values):.4e}, {np.max(new_values):.4e}]")
        
        return augmented_points, augmented_values
    
    def _generate_sampling_points(self,
                                roi_bounds: Dict[str, np.ndarray], 
                                n_points: int,
                                strategy: str,
                                existing_points: np.ndarray) -> np.ndarray:
        """åœ¨ROIå†…ç”Ÿæˆé‡‡æ ·ç‚¹"""
        roi_min = roi_bounds['min']
        roi_max = roi_bounds['max']
        
        if strategy == 'grid':
            return self._generate_grid_points(roi_min, roi_max, n_points)
        elif strategy == 'random':
            return self._generate_random_points(roi_min, roi_max, n_points)
        elif strategy == 'adaptive':
            return self._generate_adaptive_points(roi_min, roi_max, n_points, existing_points)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·ç­–ç•¥: {strategy}")
    
    def _generate_grid_points(self, roi_min: np.ndarray, roi_max: np.ndarray, n_points: int) -> np.ndarray:
        """ç”Ÿæˆè§„åˆ™ç½‘æ ¼ç‚¹"""
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ç‚¹æ•°ï¼ˆå°½é‡æ¥è¿‘ç«‹æ–¹ä½“ï¼‰
        points_per_dim = int(np.ceil(n_points ** (1/3)))
        
        x = np.linspace(roi_min[0], roi_max[0], points_per_dim)
        y = np.linspace(roi_min[1], roi_max[1], points_per_dim)
        z = np.linspace(roi_min[2], roi_max[2], points_per_dim)
        
        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
        grid_points = np.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)
        
        # å¦‚æœç”Ÿæˆçš„ç‚¹æ•°è¶…è¿‡éœ€è¦çš„æ•°é‡ï¼Œéšæœºé€‰æ‹©
        if len(grid_points) > n_points:
            indices = np.random.choice(len(grid_points), n_points, replace=False)
            grid_points = grid_points[indices]
        
        return grid_points
    
    def _generate_random_points(self, roi_min: np.ndarray, roi_max: np.ndarray, n_points: int) -> np.ndarray:
        """ç”Ÿæˆéšæœºé‡‡æ ·ç‚¹"""
        random_points = np.random.rand(n_points, 3)
        random_points = roi_min + random_points * (roi_max - roi_min)
        return random_points
    
    def _generate_adaptive_points(self, roi_min: np.ndarray, roi_max: np.ndarray, 
                                n_points: int, existing_points: np.ndarray) -> np.ndarray:
        """ç”Ÿæˆè‡ªé€‚åº”é‡‡æ ·ç‚¹ï¼ˆé¿å¼€å·²æœ‰ç‚¹å¯†é›†åŒºåŸŸï¼‰"""
        from scipy.spatial import cKDTree
        
        # æ„å»ºå·²æœ‰ç‚¹çš„KDæ ‘
        tree = cKDTree(existing_points)
        
        # ç”Ÿæˆå€™é€‰ç‚¹ï¼ˆæ¯”éœ€è¦çš„å¤šä¸€äº›ï¼‰
        n_candidates = n_points * 3
        candidate_points = self._generate_random_points(roi_min, roi_max, n_candidates)
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰ç‚¹åˆ°æœ€è¿‘å·²æœ‰ç‚¹çš„è·ç¦»
        distances, _ = tree.query(candidate_points)
        
        # é€‰æ‹©è·ç¦»è¾ƒå¤§çš„ç‚¹ï¼ˆè¿œç¦»å·²æœ‰ç‚¹ï¼‰
        sorted_indices = np.argsort(distances)[::-1]  # é™åºæ’åˆ—
        selected_indices = sorted_indices[:n_points]
        
        return candidate_points[selected_indices]

# ==================== ç«¯åˆ°ç«¯è€¦åˆå·¥ä½œæµ ====================
# End-to-end coupling workflows

class CouplingWorkflow:
    """
    è€¦åˆå·¥ä½œæµç®¡ç†å™¨
    Coupling workflow manager
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.mode1_tools = {
            'residual_kriging': Mode1ResidualKriging(config),
            'fusion': Mode1Fusion()
        }
        self.mode2_tools = {
            'roi_detector': Mode2ROIDetector(),
            'augmentor': Mode2SampleAugmentor(config)
        }
        self.pinn_adapter = PINNAdapter(config)
        
    def run_mode1_pipeline(self,
                          train_points: np.ndarray,
                          train_values: np.ndarray,
                          prediction_points: np.ndarray,
                          fusion_weight: Optional[float] = None,
                          dose_data: Optional[Dict] = None,
                          **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–¹æ¡ˆ1å®Œæ•´æµç¨‹: PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ
        Execute Mode 1 complete pipeline
        """
        if fusion_weight is None:
            fusion_weight = self.config.fusion_weight
            
        results = {}
        
        # æ­¥éª¤1: è®­ç»ƒPINN
        print("ğŸ”¥ æ­¥éª¤1: è®­ç»ƒPINNæ¨¡å‹...")
        self.pinn_adapter.fit(
            train_points, train_values,
            space_dims=kwargs.get('space_dims'),
            world_bounds=kwargs.get('world_bounds'),
            dose_data=dose_data,
            epochs=kwargs.get('epochs'),
            max_training_points=kwargs.get('max_training_points'),
            loss_weights=self.config.pinn_loss_weights,
            use_lbfgs=self.config.pinn_use_lbfgs
        )
        
        # æ­¥éª¤2: PINNé¢„æµ‹
        print("ğŸ”® æ­¥éª¤2: PINNå…¨åœºé¢„æµ‹...")
        pinn_train_pred = self.pinn_adapter.predict(train_points)
        pinn_field_pred = self.pinn_adapter.predict(prediction_points)
        
        # ==================== æ–°å¢ï¼šè¯¦ç»†PINNè¯¯å·®ç»Ÿè®¡ ====================
        print("\nğŸ“Š æ­¥éª¤2.1: PINNè¯¯å·®åˆ†æ...")
        
        # è®­ç»ƒç‚¹è¯¯å·®ç»Ÿè®¡
        train_errors = train_values - pinn_train_pred
        train_metrics = {
            'è®­ç»ƒé›†MAE': np.mean(np.abs(train_errors)),
            'è®­ç»ƒé›†RMSE': np.sqrt(np.mean(train_errors**2)),
            'è®­ç»ƒé›†MAPE': np.mean(np.abs(train_errors) / (np.abs(train_values) + EPSILON)) * 100,
            'è®­ç»ƒé›†æœ€å¤§è¯¯å·®': np.max(np.abs(train_errors)),
            'è®­ç»ƒé›†RÂ²': 1 - np.sum(train_errors**2) / np.sum((train_values - np.mean(train_values))**2)
        }
        
        print("   ğŸ¯ PINNè®­ç»ƒé›†æ€§èƒ½:")
        for metric, value in train_metrics.items():
            print(f"      {metric}: {value:.4f}")
        
        # é¢„æµ‹å€¼ç»Ÿè®¡ä¿¡æ¯  
        print("   ğŸ” PINNé¢„æµ‹ç»Ÿè®¡: èŒƒå›´[{:.2e}, {:.2e}]".format(np.min(pinn_train_pred), np.max(pinn_train_pred)))
        print("   ğŸ“Š æœ‰æ•ˆé¢„æµ‹æ•°é‡: {}".format(len(pinn_train_pred)))
        
        # é¢„æµ‹ç‚¹é¢„æµ‹å€¼ç»Ÿè®¡
        print("   ğŸ” PINNé¢„æµ‹ç»Ÿè®¡: èŒƒå›´[{:.2e}, {:.2e}]".format(np.min(pinn_field_pred), np.max(pinn_field_pred)))
        print("   ğŸ“Š æœ‰æ•ˆé¢„æµ‹æ•°é‡: {}".format(len(pinn_field_pred)))
        
        # æ·»åŠ è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
        error_percentiles = [5, 25, 50, 75, 95]
        error_stats = np.percentile(np.abs(train_errors), error_percentiles)
        print("   ğŸ“ˆ è®­ç»ƒè¯¯å·®åˆ†å¸ƒ (ç»å¯¹å€¼):")
        for p, val in zip(error_percentiles, error_stats):
            print(f"      {p}%åˆ†ä½æ•°: {val:.4e}")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        error_threshold = np.mean(np.abs(train_errors)) + 3 * np.std(np.abs(train_errors))
        outlier_count = np.sum(np.abs(train_errors) > error_threshold)
        outlier_percentage = outlier_count / len(train_errors) * 100
        print(f"   âš ï¸ å¼‚å¸¸è¯¯å·®ç‚¹: {outlier_count}ä¸ª ({outlier_percentage:.1f}%)")
        
        # ç©ºé—´è¯¯å·®åˆ†æï¼ˆå¦‚æœè®­ç»ƒç‚¹è¾ƒå¤šï¼‰
        if len(train_points) >= 10:
            # è®¡ç®—è¯¯å·®çš„ç©ºé—´ç›¸å…³æ€§
            spatial_distances = np.linalg.norm(train_points[:, None] - train_points[None, :], axis=2)
            error_correlations = []
            
            # é€‰æ‹©å‡ ä¸ªè·ç¦»èŒƒå›´æ¥åˆ†æè¯¯å·®ç›¸å…³æ€§
            distance_ranges = [0.5, 1.0, 2.0, 5.0]
            for dist_range in distance_ranges:
                close_pairs = (spatial_distances > 0) & (spatial_distances < dist_range)
                if np.sum(close_pairs) > 0:
                    error_pairs = train_errors[:, None] * train_errors[None, :]
                    mean_error_corr = np.mean(error_pairs[close_pairs])
                    error_correlations.append((dist_range, mean_error_corr))
            
            if error_correlations:
                print("   ğŸ—ºï¸ ç©ºé—´è¯¯å·®ç›¸å…³æ€§:")
                for dist, corr in error_correlations:
                    print(f"      è·ç¦»<{dist:.1f}m: ç›¸å…³æ€§={corr:.4e}")
        
        # å­˜å‚¨è¯¯å·®ç»Ÿè®¡ç»“æœ
        results['pinn_train_errors'] = train_errors
        results['pinn_train_metrics'] = train_metrics
        results['pinn_predictions'] = pinn_field_pred
        # ==================== PINNè¯¯å·®ç»Ÿè®¡ç»“æŸ ====================
        
        # æ­¥éª¤3: æ®‹å·®Kriging
        print("âš¡ æ­¥éª¤3: æ®‹å·®Krigingæ’å€¼...")
        print(f"   ğŸ” è®¡ç®—PINNè®­ç»ƒç‚¹é¢„æµ‹ä¸çœŸå®å€¼çš„æ®‹å·®...")
        print(f"   ğŸŒ å¯¹æ®‹å·®è¿›è¡ŒKrigingç©ºé—´æ’å€¼...")
        print(f"   ğŸ“Š è®­ç»ƒç‚¹æ•°é‡: {len(train_points)}")
        print(f"   ğŸ“ é¢„æµ‹ç‚¹æ•°é‡: {len(prediction_points)}")
        
        residual_pred, residual_std = self.mode1_tools['residual_kriging'].residual_kriging(
            train_points, train_values, pinn_train_pred, prediction_points,
            return_uncertainty=True, **kwargs.get('kriging_params', {})
        )
        
        print(f"   âœ… æ®‹å·®Krigingæ’å€¼å®Œæˆ")
        print(f"   ğŸ“ˆ æ®‹å·®é¢„æµ‹èŒƒå›´: [{np.min(residual_pred):.4e}, {np.max(residual_pred):.4e}]")
        if residual_std is not None:
            print(f"   ğŸ“Š æ®‹å·®ä¸ç¡®å®šåº¦èŒƒå›´: [{np.min(residual_std):.4e}, {np.max(residual_std):.4e}]")
        results['residual_predictions'] = residual_pred
        results['residual_std'] = residual_std
        
        # æ­¥éª¤4: åŠ æƒèåˆ
        print("ğŸ”— æ­¥éª¤4: åŠ æƒèåˆ...")
        if residual_std is not None and not np.all(residual_std == 0):
            fused_pred, confidence_bounds = self.mode1_tools['fusion'].fuse_residual(
                pinn_field_pred, residual_pred, fusion_weight, residual_std
            )
            results['confidence_bounds'] = confidence_bounds
        else:
            fused_pred = self.mode1_tools['fusion'].fuse_residual(
                pinn_field_pred, residual_pred, fusion_weight
            )
            results['confidence_bounds'] = None
            
        results['final_predictions'] = fused_pred
        results['fusion_weight'] = fusion_weight
        
        print("âœ… æ–¹æ¡ˆ1æµç¨‹å®Œæˆ!")
        return results
    
    def run_mode2_pipeline(self,
                          train_points: np.ndarray, 
                          train_values: np.ndarray,
                          prediction_points: np.ndarray,
                          roi_strategy: Optional[str] = None,
                          augment_factor: Optional[float] = None,
                          dose_data: Optional[Dict] = None,
                          **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–¹æ¡ˆ2å®Œæ•´æµç¨‹: Kriging ROIæ ·æœ¬æ‰©å…… â†’ PINNé‡è®­ç»ƒ
        Execute Mode 2 complete pipeline  
        """
        if roi_strategy is None:
            roi_strategy = self.config.roi_detection_strategy
        if augment_factor is None:
            augment_factor = self.config.sample_augment_factor
            
        results = {}
        
        # æ­¥éª¤1: ROIæ£€æµ‹
        print("ğŸ¯ æ­¥éª¤1: æ£€æµ‹æ„Ÿå…´è¶£åŒºåŸŸ(ROI)...")
        roi_bounds = self.mode2_tools['roi_detector'].detect_roi(
            train_points, train_values, roi_strategy, **kwargs.get('roi_params', {})
        )
        results['roi_bounds'] = roi_bounds
        
        # æ­¥éª¤2: Krigingæ ·æœ¬æ‰©å……
        print("ğŸ“ˆ æ­¥éª¤2: Krigingæ ·æœ¬æ‰©å……...")
        augmented_points, augmented_values = self.mode2_tools['augmentor'].augment_by_kriging(
            train_points, train_values, roi_bounds, augment_factor,
            **kwargs.get('kriging_params', {})
        )
        results['augmented_points'] = augmented_points
        results['augmented_values'] = augmented_values
        
        # æ­¥éª¤3: ç”¨æ‰©å……æ•°æ®é‡æ–°è®­ç»ƒPINN
        print("ğŸ”¥ æ­¥éª¤3: ç”¨æ‰©å……æ•°æ®é‡æ–°è®­ç»ƒPINN...")
        enhanced_pinn = PINNAdapter(self.config)
        enhanced_pinn.fit(
            augmented_points, augmented_values,
            space_dims=kwargs.get('space_dims'),
            world_bounds=kwargs.get('world_bounds'),
            dose_data=dose_data,
            epochs=kwargs.get('epochs'),
            loss_weights=self.config.pinn_loss_weights,
            use_lbfgs=self.config.pinn_use_lbfgs
        )
        
        # æ­¥éª¤4: æœ€ç»ˆé¢„æµ‹
        print("ğŸ”® æ­¥éª¤4: å¢å¼ºPINNå…¨åœºé¢„æµ‹...")
        final_pred = enhanced_pinn.predict(prediction_points)
        results['final_predictions'] = final_pred
        results['enhanced_pinn'] = enhanced_pinn
        
        print("âœ… æ–¹æ¡ˆ2æµç¨‹å®Œæˆ!")
        return results

def print_compose_banner():
    """æ‰“å°é¡¹ç›®æ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         GPU Block-Kriging Ã— PINN è€¦åˆé‡å»ºå·¥å…·æ¨¡å—            â•‘  
    â•‘        GPU-Accelerated Block Kriging Ã— PINN Coupling        â•‘
    â•‘                                                              â•‘
    â•‘  ğŸš€ æ–¹æ¡ˆ1: PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ                     â•‘
    â•‘  ğŸ¯ æ–¹æ¡ˆ2: Kriging ROIæ ·æœ¬æ‰©å…… â†’ PINNé‡è®­ç»ƒ                  â•‘  
    â•‘                                                              â•‘
    â•‘  ğŸ’¡ æ”¯æŒGPUåŠ é€Ÿ | ğŸ”¬ ç‰©ç†çº¦æŸ | ğŸ“Š ä¸ç¡®å®šåº¦é‡åŒ–              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

if __name__ == "__main__":
    print_compose_banner()
    validate_compose_environment() 