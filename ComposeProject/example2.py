import numpy as np
import time
import sys
from pathlib import Path
import torch
import deepxde as dde
import pandas as pd

# --- è·¯å¾„è®¾ç½® ---
try:
    current_dir = Path(__file__).parent.resolve()
    project_root = current_dir.parent
except NameError:
    project_root = Path('.').resolve()

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'PINN'))
sys.path.insert(0, str(project_root / 'Kriging'))

# --- æ¨¡å—å¯¼å…¥ ---
try:
    from PINN.data_processing import DataLoader
    from PINN.dataAnalysis import get_data
    from myKriging import training as kriging_training, testing as kriging_testing
    print("âœ… å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥æˆåŠŸã€‚")
except ImportError as e:
    print(f"âŒ å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# =================================================================================
#  å…è´£å£°æ˜ï¼šä»¥ä¸‹ç±»å’Œå‡½æ•°å‡ä¸ºå ä½ç¬¦ (Placeholder)
#  æ‚¨éœ€è¦æ ¹æ®æ‚¨ç°æœ‰çš„ PINN å’Œ Kriging åº“æ¥å¡«å……å®ƒä»¬çš„å…·ä½“å®ç°ã€‚
#  è¿™é‡Œçš„éª¨æ¶æ˜¯ä¸ºäº†æ¸…æ™°åœ°å±•ç¤º"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"è¿™ä¸€æŠ€æœ¯è·¯çº¿ã€‚
# =================================================================================

class DummyDataLoader:
    """
    ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»å¤–éƒ¨æ–‡ä»¶åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®(æ›¿ä»£åŸæœ‰çš„DummyDataLoader)ã€‚
    """
    def __init__(self, data_path: str, space_dims: np.ndarray, num_samples: int):
        self.data_path = data_path
        self.space_dims = space_dims
        self.num_samples = num_samples
        print(f"INFO: (DataLoader) Initialized with data_path='{self.data_path}'")

    def get_training_data(self) -> np.ndarray:
        """
        åŠ è½½ã€å¤„ç†å¹¶é‡‡æ ·ç¨€ç–è®­ç»ƒç‚¹ã€‚
        è¿™äº›ç‚¹åœ¨PINNä¸­å……å½“"æ•°æ®çœŸå€¼"(Ground Truth)ï¼Œæ˜¯æ•°æ®æŸå¤±é¡¹çš„æ¥æºã€‚
        å…¶åŠŸèƒ½ç±»ä¼¼äºåŸget_boundary_conditionsï¼Œä½†æ•°æ®æºæ˜¯å¤–éƒ¨æ–‡ä»¶ã€‚
        """
        print(f"INFO: (DataLoader) Loading raw data from {self.data_path}...")
        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            
        raw_data = get_data(self.data_path)
        
        print("INFO: (DataLoader) Normalizing dose data...")
        dose_data = DataLoader.load_dose_from_dict(
            data_dict=raw_data,
            space_dims=self.space_dims
        )
        
        print(f"INFO: (DataLoader) Sampling {self.num_samples} training points...")
        train_points, train_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=self.num_samples,
            sampling_strategy='positive_only',
        )
        print(f"INFO: (DataLoader) âœ… Successfully sampled {len(train_points)} points.")

        # å°†åæ ‡å’Œå€¼åˆå¹¶æˆ [x, y, z, value] æ ¼å¼
        all_sampled_data = np.hstack([train_points, train_values.reshape(-1, 1)])
        
        # å°†é‡‡æ ·æ•°æ®80/20åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
        np.random.shuffle(all_sampled_data)
        split_index = int(0.8 * len(all_sampled_data))
        training_data_array = all_sampled_data[:split_index]
        test_data_array = all_sampled_data[split_index:]
        
        print(f"INFO: (DataLoader) âœ… Split data into {len(training_data_array)} training points and {len(test_data_array)} test points.")
        
        # åŒæ—¶è¿”å›åŒ…å«è¾¹ç•Œç­‰å…ƒæ•°æ®çš„ dose_data å­—å…¸
        return training_data_array, test_data_array, dose_data

class GPUKriging:
    """
    [çœŸå®å®ç°] GPUåŠ é€Ÿçš„å…‹é‡Œé‡‘æ¨¡å‹çš„é€‚é…å™¨ã€‚
    è¯¥å®ç°å€Ÿé‰´äº† ComposeTools.py ä¸­çš„ KrigingAdapterï¼Œå¹¶è°ƒç”¨äº† myKriging åº“ã€‚
    """
    def __init__(self, variogram_model='exponential', **kwargs):
        """
        åˆå§‹åŒ–Krigingé€‚é…å™¨ã€‚
        
        Args:
            variogram_model (str): å…‹é‡Œé‡‘æ‰€éœ€çš„å˜å¼‚å‡½æ•°æ¨¡å‹ã€‚
            **kwargs: å…¶ä»–å¯ä»¥ä¼ é€’ç»™ myKriging åº“çš„å‚æ•° (å¦‚ nlags, block_size)ã€‚
        """
        self.model = None
        self._is_fitted = False
        self.variogram_model = variogram_model
        self.kriging_params = kwargs
        print(f"INFO: (GPUKriging) Initialized with variogram model: {self.variogram_model}")

    def fit(self, points: np.ndarray, values: np.ndarray):
        """
        ä½¿ç”¨ç¨€ç–çš„ç‚¹åæ ‡å’Œå¯¹åº”çš„æ®‹å·®å€¼æ¥è®­ç»ƒå…‹é‡Œé‡‘ä»£ç†æ¨¡å‹ã€‚
        """
        print(f"INFO: (GPUKriging) Fitting model with {len(points)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df = pd.DataFrame({
            'x': points[:, 0],
            'y': points[:, 1],
            'z': points[:, 2],
            'target': values
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_training å‡½æ•°
        self.model = kriging_training(
            df=df,
            variogram_model=self.variogram_model,
            nlags=self.kriging_params.get('nlags', 8),
            enable_plotting=False, # è®­ç»ƒä»£ç†æ¨¡å‹æ—¶é€šå¸¸ä¸ç»˜å›¾
            weight=False,
            uk=False,
            cpu_on=False # ç¡®ä¿ä½¿ç”¨GPU
        )

        self._is_fitted = True
        print("INFO: (GPUKriging) âœ… Model fitted.")

    def predict(self, points_to_predict: np.ndarray) -> np.ndarray:
        """
        å¯¹æ–°çš„ç‚¹è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼Œåˆ©ç”¨GPUåŠ é€Ÿã€‚
        """
        if not self._is_fitted:
            raise RuntimeError("Kriging model must be fitted before prediction.")
        
        print(f"INFO: (GPUKriging) Predicting values for {len(points_to_predict)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df_pred = pd.DataFrame({
            'x': points_to_predict[:, 0],
            'y': points_to_predict[:, 1],
            'z': points_to_predict[:, 2],
            'target': np.zeros(points_to_predict.shape[0]) # è™šæ‹Ÿç›®æ ‡å€¼
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_testing å‡½æ•°ï¼Œç¡®ä¿ä½¿ç”¨GPUåŠ é€Ÿé…ç½®
        predictions, _ = kriging_testing(
            df=df_pred,
            model=self.model,
            block_size=self.kriging_params.get('block_size', 10000),
            cpu_on=False, # ç¡®ä¿ä½¿ç”¨GPU
            style="gpu_b", # ä½¿ç”¨GPUæ‰¹å¤„ç†é£æ ¼
            multi_process=False,
            print_time=False,
            torch_ac=False, # ä½¿ç”¨PyTorchåŠ é€Ÿ
            compute_precision=False # é¢„æµ‹æ®‹å·®æ—¶ä¸éœ€è¦ç²¾åº¦è®¡ç®—
        )
        
        print(f"INFO: (GPUKriging) âœ… Prediction complete.")
        return predictions.flatten() # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„

class PINNModel:
    """
    [çœŸå®å®ç°] ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ã€‚
    è¯¥å®ç°è¢«è®¾è®¡ä¸ºå¯ä»å¤–éƒ¨æ§åˆ¶çš„æ¨¡å¼ï¼Œä»¥æ”¯æŒè‡ªé€‚åº”è®­ç»ƒæµç¨‹ã€‚
    """
    def __init__(self, dose_data: dict, training_data: np.ndarray, test_data: np.ndarray, num_collocation_points: int, network_layers=[3, 64, 64, 64, 1], lr=1e-3):
        """
        åˆå§‹åŒ–PINNæ¨¡å‹ï¼Œä½†ä¸PINNTrainerä¸åŒï¼Œè¿™é‡Œåªåšå‡†å¤‡å·¥ä½œï¼Œä¸å¼€å§‹è®­ç»ƒã€‚
        
        Args:
            dose_data (dict): ä»DataLoaderåŠ è½½çš„æ•°æ®å­—å…¸ã€‚
            training_data (np.ndarray): ç¨€ç–è®­ç»ƒæ•°æ® [x,y,z,value]ã€‚
            test_data (np.ndarray): ç¨€ç–æµ‹è¯•æ•°æ® [x,y,z,value]ã€‚
            num_collocation_points (int): æ±‚è§£åŸŸç‚¹çš„æ•°é‡ã€‚
            network_layers (list): ç¥ç»ç½‘ç»œç»“æ„ã€‚
            lr (float): å­¦ä¹ ç‡ã€‚
        """
        print("INFO: (PINNModel) Initializing a DeepXDE-based model for external control...")
        
        self.test_data_linear = test_data # å­˜å‚¨çº¿æ€§å°ºåº¦çš„æµ‹è¯•æ•°æ®
        
        # 1. å®šä¹‰å‡ ä½•
        world_min = dose_data['world_min']
        world_max = dose_data['world_max']
        self.geometry = dde.geometry.Cuboid(world_min, world_max)

        # 2. å®šä¹‰å¯è®­ç»ƒå‚æ•°
        k_initial_guess = 1.0 
        self.log_k_pinn = dde.Variable(np.log(k_initial_guess))
        
        # 3. å®šä¹‰PDEä¸ºç±»çš„æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å…¶ä»–åœ°æ–¹å¤ç”¨
        self.pde = self._build_pde_func()
        
        # 4. å®šä¹‰è®­ç»ƒæ•°æ®ç‚¹
        observe_x = training_data[:, :3]
        observe_y = np.log(np.maximum(training_data[:, 3:], 1e-30))
        data_points = dde.icbc.PointSetBC(observe_x, observe_y, component=0)
        
        # 5. ç»„åˆæˆdde.data.PDEå¯¹è±¡
        self.data = dde.data.PDE(
            self.geometry,
            self.pde,
            [data_points],
            num_domain=num_collocation_points,
            anchors=observe_x,
            # æˆ‘ä»¬è‡ªå®šä¹‰çš„æŒ‡æ ‡ä¼šä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„self.test_data_linear
        )
        
        # 6. åˆ›å»ºç½‘ç»œå’Œæ¨¡å‹
        self.net = dde.nn.FNN(network_layers, "tanh", "Glorot normal")
        self.model = dde.Model(self.data, self.net)
        
        # 7. å®šä¹‰è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°
        def mean_relative_error_metric(y_true_ignored, y_pred_ignored):
            """
            ä¸€ä¸ª"hack"çš„æŒ‡æ ‡å‡½æ•°ã€‚å®ƒå¿½ç•¥ddeä¼ å…¥çš„å‚æ•°ï¼Œ
            è½¬è€Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„ã€åŸºäºçœŸå®ç‰©ç†å€¼çš„æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚
            """
            # ä½¿ç”¨æ¨¡å‹å¯¹æˆ‘ä»¬è‡ªå·±çš„æµ‹è¯•ç‚¹è¿›è¡Œé¢„æµ‹
            test_x = self.test_data_linear[:, :3]
            pred_y_log = self.model.predict(test_x)
            
            # å°†é¢„æµ‹å€¼å’ŒçœŸå®å€¼éƒ½è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
            pred_y_linear = np.exp(pred_y_log)
            true_y_linear = self.test_data_linear[:, 3:]
            
            # è®¡ç®—ç›¸å¯¹è¯¯å·®
            return np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))

        mean_relative_error_metric.__name__ = "MRE_test_set"

        # 8. ç¼–è¯‘æ¨¡å‹ï¼ŒåŠ å…¥è‡ªå®šä¹‰æŒ‡æ ‡
        self.model.compile(
            "adam", 
            lr=lr, 
            loss_weights=[1, 10], 
            external_trainable_variables=[self.log_k_pinn],
            metrics=[mean_relative_error_metric]
        )
        print("INFO: (PINNModel) âœ… Model compiled and ready for training cycles.")
        
    def _build_pde_func(self):
        """å°†PDEå®šä¹‰å°è£…åœ¨ä¸€ä¸ªå·¥å‚å‡½æ•°ä¸­ï¼Œä»¥æ•è·self.log_k_pinnã€‚"""
        def pde_func(x, u):
            grad_u_sq = dde.grad.jacobian(u, x, i=0, j=0)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=1)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=2)**2
            laplacian_u = dde.grad.hessian(u, x, i=0, j=0) + \
                          dde.grad.hessian(u, x, i=1, j=1) + \
                          dde.grad.hessian(u, x, i=2, j=2)
            k_squared = dde.backend.exp(2 * self.log_k_pinn)
            return grad_u_sq + laplacian_u - k_squared
        return pde_func

    def run_training_cycle(self, epochs: int, collocation_points: np.ndarray):
        """
        [æ–°] æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå‘¨æœŸã€‚
        è¿™ä¸ªæ–¹æ³•å–ä»£äº†æ—§çš„ train_stepï¼Œä»¥å…è®¸deepxdeæ‰“å°è®­ç»ƒçŠ¶æ€ã€‚
        """
        # 1. æ›´æ–°æ±‚è§£åŸŸç‚¹ (Collocation Points)
        num_bc_points = self.data.bcs[0].points.shape[0]
        if self.model.train_state.X_train is None:
             self.model.train(iterations=0)
        start_index = num_bc_points
        end_index = len(self.model.train_state.X_train) - len(self.data.anchors)
        self.model.train_state.X_train[start_index:end_index] = collocation_points
        
        # 2. è°ƒç”¨ deepxde çš„ train å‡½æ•°è¿›è¡ŒæŒ‡å®šæ¬¡æ•°çš„è¿­ä»£
        print(f"INFO: (PINNModel) Starting training cycle for {epochs} epochs...")
        self.model.train(iterations=epochs, display_every=100)

    def compute_pde_residual(self, points: np.ndarray) -> np.ndarray:
        """
        [çœŸå®å®ç°] è®¡ç®—ç»™å®šç‚¹ä¸Šçš„ç‰©ç†æ–¹ç¨‹æ®‹å·®ã€‚
        åˆ©ç”¨ deepxde çš„ model.predict(operator=...) åŠŸèƒ½ã€‚
        """
        print(f"INFO: (PINNModel) Computing PDE residuals for {len(points)} points...")
        
        # deepxde.Model.predict å¯ä»¥æ¥å—ä¸€ä¸ª operator å‚æ•°
        # æˆ‘ä»¬å°† self.pde (åœ¨__init__ä¸­å®šä¹‰çš„å‡½æ•°) ä½œä¸ºç®—å­ä¼ å…¥
        residuals = self.model.predict(points, operator=self.pde)
        
        # è¿”å›æ®‹å·®çš„ç»å¯¹å€¼ï¼Œå¹¶å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        return np.abs(residuals).flatten()

class AdaptiveSampler:
    """
    [çœŸå®å®ç°] è‡ªé€‚åº”é‡‡æ ·å™¨ã€‚
    ç”¨äºæ ¹æ®Krigingçš„é¢„æµ‹ç»“æœç”Ÿæˆæ–°çš„è®­ç»ƒç‚¹ã€‚
    """
    def __init__(self, world_min: np.ndarray, world_max: np.ndarray, total_candidates=100000):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”é‡‡æ ·å™¨ã€‚

        Args:
            world_min (np.ndarray): çœŸå®ç‰©ç†ç©ºé—´çš„æœ€å°åæ ‡ã€‚
            world_max (np.ndarray): çœŸå®ç‰©ç†ç©ºé—´çš„æœ€å¤§åæ ‡ã€‚
            total_candidates (int): åœ¨æ•´ä¸ªåŸŸå†…ç”Ÿæˆçš„å€™é€‰ç‚¹æ•°é‡ã€‚
        """
        self.world_min = world_min
        self.world_max = world_max
        
        # åœ¨æ•´ä¸ªçœŸå®ç‰©ç†åŸŸå†…ç”Ÿæˆå¤§é‡çš„å€™é€‰ç‚¹ï¼Œåç»­ä»ä¸­ç­›é€‰
        self.candidate_points = (np.random.rand(total_candidates, 3) * 
                                 (self.world_max - self.world_min) + self.world_min)
                                 
        print(f"INFO: (AdaptiveSampler) Initialized with {total_candidates} candidate points "
              f"within physical bounds [{world_min.round(2)}, {world_max.round(2)}].")

    def generate_new_collocation_points(
        self,
        kriging_model: GPUKriging,
        num_points_to_sample: int,
        exploration_ratio: float = 0.1
    ) -> np.ndarray:
        """
        ä½¿ç”¨Krigingæ¨¡å‹å¼•å¯¼ç”Ÿæˆæ–°çš„é…ç½®ç‚¹ã€‚
        Args:
            kriging_model: è®­ç»ƒå¥½çš„æ®‹å·®ä»£ç†æ¨¡å‹ã€‚
            num_points_to_sample: éœ€è¦ç”Ÿæˆçš„æ€»ç‚¹æ•°ã€‚
            exploration_ratio: ä»æ€»ç‚¹æ•°ä¸­åˆ†å‡ºå¤šå°‘æ¯”ä¾‹ç”¨äºéšæœºæ¢ç´¢ã€‚
        Returns:
            np.ndarray: æ–°çš„é…ç½®ç‚¹é›†ã€‚
        """
        # 1. ä½¿ç”¨Krigingä»£ç†æ¨¡å‹é¢„æµ‹æ‰€æœ‰å€™é€‰ç‚¹çš„æ®‹å·®
        predicted_residuals = kriging_model.predict(self.candidate_points)

        # 2. "Hard-Case Mining": æ‰¾åˆ°é¢„æµ‹æ®‹å·®æœ€å¤§çš„ç‚¹çš„ç´¢å¼•
        num_exploitation_points = int(num_points_to_sample * (1 - exploration_ratio))
        hard_case_indices = np.argsort(predicted_residuals)[-num_exploitation_points:]
        exploitation_points = self.candidate_points[hard_case_indices]

        # 3. "Exploration": åŠ å…¥ä¸€éƒ¨åˆ†éšæœºç‚¹ä»¥é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜
        num_exploration_points = num_points_to_sample - num_exploitation_points
        random_indices = np.random.choice(len(self.candidate_points), num_exploration_points, replace=False)
        exploration_points = self.candidate_points[random_indices]

        print(f"INFO: (AdaptiveSampler) Generated {num_exploitation_points} exploitation points and {num_exploration_points} exploration points.")
        
        return np.vstack([exploitation_points, exploration_points])


def main():
    """
    ä¸»å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ª"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"æµç¨‹ã€‚
    """
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œï¼šå…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ")
    print("="*60 + "\n")

    # --- 1. åˆå§‹åŒ– ---
    # !! æ³¨æ„: DOMAIN_BOUNDS ç°åœ¨ä»…ç”¨äºå¯è§†åŒ–æˆ–é‡‡æ ·å™¨ï¼Œå®é™…ç‰©ç†è¾¹ç•Œç”±åŠ è½½çš„æ•°æ®å†³å®š !!
    DOMAIN_BOUNDS = np.array([[0., 0., 0.], [1., 1., 1.]]) 
    TOTAL_EPOCHS = 5000
    ADAPTIVE_CYCLE_EPOCHS = 1000  # æ¯å¤šå°‘ä¸ªepochæ‰§è¡Œä¸€æ¬¡è‡ªé€‚åº”è°ƒæ•´
    
    # --- æ•°æ®åŠ è½½å‚æ•° ---
    DATA_PATH = "PINN/DATA.xlsx"
    SPACE_DIMS = np.array([20.0, 10.0, 10.0])
    NUM_SAMPLES = 300
    
    # --- æ¨¡å‹è®­ç»ƒå‚æ•° ---
    NUM_COLLOCATION_POINTS = 4096
    NUM_RESIDUAL_SCOUT_POINTS = 500 # ç”¨äºä¾¦å¯Ÿçš„ç‚¹æ•°ï¼Œè¿œå°‘äºè®­ç»ƒç‚¹æ•°

    # 1. æ•°æ®åŠ è½½
    data_loader = DummyDataLoader(
        data_path=DATA_PATH,
        space_dims=SPACE_DIMS,
        num_samples=NUM_SAMPLES
    )
    training_data, test_data, dose_data = data_loader.get_training_data()

    # 2. æ¨¡å‹å’Œé‡‡æ ·å™¨åˆå§‹åŒ–
    pinn = PINNModel(
        dose_data=dose_data, 
        training_data=training_data,
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS
    )
    kriging = GPUKriging()
    
    # ä» dose_data ä¸­è·å–çœŸå®çš„ç‰©ç†è¾¹ç•Œ
    world_min = dose_data['world_min']
    world_max = dose_data['world_max']
    
    sampler = AdaptiveSampler(world_min, world_max)
    
    # åˆå§‹é…ç½®ç‚¹ï¼šåœ¨çœŸå®ç‰©ç†ç©ºé—´å†…é‡‡æ ·
    current_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                  (world_max - world_min) + world_min)

    # --- 3. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(0, TOTAL_EPOCHS, ADAPTIVE_CYCLE_EPOCHS):
        
        print(f"\n--- ä¸»å¾ªç¯å‘¨æœŸ: Epochs [{epoch} - {epoch + ADAPTIVE_CYCLE_EPOCHS - 1}] ---")

        # 2a. ä½¿ç”¨å½“å‰çš„é…ç½®ç‚¹ï¼Œå¯¹PINNè¿›è¡Œä¸€è½®å¸¸è§„è®­ç»ƒ
        print(f"PHASE 2a: å¸¸è§„PINNè®­ç»ƒ...")
        pinn.run_training_cycle(
            epochs=ADAPTIVE_CYCLE_EPOCHS,
            collocation_points=current_collocation_points
        )
        
        print("\nPHASE 2b: å¼€å§‹å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ ·...")
        
        # 2b. æ®‹å·®"ä¾¦å¯Ÿ"ï¼šç”¨å½“å‰PINNè®¡ç®—ä¸€å°æ‰¹éšæœºç‚¹çš„çœŸå®æ®‹å·®
        scout_points = (np.random.rand(NUM_RESIDUAL_SCOUT_POINTS, 3) *
                        (world_max - world_min) + world_min)
        true_residuals = pinn.compute_pde_residual(scout_points)
        
        # 2c. å…‹é‡Œé‡‘ä»£ç†å»ºæ¨¡ï¼šè®­ç»ƒKrigingæ¨¡å‹æ¥æ‹Ÿåˆæ®‹å·®åˆ†å¸ƒ
        kriging.fit(scout_points, true_residuals)

        # 2d. è‡ªé€‚åº”é‡‡æ ·ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„Krigingæ¨¡å‹ç”Ÿæˆä¸‹ä¸€æ‰¹"æ›´èªæ˜"çš„é…ç½®ç‚¹
        current_collocation_points = sampler.generate_new_collocation_points(
            kriging_model=kriging,
            num_points_to_sample=NUM_COLLOCATION_POINTS
        )
        print("PHASE 2b: âœ… æ–°çš„è‡ªé€‚åº”é…ç½®ç‚¹å·²ç”Ÿæˆã€‚")

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

if __name__ == "__main__":
    main() 