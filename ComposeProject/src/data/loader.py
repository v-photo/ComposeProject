"""
æ•°æ®åŠ è½½ä¸å¤„ç†æ¨¡å—
Module for data loading and processing.
"""
import numpy as np
import sys
from pathlib import Path
from sklearn.model_selection import train_test_split
from typing import List, Dict, Tuple, Optional, Any
import pandas as pd
import os

# æˆ‘ä»¬çš„æ–°ç¯å¢ƒæ¨¡å—ä¼šå¤„ç†è·¯å¾„å’Œä¾èµ–æ£€æŸ¥
# This centralized approach is cleaner.
from ..utils.environment import PINN_AVAILABLE

if not PINN_AVAILABLE:
    # å¦‚æœç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œç«‹å³æŠ›å‡ºå¼‚å¸¸
    raise ImportError("PINN æ¨¡å—æ— æ³•åŠ è½½ã€‚è¯·æ£€æŸ¥é¡¹ç›®ç»“æ„å’Œä¾èµ–ã€‚")

# æ—¢ç„¶PINN_AVAILABLEä¸ºTrueï¼Œè¯´æ˜è·¯å¾„å·²è®¾ç½®ï¼Œå¯ä»¥ç›´æ¥å¯¼å…¥
from data_processing import DataLoader
from dataAnalysis import get_data


class AdaptiveDataLoader:
    """
    ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»å¤–éƒ¨æ–‡ä»¶åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®ï¼Œå¹¶æ”¯æŒçµæ´»çš„æ•°æ®é›†åˆ†å‰²ç­–ç•¥ã€‚
    (åŸå DummyDataLoader)
    """
    def __init__(self, data_path: str, space_dims: np.ndarray, num_samples: int):
        self.data_path = data_path
        self.space_dims = space_dims
        self.num_samples = num_samples
        print(f"INFO: (DataLoader) Initialized with data_path='{self.data_path}'")

    def get_training_data(self, split_ratios: Optional[List[float]] = None, test_set_size: Optional[int] = None) -> Tuple[np.ndarray, List[np.ndarray], np.ndarray, Dict]:
        """
        åŠ è½½ã€å¤„ç†å¹¶é‡‡æ ·ç¨€ç–è®­ç»ƒç‚¹ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ¯”ä¾‹åˆ—è¡¨è¿›è¡Œåˆ†å‰²ã€‚

        Args:
            split_ratios (list, optional): ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œå…¶å’Œåº”å°äº1ã€‚
                ä¾‹å¦‚ [0.7, 0.1, 0.1] ä»£è¡¨ï¼š
                - 70% ä½œä¸ºä¸»è®­ç»ƒé›†
                - 10% ä½œä¸ºç¬¬ä¸€ä¸ªå‚¨å¤‡é›†
                - 10% ä½œä¸ºç¬¬äºŒä¸ªå‚¨å¤‡é›†
                - å‰©ä½™çš„ 10% å°†ä½œä¸ºæµ‹è¯•é›†ã€‚
                å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ 80/20 è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ã€‚
            test_set_size (int, optional): å¦‚æœæŒ‡å®šï¼Œå°†ç”Ÿæˆç‹¬ç«‹çš„æµ‹è¯•é›†è€Œéä»è®­ç»ƒæ•°æ®åˆ†å‰²ã€‚
        
        Returns:
            - main_train_set (np.ndarray)
            - reserve_pools (List[np.ndarray])
            - test_set (np.ndarray)
            - dose_data (Dict)
        """
        print(f"INFO: (DataLoader) Loading raw data from {self.data_path}...")
        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            
        raw_data = get_data(self.data_path)
        
        print("INFO: (DataLoader) Normalizing dose data...")
        dose_data = DataLoader.load_dose_from_dict(
            data_dict=raw_data,
            space_dims=self.space_dims
        )
        
        print(f"INFO: (DataLoader) Sampling {self.num_samples} training points...")
        train_points, train_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=self.num_samples,
            sampling_strategy='positive_only',
        )
        print(f"INFO: (DataLoader) âœ… Successfully sampled {len(train_points)} points.")

        # å°†åæ ‡å’Œå€¼åˆå¹¶æˆ [x, y, z, value] æ ¼å¼
        all_sampled_data = np.hstack([train_points, train_values.reshape(-1, 1)])
        
        # [æ–°å¢] ç”Ÿæˆç‹¬ç«‹æµ‹è¯•é›†ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if test_set_size is not None:
            print(f"INFO: (DataLoader) Generating independent test set of size {test_set_size}...")
            test_set = self._generate_independent_test_set(dose_data, test_set_size)
        else:
            test_set = None  # å°†åœ¨ä¸‹é¢çš„åˆ†å‰²é€»è¾‘ä¸­å¤„ç†
        
        # [æ–°é€»è¾‘] ä½¿ç”¨å¯é…ç½®çš„åˆ†å‰²ç­–ç•¥
        if split_ratios is None:
            # é»˜è®¤è¡Œä¸ºï¼š80/20 åˆ†å‰²
            if test_set is None:
                main_train_set, test_set = train_test_split(all_sampled_data, test_size=0.2, random_state=42)
            else:
                main_train_set = all_sampled_data  # å…¨éƒ¨ç”¨ä½œè®­ç»ƒæ•°æ®
            reserve_pools = []
        else:
            if test_set is None and sum(split_ratios) >= 1.0:
                raise ValueError("split_ratios çš„æ€»å’Œå¿…é¡»å°äº 1.0ï¼Œä»¥ä¾¿ä¸ºæµ‹è¯•é›†ç•™å‡ºç©ºé—´ã€‚")

            remaining_data = all_sampled_data
            data_pools = []
            
            # å¾ªç¯åˆ‡åˆ†å‡ºä¸»è®­ç»ƒé›†å’Œæ‰€æœ‰å‚¨å¤‡é›†
            current_total_fraction = 1.0
            for ratio in split_ratios:
                # è®¡ç®—å½“å‰æ¯”ä¾‹ç›¸å¯¹äºå‰©ä½™æ•°æ®é‡çš„æ¯”ä¾‹
                split_fraction = ratio / current_total_fraction
                
                # [ä¿®å¤] æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
                test_size_fraction = 1.0 - split_fraction
                if test_size_fraction < 1e-10:
                    test_size_fraction = 0.0
                elif test_size_fraction > 1.0:
                    test_size_fraction = 1.0
                
                if test_size_fraction == 0.0:
                    pool = remaining_data
                    remaining_data = np.array([]).reshape(0, remaining_data.shape[1]) if len(remaining_data) > 0 else np.array([])
                else:
                    pool, remaining_data = train_test_split(remaining_data, test_size=test_size_fraction, random_state=42)
                
                data_pools.append(pool)
                current_total_fraction -= ratio

            main_train_set = data_pools[0]
            reserve_pools = data_pools[1:]
            
            # å¦‚æœæ²¡æœ‰ç‹¬ç«‹æµ‹è¯•é›†ï¼Œåˆ™ä½¿ç”¨å‰©ä½™æ•°æ®
            if test_set is None:
                if len(remaining_data) == 0 and len(all_sampled_data) > 0:
                     # å¦‚æœç»è¿‡åˆ†å‰²åæ²¡æœ‰å‰©ä¸‹ä»»ä½•æ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ½œåœ¨é—®é¢˜
                    print("WARNING: No data left for the test set after splitting according to split_ratios.")
                    # æ ¹æ®åœºæ™¯ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç©ºçš„æµ‹è¯•é›†æˆ–æŠ›å‡ºé”™è¯¯
                    test_set = np.array([]).reshape(0, all_sampled_data.shape[1])
                else:
                    test_set = remaining_data
        
        print(f"INFO: (DataLoader) âœ… Split data into: Main training ({len(main_train_set)}), Test ({len(test_set)}), Reserve Pools ({len(reserve_pools)} pools).")
        if reserve_pools:
            for i, pool in enumerate(reserve_pools):
                print(f"    - Reserve Pool {i+1}: {len(pool)} points")

        return main_train_set, reserve_pools, test_set, dose_data

    def _generate_independent_test_set(self, dose_data: dict, test_set_size: int) -> np.ndarray:
        """
        ç”Ÿæˆå®Œå…¨ç‹¬ç«‹äºè®­ç»ƒæ•°æ®çš„æµ‹è¯•é›†ï¼Œåœ¨æ•´ä¸ªç‰©ç†åŸŸå†…å‡åŒ€é‡‡æ ·ã€‚
        """
        # ä½¿ç”¨ DataLoader.sample_training_points åœ¨æ•´ä¸ªåŸŸå†…é‡‡æ ·æµ‹è¯•ç‚¹
        test_points, test_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=test_set_size,
            sampling_strategy='uniform',  # ä½¿ç”¨å‡åŒ€é‡‡æ ·
        )
        
        # åˆå¹¶ä¸º [x, y, z, value] æ ¼å¼
        test_set = np.hstack([test_points, test_values.reshape(-1, 1)])
        print(f"INFO: (DataLoader) âœ… Generated independent test set with {len(test_set)} points.")
        return test_set

def load_data_from_xlsx(
    file_path: str,
    column_map: Dict[str, str],
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:
    """
    ä» .xlsx æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œå¹¶å°†å…¶åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

    Args:
        file_path (str): .xlsxæ–‡ä»¶çš„è·¯å¾„ã€‚
        column_map (Dict[str, str]): å°†æ ‡å‡†åç§°æ˜ å°„åˆ°æ–‡ä»¶ä¸­å®é™…åˆ—åçš„å­—å…¸ã€‚
                                     éœ€è¦åŒ…å« 'x', 'y', 'z', 'value'ã€‚
        test_size (float): ç”¨äºæµ‹è¯•é›†çš„æ•°æ®æ¯”ä¾‹ã€‚
        random_state (int): éšæœºç§å­ï¼Œç”¨äºå¯å¤ç°çš„åˆ’åˆ†ã€‚

    Returns:
        A tuple containing:
        - train_points (np.ndarray): è®­ç»ƒç‚¹çš„åæ ‡ (N, 3)ã€‚
        - train_values (np.ndarray): è®­ç»ƒç‚¹çš„å€¼ (N, 1)ã€‚
        - test_data (np.ndarray): æµ‹è¯•æ•°æ®ï¼ŒåŒ…å«åæ ‡å’Œå€¼ (M, 4)ã€‚
        - dose_data (Dict): åŒ…å«æ•°æ®è¾¹ç•Œä¿¡æ¯çš„å­—å…¸ã€‚
    """
    print(f"\n--- ğŸ’¾ æ­£åœ¨ä» {file_path} åŠ è½½æ•°æ® ---")
    
    try:
        df = pd.read_excel(file_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°äº '{file_path}'ã€‚è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    except Exception as e:
        raise IOError(f"é”™è¯¯: è¯»å– {file_path} æ—¶å‡ºé”™: {e}")

    # æ ¹æ®æ˜ å°„é‡å‘½ååˆ—
    try:
        df = df.rename(columns={v: k for k, v in column_map.items()})
        required_cols = {'x', 'y', 'z', 'value'}
        if not required_cols.issubset(df.columns):
            missing = required_cols - set(df.columns)
            raise ValueError(f"é”™è¯¯: æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing} (å·²æ ¹æ®åˆ—æ˜ å°„é‡å‘½å)")
    except Exception as e:
         raise ValueError(f"é”™è¯¯: åº”ç”¨åˆ—æ˜ å°„æ—¶å‡ºé”™: {e}")

    all_points = df[['x', 'y', 'z']].values
    all_values = df[['value']].values

    # åˆ’åˆ†æ•°æ®
    train_points, test_points, train_values, test_values = train_test_split(
        all_points, all_values, test_size=test_size, random_state=random_state
    )
    
    test_data = np.hstack([test_points, test_values])

    # è®¡ç®—æ•°æ®è¾¹ç•Œ
    dose_data = {
        'world_min': all_points.min(axis=0),
        'world_max': all_points.max(axis=0),
        'space_dims': all_points.max(axis=0) - all_points.min(axis=0)
    }
    
    print(f"  âœ… æ•°æ®åŠ è½½å®Œæ¯•: {len(train_points)}ä¸ªè®­ç»ƒç‚¹, {len(test_points)}ä¸ªæµ‹è¯•ç‚¹ã€‚")
    
    return train_points, train_values.flatten(), test_data, dose_data

def load_and_process_data(file_path: str, column_map: Dict[str, str]) -> Dict[str, Any]:
    """
    ä» .xlsx æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®ï¼Œå¹¶å°†å…¶å¤„ç†æˆåŒ…å«å‰‚é‡ç½‘æ ¼å’Œç©ºé—´ä¿¡æ¯çš„æ ‡å‡†åŒ–å­—å…¸ã€‚

    Args:
        file_path (str): .xlsxæ–‡ä»¶çš„è·¯å¾„ã€‚
        column_map (Dict[str, str]): æ ‡å‡†åç§°åˆ°æ–‡ä»¶ä¸­å®é™…åˆ—åçš„æ˜ å°„ã€‚

    Returns:
        ä¸€ä¸ªåŒ…å«å®Œæ•´æ•°æ®ä¿¡æ¯çš„å­—å…¸ (dose_data)ã€‚
    """
    print(f"\n--- ğŸ’¾ æ­£åœ¨ä» {file_path} åŠ è½½å¹¶å¤„ç†æ•°æ® ---")
    
    try:
        df = pd.read_excel(file_path, header=0)
    except FileNotFoundError:
        raise FileNotFoundError(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°äº '{file_path}'ã€‚")
    
    df = df.rename(columns={v: k for k, v in column_map.items()})
    required_cols = {'x', 'y', 'z', 'value'}
    if not required_cols.issubset(df.columns):
        missing = required_cols - set(df.columns)
        raise ValueError(f"é”™è¯¯: æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing}")

    # ä»æ•°æ®ä¸­æ¨æ–­ç½‘æ ¼ç»“æ„
    x_coords = np.unique(df['x'])
    y_coords = np.unique(df['y'])
    z_coords = np.unique(df['z'])
    
    grid_shape = (len(x_coords), len(y_coords), len(z_coords))
    
    # ç¡®ä¿æ•°æ®ç‚¹æ•°é‡ä¸ç½‘æ ¼å¤§å°åŒ¹é…
    if len(df) != np.prod(grid_shape):
        raise ValueError(f"æ•°æ®ç‚¹æ€»æ•° ({len(df)})ä¸æ¨æ–­çš„ç½‘æ ¼å¤§å° {grid_shape} ä¸åŒ¹é…ã€‚")
        
    # å°†DataFrameé‡å¡‘ä¸º3Då‰‚é‡ç½‘æ ¼
    df_sorted = df.sort_values(by=['z', 'y', 'x'])
    dose_grid = df_sorted['value'].values.reshape(grid_shape, order='F')

    world_min = df[['x', 'y', 'z']].min().values
    world_max = df[['x', 'y', 'z']].max().values
    
    dose_data = {
        'dose_grid': dose_grid,
        'world_min': world_min,
        'world_max': world_max,
        'space_dims': world_max - world_min,
        'voxel_size': (world_max - world_min) / (np.array(grid_shape) - 1),
        'grid_shape': grid_shape
    }
    
    print(f"  âœ… æ•°æ®å¤„ç†å®Œæ¯•ã€‚ç½‘æ ¼å°ºå¯¸: {grid_shape}ã€‚")
    return dose_data

def load_3d_data_from_sheets(
    file_path: str,
    sheet_name_template: str,
    use_cols: str,
    z_size: int,
    y_size: int
) -> np.ndarray:
    """
    ä»ä¸€ä¸ªExcelæ–‡ä»¶çš„å¤šä¸ªsheetsä¸­åŠ è½½æ•°æ®ï¼Œå¹¶å°†å…¶ç»„è£…æˆä¸€ä¸ª3D Numpyæ•°ç»„ã€‚
    æ­¤å‡½æ•°å¤ç°äº† 'PINN/dataAnalysis.py' ä¸­ get_data çš„æ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…æ‹¬Pickleç¼“å­˜ã€‚

    è¿”å›:
        ä¸€ä¸ªä¸‰ç»´Numpyæ•°ç»„ï¼Œä»£è¡¨å‰‚é‡ç½‘æ ¼ (dose_grid)ã€‚
    """
    print(f"\n--- ğŸ’¾ æ­£åœ¨ä» {file_path} çš„å¤šä¸ªSheetsåŠ è½½3Dæ•°æ® ---")
    
    p = Path(file_path)
    cache_dir = p.parent / f"{p.stem}_pkl_data"
    cache_dir.mkdir(exist_ok=True)
    
    data_sheets = {}
    use_cache = all((cache_dir / f"pkl{z}.pkl").exists() for z in range(z_size))

    if use_cache:
        print(f"  - æ­£åœ¨ä»ç¼“å­˜ç›®å½•åŠ è½½: {cache_dir}")
        for z in range(z_size):
            df = pd.read_pickle(cache_dir / f"pkl{z}.pkl")
            data_sheets[z] = df.values
    else:
        print(f"  - æ­£åœ¨ä»Excelæ–‡ä»¶è¯»å–å¹¶åˆ›å»ºç¼“å­˜...")
        for z in range(z_size):
            sheet_name = sheet_name_template.replace("z", str(z + 1))
            try:
                df = pd.read_excel(
                    file_path,
                    sheet_name=sheet_name,
                    header=None,
                    usecols=use_cols,
                    names=list(range(y_size))
                )
                pd.to_pickle(df, cache_dir / f"pkl{z}.pkl")
                data_sheets[z] = df.values
                if z % 10 == 0: print(f"    ...å·²å¤„ç† {z+1}/{z_size} ä¸ªsheets")
            except Exception as e:
                raise IOError(f"è¯»å– Sheet '{sheet_name}' æ—¶å‡ºé”™: {e}")

    # å°†æ‰€æœ‰2Dåˆ‡ç‰‡å †å æˆä¸€ä¸ª3Dæ•°ç»„
    # åŸå§‹æ•°æ®[z][y][x]ï¼Œæˆ‘ä»¬éœ€è¦[x][y][z]
    dose_grid_zyx = np.stack(list(data_sheets.values()), axis=0)
    dose_grid = np.transpose(dose_grid_zyx, (2, 1, 0))
    
    print(f"  âœ… 3Dæ•°æ®åŠ è½½å®Œæ¯•ã€‚æœ€ç»ˆç½‘æ ¼å°ºå¯¸: {dose_grid.shape}")
    return dose_grid

def process_grid_to_dose_data(
    dose_grid: np.ndarray, 
    space_dims: Tuple[float, float, float] = (20.0, 10.0, 10.0) # å‡è®¾å€¼ï¼Œåº”ä»é…ç½®è·å–
) -> Dict[str, Any]:
    """
    å°†åŠ è½½çš„å‰‚é‡ç½‘æ ¼å¤„ç†æˆåŒ…å«ç‰©ç†ç©ºé—´ä¿¡æ¯çš„æ ‡å‡†åŒ–å­—å…¸ã€‚
    """
    grid_shape = np.array(dose_grid.shape)
    
    # å‡è®¾åŸç‚¹åœ¨[0,0,0]
    world_min = np.array([0., 0., 0.])
    world_max = np.array(space_dims)

    dose_data = {
        'dose_grid': dose_grid,
        'world_min': world_min,
        'world_max': world_max,
        'space_dims': world_max - world_min,
        'voxel_size': (world_max - world_min) / (grid_shape - 1),
        'grid_shape': grid_shape
    }
    return dose_data

def sample_training_points(
    dose_data: Dict, num_samples: int, strategy: str = 'positive_only'
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä»å¤„ç†è¿‡çš„æ•°æ®ä¸­é‡‡æ ·è®­ç»ƒç‚¹ã€‚
    """
    if strategy != 'positive_only':
        raise NotImplementedError("ç›®å‰ä»…æ”¯æŒ 'positive_only' é‡‡æ ·ç­–ç•¥ã€‚")

    positive_indices = np.argwhere(dose_data['dose_grid'] > 1e-10) # ä»…åœ¨æœ‰å‰‚é‡å¤„é‡‡æ ·
    
    if len(positive_indices) < num_samples:
        print(f"è­¦å‘Š: è¯·æ±‚é‡‡æ · {num_samples} ç‚¹, ä½†åªæœ‰ {len(positive_indices)} ä¸ªæ­£å‰‚é‡ç‚¹å¯ç”¨ã€‚")
        num_samples = len(positive_indices)

    sample_indices = positive_indices[
        np.random.choice(len(positive_indices), num_samples, replace=False)
    ]
    
    # å°†ç½‘æ ¼ç´¢å¼•è½¬æ¢å›ä¸–ç•Œåæ ‡
    train_points = dose_data['world_min'] + sample_indices * dose_data['voxel_size']
    train_values = dose_data['dose_grid'][
        sample_indices[:, 0], sample_indices[:, 1], sample_indices[:, 2]
    ]

    return train_points, train_values.reshape(-1, 1)

def create_prediction_grid(dose_data: Dict, downsample_factor: int = 1) -> np.ndarray:
    """
    æ ¹æ®é™é‡‡æ ·ç³»æ•°åˆ›å»ºç”¨äºå…¨åœºé¢„æµ‹çš„åæ ‡ç½‘æ ¼ã€‚
    """
    grid_shape = dose_data['grid_shape']
    
    if downsample_factor > 1:
        print(f"âš ï¸  é¢„æµ‹ç½‘æ ¼å°†ä»¥ç³»æ•° {downsample_factor} è¿›è¡Œé™é‡‡æ ·ã€‚")
        step = int(downsample_factor)
        x_indices = np.arange(0, grid_shape[0], step)
        y_indices = np.arange(0, grid_shape[1], step)
        z_indices = np.arange(0, grid_shape[2], step)
    else:
        x_indices = np.arange(grid_shape[0])
        y_indices = np.arange(grid_shape[1])
        z_indices = np.arange(grid_shape[2])

    pred_x = dose_data['world_min'][0] + x_indices * dose_data['voxel_size'][0]
    pred_y = dose_data['world_min'][1] + y_indices * dose_data['voxel_size'][1]
    pred_z = dose_data['world_min'][2] + z_indices * dose_data['voxel_size'][2]
    
    XX, YY, ZZ = np.meshgrid(pred_x, pred_y, pred_z, indexing='ij')
    prediction_points = np.vstack([XX.ravel(), YY.ravel(), ZZ.ravel()]).T
    
    return prediction_points


# ==================== Kriging é£æ ¼é‡‡æ ·æ‰©å±• ====================

def sample_kriging_style(
    dose_data: Dict,
    box_origin: List[int] = [5, 5, 5],
    box_extent: List[int] = [90, 90, 90],
    step_sizes: List[int] = [5],
    source_positions: List[List[int]] = None,
    source_exclusion_radius: float = 30.0,
    return_dataframe: bool = False
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨Krigingé£æ ¼çš„ç»“æ„åŒ–ç½‘æ ¼é‡‡æ ·ï¼ˆä¸ Kriging/dataAnalysis.py çš„ training_sampling ä¸€è‡´ï¼‰
    
    æ­¤å‡½æ•°æ˜¯å¯¹ unified_sampling.py çš„ç®€åŒ–å°è£…ï¼Œæä¾›ä¸ sample_training_points ä¸€è‡´çš„æ¥å£ã€‚
    
    Args:
        dose_data: PINNæ ¼å¼çš„æ•°æ®å­—å…¸
        box_origin: é‡‡æ ·åŒºåŸŸèµ·ç‚¹ [x, y, z] (ç½‘æ ¼ç´¢å¼•)
        box_extent: é‡‡æ ·åŒºåŸŸåœ¨å„æ–¹å‘çš„å»¶ä¼¸é•¿åº¦ [x_len, y_len, z_len]
        step_sizes: é‡‡æ ·æ­¥é•¿åˆ—è¡¨
        source_positions: æºç‚¹ä½ç½®åˆ—è¡¨ï¼Œç”¨äºæ’é™¤æºç‚¹é™„è¿‘åŒºåŸŸ
        source_exclusion_radius: æºç‚¹æ’é™¤åŠå¾„
        return_dataframe: æ˜¯å¦åŒæ—¶è¿”å›DataFrame
        
    Returns:
        (train_points, train_values) æˆ– (train_points, train_values, df) å¦‚æœ return_dataframe=True
    """
    dose_grid = dose_data['dose_grid']
    world_min = dose_data.get('world_min', np.zeros(3))
    voxel_size = dose_data.get('voxel_size', np.ones(3))
    grid_shape = np.array(dose_grid.shape)
    
    if source_positions is None:
        source_positions = []
    
    sampled_data = []
    
    for step in step_sizes:
        x_range = box_extent[0] // step
        y_range = box_extent[1] // step
        z_range = box_extent[2] // step
        
        for xi in range(0, x_range + 1):
            for yi in range(0, y_range + 1):
                for zi in range(0, z_range + 1):
                    x_coord = box_origin[0] + xi * step
                    y_coord = box_origin[1] + yi * step
                    z_coord = box_origin[2] + zi * step
                    
                    # è¾¹ç•Œæ£€æŸ¥
                    if (x_coord >= grid_shape[0] or y_coord >= grid_shape[1] or 
                        z_coord >= grid_shape[2] or x_coord < 0 or y_coord < 0 or z_coord < 0):
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤æºç‚¹é™„è¿‘
                    skip = False
                    for pos in source_positions:
                        distance = np.sqrt((x_coord - pos[0])**2 + 
                                         (y_coord - pos[1])**2 + 
                                         (z_coord - pos[2])**2)
                        if distance <= source_exclusion_radius:
                            skip = True
                            break
                    
                    if skip:
                        continue
                    
                    value = dose_grid[x_coord, y_coord, z_coord]
                    if value > 1e-10:  # åªé‡‡æ ·æ­£å‰‚é‡ç‚¹
                        sampled_data.append((x_coord, y_coord, z_coord, value))
    
    if len(sampled_data) == 0:
        raise ValueError("Krigingé£æ ¼é‡‡æ ·æœªèƒ½è·å–åˆ°ä»»ä½•æœ‰æ•ˆç‚¹ï¼Œè¯·æ£€æŸ¥å‚æ•°è®¾ç½®")
    
    # å»é‡
    sampled_data = list(set(sampled_data))
    sampled_data = np.array(sampled_data)
    
    sampled_indices = sampled_data[:, :3].astype(int)
    sampled_values = sampled_data[:, 3]
    
    # è½¬æ¢ä¸ºç‰©ç†åæ ‡
    train_points = world_min + sampled_indices * voxel_size + voxel_size / 2.0
    train_values = sampled_values.reshape(-1, 1)
    
    print(f"Krigingé£æ ¼é‡‡æ ·å®Œæˆ: {len(train_points)} ä¸ªè®­ç»ƒç‚¹")
    
    if return_dataframe:
        df = pd.DataFrame({
            'x': sampled_indices[:, 0],
            'y': sampled_indices[:, 1],
            'z': sampled_indices[:, 2],
            'target': sampled_values
        })
        return train_points, train_values, df
    
    return train_points, train_values
