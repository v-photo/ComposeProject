import numpy as np
import sys
from pathlib import Path
import deepxde as dde
import pandas as pd
import os
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# --- è·¯å¾„è®¾ç½® ---
try:
    current_dir = Path(__file__).parent.resolve()
    project_root = current_dir.parent
except NameError:
    project_root = Path('.').resolve()

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'PINN'))
sys.path.insert(0, str(project_root / 'Kriging'))

# --- æ¨¡å—å¯¼å…¥ ---
try:
    from PINN.data_processing import DataLoader
    from PINN.dataAnalysis import get_data
    from myKriging import training as kriging_training, testing as kriging_testing
    print("âœ… å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥æˆåŠŸã€‚")
except ImportError as e:
    print(f"âŒ å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# =================================================================================
#  å…è´£å£°æ˜ï¼šä»¥ä¸‹ç±»å’Œå‡½æ•°å‡ä¸ºå ä½ç¬¦ (Placeholder)
#  æ‚¨éœ€è¦æ ¹æ®æ‚¨ç°æœ‰çš„ PINN å’Œ Kriging åº“æ¥å¡«å……å®ƒä»¬çš„å…·ä½“å®ç°ã€‚
#  è¿™é‡Œçš„éª¨æ¶æ˜¯ä¸ºäº†æ¸…æ™°åœ°å±•ç¤º"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"è¿™ä¸€æŠ€æœ¯è·¯çº¿ã€‚
# =================================================================================

class DummyDataLoader:
    """
    ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»å¤–éƒ¨æ–‡ä»¶åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®(æ›¿ä»£åŸæœ‰çš„DummyDataLoader)ã€‚
    """
    def __init__(self, data_path: str, space_dims: np.ndarray, num_samples: int):
        self.data_path = data_path
        self.space_dims = space_dims
        self.num_samples = num_samples
        print(f"INFO: (DataLoader) Initialized with data_path='{self.data_path}'")

    def get_training_data(self, split_ratios: list = None):
        """
        åŠ è½½ã€å¤„ç†å¹¶é‡‡æ ·ç¨€ç–è®­ç»ƒç‚¹ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ¯”ä¾‹åˆ—è¡¨è¿›è¡Œåˆ†å‰²ã€‚
        
        Args:
            split_ratios (list, optional): ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œå…¶å’Œåº”å°äº1ã€‚
                ä¾‹å¦‚ [0.7, 0.1, 0.1] ä»£è¡¨ï¼š
                - 70% ä½œä¸ºä¸»è®­ç»ƒé›†
                - 10% ä½œä¸ºç¬¬ä¸€ä¸ªå‚¨å¤‡é›†
                - 10% ä½œä¸ºç¬¬äºŒä¸ªå‚¨å¤‡é›†
                - å‰©ä½™çš„ 10% å°†ä½œä¸ºæµ‹è¯•é›†ã€‚
                å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ 80/20 è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ã€‚
        """
        # ... (å‰é¢åŠ è½½å’Œé‡‡æ ·æ•°æ®çš„éƒ¨åˆ†ä¿æŒä¸å˜) ...
        print(f"INFO: (DataLoader) Loading raw data from {self.data_path}...")
        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            
        raw_data = get_data(self.data_path)
        
        print("INFO: (DataLoader) Normalizing dose data...")
        dose_data = DataLoader.load_dose_from_dict(
            data_dict=raw_data,
            space_dims=self.space_dims
        )
        
        print(f"INFO: (DataLoader) Sampling {self.num_samples} training points...")
        train_points, train_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=self.num_samples,
            sampling_strategy='positive_only',
        )
        print(f"INFO: (DataLoader) âœ… Successfully sampled {len(train_points)} points.")

        # å°†åæ ‡å’Œå€¼åˆå¹¶æˆ [x, y, z, value] æ ¼å¼
        all_sampled_data = np.hstack([train_points, train_values.reshape(-1, 1)])
        
        # [æ–°é€»è¾‘] ä½¿ç”¨å¯é…ç½®çš„åˆ†å‰²ç­–ç•¥
        if split_ratios is None:
            # é»˜è®¤è¡Œä¸ºï¼š80/20 åˆ†å‰²
            main_train_set, test_set = train_test_split(all_sampled_data, test_size=0.2, random_state=42)
            reserve_pools = []
        else:
            if sum(split_ratios) >= 1.0:
                raise ValueError("split_ratios çš„æ€»å’Œå¿…é¡»å°äº 1.0ï¼Œä»¥ä¾¿ä¸ºæµ‹è¯•é›†ç•™å‡ºç©ºé—´ã€‚")

            remaining_data = all_sampled_data
            data_pools = []
            
            # å¾ªç¯åˆ‡åˆ†å‡ºä¸»è®­ç»ƒé›†å’Œæ‰€æœ‰å‚¨å¤‡é›†
            current_total_fraction = 1.0
            for ratio in split_ratios:
                # è®¡ç®—å½“å‰æ¯”ä¾‹ç›¸å¯¹äºå‰©ä½™æ•°æ®é‡çš„æ¯”ä¾‹
                split_fraction = ratio / current_total_fraction
                pool, remaining_data = train_test_split(remaining_data, test_size=(1.0 - split_fraction), random_state=42)
                data_pools.append(pool)
                current_total_fraction -= ratio

            main_train_set = data_pools[0]
            reserve_pools = data_pools[1:]
            test_set = remaining_data # å‰©ä¸‹çš„æ‰€æœ‰æ•°æ®éƒ½ä½œä¸ºæµ‹è¯•é›†
        
        print(f"INFO: (DataLoader) âœ… Split data into: Main training ({len(main_train_set)}), Test ({len(test_set)}), Reserve Pools ({len(reserve_pools)} pools).")
        if reserve_pools:
            for i, pool in enumerate(reserve_pools):
                print(f"    - Reserve Pool {i+1}: {len(pool)} points")

        return main_train_set, reserve_pools, test_set, dose_data

class GPUKriging:
    """
    [çœŸå®å®ç°] GPUåŠ é€Ÿçš„å…‹é‡Œé‡‘æ¨¡å‹çš„é€‚é…å™¨ã€‚
    è¯¥å®ç°å€Ÿé‰´äº† ComposeTools.py ä¸­çš„ KrigingAdapterï¼Œå¹¶è°ƒç”¨äº† myKriging åº“ã€‚
    """
    def __init__(self, variogram_model='exponential', **kwargs):
        """
        åˆå§‹åŒ–Krigingé€‚é…å™¨ã€‚
        
        Args:
            variogram_model (str): å…‹é‡Œé‡‘æ‰€éœ€çš„å˜å¼‚å‡½æ•°æ¨¡å‹ã€‚
            **kwargs: å…¶ä»–å¯ä»¥ä¼ é€’ç»™ myKriging åº“çš„å‚æ•° (å¦‚ nlags, block_size)ã€‚
        """
        self.model = None
        self._is_fitted = False
        self.variogram_model = variogram_model
        self.kriging_params = kwargs
        print(f"INFO: (GPUKriging) Initialized with variogram model: {self.variogram_model}")

    def fit(self, points: np.ndarray, values: np.ndarray):
        """
        ä½¿ç”¨ç¨€ç–çš„ç‚¹åæ ‡å’Œå¯¹åº”çš„æ®‹å·®å€¼æ¥è®­ç»ƒå…‹é‡Œé‡‘ä»£ç†æ¨¡å‹ã€‚
        """
        print(f"INFO: (GPUKriging) Fitting model with {len(points)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df = pd.DataFrame({
            'x': points[:, 0],
            'y': points[:, 1],
            'z': points[:, 2],
            'target': values
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_training å‡½æ•°
        self.model = kriging_training(
            df=df,
            variogram_model=self.variogram_model,
            nlags=self.kriging_params.get('nlags', 8),
            enable_plotting=False, # è®­ç»ƒä»£ç†æ¨¡å‹æ—¶é€šå¸¸ä¸ç»˜å›¾
            weight=False,
            uk=False,
            cpu_on=False # ç¡®ä¿ä½¿ç”¨GPU
        )
        
        self._is_fitted = True
        print("INFO: (GPUKriging) âœ… Model fitted.")

    def predict(self, points_to_predict: np.ndarray) -> np.ndarray:
        """
        å¯¹æ–°çš„ç‚¹è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼Œåˆ©ç”¨GPUåŠ é€Ÿã€‚
        """
        if not self._is_fitted:
            raise RuntimeError("Kriging model must be fitted before prediction.")
        
        print(f"INFO: (GPUKriging) Predicting values for {len(points_to_predict)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df_pred = pd.DataFrame({
            'x': points_to_predict[:, 0],
            'y': points_to_predict[:, 1],
            'z': points_to_predict[:, 2],
            'target': np.zeros(points_to_predict.shape[0]) # è™šæ‹Ÿç›®æ ‡å€¼
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_testing å‡½æ•°ï¼Œç¡®ä¿ä½¿ç”¨GPUåŠ é€Ÿé…ç½®
        predictions, _ = kriging_testing(
            df=df_pred,
            model=self.model,
            block_size=self.kriging_params.get('block_size', 10000),
            cpu_on=False, # ç¡®ä¿ä½¿ç”¨GPU
            style="gpu_b", # ä½¿ç”¨GPUæ‰¹å¤„ç†é£æ ¼
            multi_process=False,
            print_time=False,
            torch_ac=False, # ä½¿ç”¨PyTorchåŠ é€Ÿ
            compute_precision=False # é¢„æµ‹æ®‹å·®æ—¶ä¸éœ€è¦ç²¾åº¦è®¡ç®—
        )
        
        print(f"INFO: (GPUKriging) âœ… Prediction complete.")
        return predictions.flatten() # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„

class PINNModel:
    """
    [çœŸå®å®ç°] ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ã€‚
    è¯¥å®ç°è¢«è®¾è®¡ä¸ºå¯ä»å¤–éƒ¨æ§åˆ¶çš„æ¨¡å¼ï¼Œä»¥æ”¯æŒè‡ªé€‚åº”è®­ç»ƒæµç¨‹ã€‚
    """
    def __init__(self, dose_data: dict, training_data: np.ndarray, test_data: np.ndarray, num_collocation_points: int, network_layers=[3, 64, 64, 64, 1], lr=1e-3):
        """
        åˆå§‹åŒ–PINNæ¨¡å‹ï¼Œä½†ä¸PINNTrainerä¸åŒï¼Œè¿™é‡Œåªåšå‡†å¤‡å·¥ä½œï¼Œä¸å¼€å§‹è®­ç»ƒã€‚
        
        Args:
            dose_data (dict): ä»DataLoaderåŠ è½½çš„æ•°æ®å­—å…¸ã€‚
            training_data (np.ndarray): ç¨€ç–è®­ç»ƒæ•°æ® [x,y,z,value]ã€‚
            test_data (np.ndarray): ç¨€ç–æµ‹è¯•æ•°æ® [x,y,z,value]ã€‚
            num_collocation_points (int): æ±‚è§£åŸŸç‚¹çš„æ•°é‡ã€‚
            network_layers (list): ç¥ç»ç½‘ç»œç»“æ„ã€‚
            lr (float): å­¦ä¹ ç‡ã€‚
        """
        print("INFO: (PINNModel) Initializing a DeepXDE-based model for external control...")
        
        self.test_data_linear = test_data # å­˜å‚¨çº¿æ€§å°ºåº¦çš„æµ‹è¯•æ•°æ®
        
        # 1. å®šä¹‰å‡ ä½•
        world_min = dose_data['world_min']
        world_max = dose_data['world_max']
        self.geometry = dde.geometry.Cuboid(world_min, world_max)

        # 2. å®šä¹‰å¯è®­ç»ƒå‚æ•°
        k_initial_guess = 1.0 
        self.log_k_pinn = dde.Variable(np.log(k_initial_guess))
        
        # 3. å®šä¹‰PDEä¸ºç±»çš„æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å…¶ä»–åœ°æ–¹å¤ç”¨
        self.pde = self._build_pde_func()
        
        # 4. å®šä¹‰è®­ç»ƒæ•°æ®ç‚¹
        observe_x = training_data[:, :3]
        observe_y = np.log(np.maximum(training_data[:, 3:], 1e-30))
        data_points = dde.icbc.PointSetBC(observe_x, observe_y, component=0)
        
        # 5. ç»„åˆæˆdde.data.PDEå¯¹è±¡
        self.data = dde.data.PDE(
            self.geometry,
            self.pde,
            [data_points],
            num_domain=num_collocation_points,
            anchors=observe_x,
            # æˆ‘ä»¬è‡ªå®šä¹‰çš„æŒ‡æ ‡ä¼šä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„self.test_data_linear
        )
        
        # 6. åˆ›å»ºç½‘ç»œå’Œæ¨¡å‹
        self.net = dde.nn.FNN(network_layers, "tanh", "Glorot normal")
        self.model = dde.Model(self.data, self.net)
        
        # 7. è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°è¢«ç§»å‡ºä¸ºç±»æ–¹æ³• mean_relative_error_metric

        self.lr = lr # ä¿å­˜å­¦ä¹ ç‡ä»¥å¤‡é‡ç¼–è¯‘æ—¶ä½¿ç”¨
        
        # [æ–°å¢] MREå†å²è®°å½•åˆ—è¡¨
        self.mre_history = []
        self.epoch_history = []
        
        # 8. ç¼–è¯‘æ¨¡å‹ï¼ŒåŠ å…¥è‡ªå®šä¹‰æŒ‡æ ‡
        self.compile_model()
        print("INFO: (PINNModel) âœ… Model compiled and ready for training cycles.")
        
    def compile_model(self):
        """å°†æ¨¡å‹ç¼–è¯‘å°è£…æˆä¸€ä¸ªæ–¹æ³•ï¼Œæ–¹ä¾¿é‡ç”¨ã€‚"""
        # [ä¿®æ­£] åœ¨è¿™é‡Œè®¾ç½®æŒ‡æ ‡å‡½æ•°çš„æ˜¾ç¤ºåç§°
        # self.mean_relative_error_metric.__name__ = "MRE_test_set" # [ä¿®æ­£] ç§»é™¤æ­¤è¡Œï¼Œä¸èƒ½ä¸ºç±»æ–¹æ³•è®¾ç½®__name__

        self.model.compile(
            "adam", 
            lr=self.lr, 
            loss_weights=[1, 10], 
            external_trainable_variables=[self.log_k_pinn],
            metrics=[self.mean_relative_error_metric] # [ä¿®æ­£] ä¼ é€’å‡½æ•°å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        )

    def mean_relative_error_metric(self, y_true_ignored, y_pred_ignored):
        """
        ä¸€ä¸ª"hack"çš„æŒ‡æ ‡å‡½æ•°ã€‚å®ƒå¿½ç•¥ddeä¼ å…¥çš„å‚æ•°ï¼Œ
        è½¬è€Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„ã€åŸºäºçœŸå®ç‰©ç†å€¼çš„æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚
        """
        # ä½¿ç”¨æ¨¡å‹å¯¹æˆ‘ä»¬è‡ªå·±çš„æµ‹è¯•ç‚¹è¿›è¡Œé¢„æµ‹
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        
        # å°†é¢„æµ‹å€¼å’ŒçœŸå®å€¼éƒ½è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))
        
        # [æ–°å¢] è®°å½•MREå†å²
        current_epoch = self.model.train_state.step if self.model.train_state.step else 0
        self.mre_history.append(mre)
        self.epoch_history.append(current_epoch)
        
        return mre

    def inject_new_data(self, new_data_array: np.ndarray):
        """
        [æ–°èƒ½åŠ›] å‘æ¨¡å‹ä¸­æ³¨å…¥æ–°çš„è®­ç»ƒæ•°æ®ç‚¹ã€‚
        """
        print(f"\nINFO: (PINNModel)  injecting {len(new_data_array)} new data points...")
        
        # 1. è·å–ç°æœ‰æ•°æ®
        current_bc = self.data.bcs[0]
        current_points = current_bc.points
        current_values_log = current_bc.values.cpu()

        # 2. å‡†å¤‡æ–°æ•°æ®
        new_points = new_data_array[:, :3]
        new_values_log = np.log(np.maximum(new_data_array[:, 3:], 1e-30)).reshape(-1, 1)

        # 3. åˆå¹¶æ–°æ—§æ•°æ®
        combined_points = np.vstack([current_points, new_points])
        combined_values_log = np.vstack([current_values_log, new_values_log])
        
        print(f"    Total training points increased to {len(combined_points)}.")

        # 4. åˆ›å»ºæ–°çš„ PointSetBC å’Œ PDE æ•°æ®å¯¹è±¡
        new_bc = dde.icbc.PointSetBC(combined_points, combined_values_log, component=0)
        
        # æ›´æ–°é”šç‚¹ä»¥åŒ…å«æ‰€æœ‰è®­ç»ƒæ•°æ®
        new_anchors = combined_points
        
        new_data_obj = dde.data.PDE(
            self.geometry,
            self.pde,
            [new_bc],
            num_domain=self.data.num_domain,
            anchors=new_anchors
        )
        
        # 5. æ›´æ–°æ¨¡å‹çš„æ•°æ®å¹¶é‡æ–°ç¼–è¯‘
        self.data = new_data_obj
        self.model.data = self.data
        self.compile_model()
        print("INFO: (PINNModel) âœ… Model re-compiled with new data. Initializing new train state...")
        # [ä¿®æ­£] è°ƒç”¨ train(0) æ¥å¼ºåˆ¶ä½¿ç”¨æ–°æ•°æ®å¯¹è±¡é‡å»ºè®­ç»ƒçŠ¶æ€
        self.model.train(iterations=0, display_every=100000) # display_everyè®¾ä¸ºå¤§æ•°ä»¥é¿å…ä¸å¿…è¦çš„è¾“å‡º
        print("INFO: (PINNModel) âœ… New train state initialized.")
        
        # [ä¿®æ­£] æ³¨å…¥æ•°æ®åï¼Œè®°å½•ä¸€æ¬¡å½“å‰MREä»¥ä¿æŒå†å²è¿ç»­æ€§
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        current_mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))
        current_epoch = self.model.train_state.step if self.model.train_state.step else 0
        self.mre_history.append(current_mre)
        self.epoch_history.append(current_epoch)
        print(f"INFO: (PINNModel) MRE after data injection: {current_mre:.6f} at epoch {current_epoch}")
        
    def _build_pde_func(self):
        """å°†PDEå®šä¹‰å°è£…åœ¨ä¸€ä¸ªå·¥å‚å‡½æ•°ä¸­ï¼Œä»¥æ•è·self.log_k_pinnã€‚"""
        def pde_func(x, u):
            grad_u_sq = dde.grad.jacobian(u, x, i=0, j=0)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=1)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=2)**2
            laplacian_u = dde.grad.hessian(u, x, i=0, j=0) + \
                          dde.grad.hessian(u, x, i=1, j=1) + \
                          dde.grad.hessian(u, x, i=2, j=2)
            k_squared = dde.backend.exp(2 * self.log_k_pinn)
            return grad_u_sq + laplacian_u - k_squared
        return pde_func

    def run_training_cycle(self, max_epochs: int, detect_every: int, collocation_points: np.ndarray, 
                         detection_threshold: float = 0.1):
        """
        [é‡æ„] æ‰§è¡Œä¸€ä¸ªå¸¦æœ‰åŠ¨æ€åœæ­¢æ¡ä»¶çš„è®­ç»ƒå‘¨æœŸã€‚
        
        Args:
            max_epochs (int): å½“å‰å‘¨æœŸçš„æœ€å¤§è®­ç»ƒè½®æ•°ã€‚
            detect_every (int): æ¯éš”å¤šå°‘è½®è¿›è¡Œä¸€æ¬¡æ€§èƒ½æ£€æµ‹ã€‚
            collocation_points (np.ndarray): ç”¨äºæœ¬å‘¨æœŸçš„é…ç½®ç‚¹ã€‚
            detection_threshold (float): è§¦å‘æ—©åœçš„ç›¸å¯¹æ”¹è¿›é˜ˆå€¼ã€‚
        
        Returns:
            dict: ä¸€ä¸ªåŒ…å«è®­ç»ƒç»“æœä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚{'stagnation_detected': bool}
        """
        # 1. ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä¹‰æ£€æŸ¥ç‚¹æ–‡ä»¶åçš„å‰ç¼€ï¼Œå¹¶ç¡®ä¿ç›®å½•å­˜åœ¨
        script_dir = Path(__file__).parent.resolve()
        checkpoint_path_prefix = str(script_dir / "models" / "best_model_in_cycle")
        os.makedirs(Path(checkpoint_path_prefix).parent, exist_ok=True)

        # 2. æ›´æ–°æ±‚è§£åŸŸç‚¹
        num_bc_points = self.data.bcs[0].points.shape[0]
        if self.model.train_state.X_train is None:
            # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ï¼Œä»¥ä¾¿å¯ä»¥ä¿®æ”¹X_train
            self.model.train(iterations=0)
        start_index = num_bc_points
        end_index = len(self.model.train_state.X_train) - len(self.data.anchors)
        self.model.train_state.X_train[start_index:end_index] = collocation_points
        
        # [æ–°] åˆå§‹åŒ–æœ¬å‘¨æœŸçš„è¿”å›çŠ¶æ€
        stagnation_detected_this_run = False
        data_injected_this_cycle = False
        
        # 3. åˆ›å»ºå›è°ƒï¼Œå¹¶ç”¨å½“å‰æ¨¡å‹çš„æ€§èƒ½åˆå§‹åŒ–å®ƒ
        stopper = EarlyCycleStopper(
            detection_threshold=detection_threshold,
            display_every=5,
            checkpoint_path_prefix=checkpoint_path_prefix
        )
        # [ä¿®æ­£] åœ¨é‡ç½®æ—¶ï¼Œä¼ å…¥å½“å‰æ¨¡å‹çš„MREå’Œåˆå§‹æ£€æŸ¥ç‚¹ä½œä¸ºåŸºçº¿
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        initial_mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))

        # ä¸ºåˆå§‹çŠ¶æ€åˆ›å»ºç¬¬ä¸€ä¸ªåŸºå‡†æ£€æŸ¥ç‚¹
        epochs_before_cycle = self.model.train_state.step or 0
        self.model.save(checkpoint_path_prefix, verbose=0)
        initial_model_path = f"{checkpoint_path_prefix}-{epochs_before_cycle}.pt"
        
        stopper.reset_cycle(initial_mre=initial_mre, initial_model_path=initial_model_path)
        
        print(f"INFO: (PINNModel) Starting dynamic training cycle (max: {max_epochs} epochs, detect every: {detect_every})...")
        print(f"    Initial MRE for this cycle is {initial_mre:.4f}")
        
        remaining_epochs = max_epochs
        while remaining_epochs > 0:
            epochs_to_run = min(detect_every, remaining_epochs)
            
            self.model.train(
                iterations=epochs_to_run, 
                display_every=5,
                callbacks=[stopper]
            )

            # --- [æ–°ç­–ç•¥] æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰ç»“æŸæœ¬è½®è‡ªé€‚åº”å‘¨æœŸ ---
            should_exit_cycle = False

            # æ¡ä»¶1: åœæ» (Stagnation) - æ¨¡å‹æ€§èƒ½åœ¨æœ¬è½®è®­ç»ƒåå˜å·®
            if stopper.best_model_path and os.path.exists(stopper.best_model_path):
                latest_mre = self.model.train_state.metrics_test[-1]
                if latest_mre > stopper.best_mre:
                    print(f"    âš ï¸ Stagnation detected: MRE increased to {latest_mre:.4f} (best is {stopper.best_mre:.4f}).")
                    stagnation_detected_this_run = True
                    
                    print(f"    â†³ Forcing new adaptive resampling cycle...")
                    self.model.restore(stopper.best_model_path, verbose=0) 
                    should_exit_cycle = True

            # æ¡ä»¶2: å¿«é€Ÿæå‡ (Rapid Improvement) - æˆ‘ä»¬çš„æ—§æ—©åœé€»è¾‘
            if stopper.should_stop:
                print(f"\nINFO: (PINNModel) ğŸ“ˆ Rapid improvement! Capitalizing on gains and forcing new resampling.")
                should_exit_cycle = True
            
            if should_exit_cycle:
                break
                
            remaining_epochs -= epochs_to_run
        else:
             print(f"\nINFO: (PINNModel) Max epochs reached for this cycle.")

        # 4. [é‡è¦] æ— è®ºå¦‚ä½•ï¼Œéƒ½ä»æœ€ç»ˆçš„æœ€ä½³æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å‹ï¼Œå¹¶æ¸…ç†æ–‡ä»¶
        if stopper.best_model_path and os.path.exists(stopper.best_model_path):
            print(f"INFO: (PINNModel) Restoring model to best state from '{stopper.best_model_path}'...")
            self.model.restore(stopper.best_model_path, verbose=1)
            os.remove(stopper.best_model_path) # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        else:
            print("WARNING: (PINNModel) Best checkpoint file not found. Model may not be in its best state.")
            
        return {'stagnation_detected': stagnation_detected_this_run}

    def predict(self, points: np.ndarray) -> np.ndarray:
        """
        [æ–°] ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¿”å›çº¿æ€§å°ºåº¦çš„ç‰©ç†å€¼ã€‚
        
        Args:
            points (np.ndarray): å¾…é¢„æµ‹ç‚¹çš„åæ ‡ï¼Œå½¢çŠ¶ä¸º (N, 3)ã€‚
            
        Returns:
            np.ndarray: é¢„æµ‹çš„ç‰©ç†å€¼ï¼Œå½¢çŠ¶ä¸º (N,)ã€‚
        """
        print(f"INFO: (PINNModel) Predicting on {len(points)} points...")
        # model.predict è¿”å›çš„æ˜¯å¯¹æ•°å°ºåº¦çš„å€¼
        pred_y_log = self.model.predict(points)
        # è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
        pred_y_linear = np.exp(pred_y_log)
        return pred_y_linear.flatten()

    def compute_pde_residual(self, points: np.ndarray) -> np.ndarray:
        """
        [çœŸå®å®ç°] è®¡ç®—ç»™å®šç‚¹ä¸Šçš„ç‰©ç†æ–¹ç¨‹æ®‹å·®ã€‚
        åˆ©ç”¨ deepxde çš„ model.predict(operator=...) åŠŸèƒ½ã€‚
        """
        print(f"INFO: (PINNModel) Computing PDE residuals for {len(points)} points...")
        
        # deepxde.Model.predict å¯ä»¥æ¥å—ä¸€ä¸ª operator å‚æ•°
        # æˆ‘ä»¬å°† self.pde (åœ¨__init__ä¸­å®šä¹‰çš„å‡½æ•°) ä½œä¸ºç®—å­ä¼ å…¥
        residuals = self.model.predict(points, operator=self.pde)
        
        # è¿”å›æ®‹å·®çš„ç»å¯¹å€¼ï¼Œå¹¶å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        return np.abs(residuals).flatten()

class EarlyCycleStopper(dde.callbacks.Callback):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰å›è°ƒï¼Œç”¨äºåœ¨è®­ç»ƒå‘¨æœŸå†…å®ç°åŸºäºæ€§èƒ½çš„"æ—©åœ"ã€‚
    åŒæ—¶è‡ªå·±è´Ÿè´£ä¿å­˜å‘¨æœŸå†…çš„æœ€ä½³æ¨¡å‹ã€‚
    """
    def __init__(self, detection_threshold: float, display_every: int, checkpoint_path_prefix: str):
        super().__init__()
        self.threshold = detection_threshold
        self.display_every = display_every
        self.checkpoint_path_prefix = checkpoint_path_prefix
        self.best_mre = np.inf
        self.should_stop = False
        self.best_model_path = "" # å°†å­˜å‚¨æœ€ä½³æ¨¡å‹çš„å®Œæ•´çœŸå®è·¯å¾„

    def reset_cycle(self, initial_mre: float = np.inf, initial_model_path: str = ""):
        """
        æ‰‹åŠ¨é‡ç½®æ•´ä¸ªå‘¨æœŸçš„çŠ¶æ€ï¼Œä¸ºæ–°çš„è‡ªé€‚åº”å‘¨æœŸåšå‡†å¤‡ã€‚
        å¯ä»¥æ¥æ”¶ä¸€ä¸ªåˆå§‹MREå’Œæ¨¡å‹è·¯å¾„ä½œä¸ºæœ¬å‘¨æœŸçš„æ€§èƒ½åŸºçº¿ã€‚
        """
        # æ¸…ç†ä¸Šä¸€è½®å¯èƒ½é—ç•™çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        if self.best_model_path and os.path.exists(self.best_model_path):
            os.remove(self.best_model_path)
        
        self.best_mre = initial_mre
        self.best_model_path = initial_model_path
        self.should_stop = False

    def on_epoch_end(self):
        """åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¢«è°ƒç”¨, å¹¶ä¸”åœ¨è¿™é‡Œæ£€æŸ¥æ€§èƒ½"""
        if self.model.train_state.step > 0 and self.model.train_state.step % self.display_every == 0:
            if not self.model.train_state.metrics_test:
                 return

            latest_mre = self.model.train_state.metrics_test[-1]
            
            if self.best_mre != np.inf:
                improvement = self.best_mre - latest_mre
                required_improvement_amount = self.best_mre * self.threshold
                
                if improvement > required_improvement_amount:
                    print(f"    ğŸ’¡ Early Stop: MRE dropped from {self.best_mre:.4f} to {latest_mre:.4f} (>{self.threshold:.0%}).")
                    self.should_stop = True
            
            # åˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯å¦æ˜¯æ–°çš„æœ€ä¼˜æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¿å­˜å®ƒ
            if latest_mre < self.best_mre:
                print(f"    â­ New best model found (MRE: {latest_mre:.4f}). Checkpointing...")
                self.best_mre = latest_mre

                # æ¸…ç†ä¸Šä¸€ä¸ªæœ€ä½³æ¨¡å‹
                if self.best_model_path and os.path.exists(self.best_model_path):
                    os.remove(self.best_model_path)
                
                # æ„å»ºæ–°çš„æœ€ä½³æ¨¡å‹è·¯å¾„å¹¶ä¿å­˜
                current_step = self.model.train_state.step
                self.best_model_path = f"{self.checkpoint_path_prefix}-{current_step}.pt"
                self.model.save(self.checkpoint_path_prefix, verbose=0)

class AdaptiveSampler:
    """
    [å»ºè®®æ‚¨å®ç°] è‡ªé€‚åº”é‡‡æ ·å™¨ã€‚
    """
    def __init__(self, domain_bounds, total_candidates=100000):
        self.bounds = domain_bounds
        # é¢„å…ˆåœ¨æ•´ä¸ªåŸŸå†…ç”Ÿæˆå¤§é‡çš„å€™é€‰ç‚¹ï¼Œåç»­ä»ä¸­ç­›é€‰
        self.candidate_points = np.random.rand(total_candidates, 3) * \
            (domain_bounds[1] - domain_bounds[0]) + domain_bounds[0]
        print(f"INFO: (AdaptiveSampler) Initialized with {total_candidates} candidate points.")

    def generate_new_collocation_points(
        self,
        kriging_model: GPUKriging,
        num_points_to_sample: int,
        force_exploration: bool = False
    ) -> np.ndarray:
        """
        ä½¿ç”¨Krigingæ¨¡å‹å¼•å¯¼ç”Ÿæˆæ–°çš„é…ç½®ç‚¹ã€‚
        Args:
            kriging_model: è®­ç»ƒå¥½çš„æ®‹å·®ä»£ç†æ¨¡å‹ã€‚
            num_points_to_sample: éœ€è¦ç”Ÿæˆçš„æ€»ç‚¹æ•°ã€‚
            force_exploration: æ˜¯å¦å› æ¨¡å‹åœæ»è€Œå¼ºåˆ¶å¢åŠ æ¢ç´¢æ¯”ä¾‹ã€‚
        Returns:
            np.ndarray: æ–°çš„é…ç½®ç‚¹é›†ã€‚
        """
        # [æ–°é€»è¾‘] æ ¹æ®æ˜¯å¦åœæ»æ¥åŠ¨æ€è°ƒæ•´æ¢ç´¢ç‡
        if force_exploration:
            exploration_ratio = 0.3 # åœæ»æ—¶ï¼Œå¤§å¹…å¢åŠ éšæœºæ¢ç´¢
            print(f"INFO: (AdaptiveSampler) Stagnation detected! Increasing exploration ratio to {exploration_ratio:.0%}.")
        else:
            exploration_ratio = 0.1 # æ­£å¸¸æƒ…å†µä¸‹çš„æ¢ç´¢ç‡

        # 1. ä½¿ç”¨Krigingä»£ç†æ¨¡å‹é¢„æµ‹æ‰€æœ‰å€™é€‰ç‚¹çš„æ®‹å·®
        predicted_residuals = kriging_model.predict(self.candidate_points)
        print(f"    - Krigingé¢„æµ‹æ®‹å·®ç»Ÿè®¡ (åœ¨ {len(self.candidate_points)} ä¸ªå€™é€‰ç‚¹ä¸Š):")
        print(f"      - Max={np.max(predicted_residuals):.4e}, "
              f"Min={np.min(predicted_residuals):.4e}, "
              f"Mean={np.mean(predicted_residuals):.4e}, "
              f"Std={np.std(predicted_residuals):.4e}")

        # 2. "Hard-Case Mining": æ‰¾åˆ°é¢„æµ‹æ®‹å·®æœ€å¤§çš„ç‚¹çš„ç´¢å¼•
        num_exploitation_points = int(num_points_to_sample * (1 - exploration_ratio))
        hard_case_indices = np.argsort(predicted_residuals)[-num_exploitation_points:]
        exploitation_points = self.candidate_points[hard_case_indices]

        # 3. "Exploration": åŠ å…¥ä¸€éƒ¨åˆ†éšæœºç‚¹ä»¥é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜
        num_exploration_points = num_points_to_sample - num_exploitation_points
        random_indices = np.random.choice(len(self.candidate_points), num_exploration_points, replace=False)
        exploration_points = self.candidate_points[random_indices]

        print(f"INFO: (AdaptiveSampler) Generated {num_exploitation_points} exploitation points and {num_exploration_points} exploration points.")
        
        return np.vstack([exploitation_points, exploration_points])

def main():
    """
    ä¸»å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ª"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"æµç¨‹ã€‚
    """
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œï¼šå…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ")
    print("="*60 + "\n")

    # --- 1. åˆå§‹åŒ– ---
    # !! æ³¨æ„: DOMAIN_BOUNDS ç°åœ¨ä»…ç”¨äºå¯è§†åŒ–æˆ–é‡‡æ ·å™¨ï¼Œå®é™…ç‰©ç†è¾¹ç•Œç”±åŠ è½½çš„æ•°æ®å†³å®š !!
    DOMAIN_BOUNDS = np.array([[0., 0., 0.], [1., 1., 1.]]) 
    TOTAL_EPOCHS = 2400
    ADAPTIVE_CYCLE_EPOCHS = 400  # æ¯å¤šå°‘ä¸ªepochæ‰§è¡Œä¸€æ¬¡è‡ªé€‚åº”è°ƒæ•´
    DETECT_EPOCHS = 100 # æ¯50è½®æ£€æµ‹ä¸€æ¬¡æ€§èƒ½ [ä¿®æ­£æ³¨é‡Š]
    DATA_SPLIT_RATIOS = [0.7] + [0.05]*6
    
    # --- æ•°æ®åŠ è½½å‚æ•° ---
    DATA_PATH = "PINN/DATA.xlsx"
    SPACE_DIMS = np.array([20.0, 10.0, 10.0])
    NUM_SAMPLES = 50
    
    # --- æ¨¡å‹è®­ç»ƒå‚æ•° ---
    NUM_COLLOCATION_POINTS = 4096
    NUM_RESIDUAL_SCOUT_POINTS = 5000 # ç”¨äºä¾¦å¯Ÿçš„ç‚¹æ•°ï¼Œè¿œå°‘äºè®­ç»ƒç‚¹æ•°

    # 1. æ•°æ®åŠ è½½
    data_loader = DummyDataLoader(
        data_path=DATA_PATH,
        space_dims=SPACE_DIMS,
        num_samples=NUM_SAMPLES
    )
    main_train_set, reserve_data_pools, test_data, dose_data = data_loader.get_training_data(
        split_ratios=DATA_SPLIT_RATIOS
    )

    # 2. æ¨¡å‹å’Œé‡‡æ ·å™¨åˆå§‹åŒ–
    world_min = dose_data['world_min']
    world_max = dose_data['world_max']
    
    pinn = PINNModel(
        dose_data=dose_data, 
        training_data=main_train_set, # [æ–°] åªç”¨ä¸»è®­ç»ƒé›†åˆå§‹åŒ–
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS
    )
    kriging = GPUKriging()
    sampler = AdaptiveSampler(domain_bounds=np.vstack([world_min, world_max]))
    
    # åˆå§‹é…ç½®ç‚¹ï¼šåœ¨çœŸå®ç‰©ç†ç©ºé—´å†…é‡‡æ ·
    current_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                  (world_max - world_min) + world_min)

    # --- 3. [ä¿®æ­£] è®­ç»ƒå¾ªç¯ ---
    # ä½¿ç”¨ while å¾ªç¯æ¥ç¡®ä¿æ€»è®­ç»ƒè½®æ•°è¾¾æ ‡
    total_epochs_trained = 0
    consecutive_stagnation_count = 0 # [æ–°] è¿ç»­åœæ»è®¡æ•°å™¨
    
    # [æ–°å¢] é‡è¦äº‹ä»¶è®°å½•åˆ—è¡¨ï¼Œç”¨äºå›¾è¡¨æ ‡æ³¨
    important_events = []  # æ ¼å¼: [(epoch, event_type, description), ...]

    while total_epochs_trained < TOTAL_EPOCHS:
        remaining_total_epochs = TOTAL_EPOCHS - total_epochs_trained
        
        # æœ¬æ¬¡è‡ªé€‚åº”å‘¨æœŸçš„æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œä¸èƒ½è¶…è¿‡æ€»å‰©ä½™è½®æ•°
        cycle_max_epochs = min(ADAPTIVE_CYCLE_EPOCHS, remaining_total_epochs)
        
        print(f"\n--- ä¸»å¾ªç¯å‘¨æœŸ: ç›®æ ‡è®­ç»ƒ {total_epochs_trained} -> {total_epochs_trained + cycle_max_epochs} ---")

        # 2a. ä½¿ç”¨å½“å‰çš„é…ç½®ç‚¹ï¼Œå¯¹PINNè¿›è¡Œä¸€è½®å¸¸è§„è®­ç»ƒ
        print(f"PHASE 2a: å¸¸è§„PINNè®­ç»ƒ (æœ¬å‘¨æœŸä¸Šé™: {cycle_max_epochs} epochs)...")
        
        # è®°å½•è¿›å…¥æ­¤å‘¨æœŸå‰çš„è®­ç»ƒæ­¥æ•°
        epochs_before_cycle = pinn.model.train_state.step or 0
        
        # è°ƒç”¨æ”¹é€ åçš„æ–¹æ³•ï¼Œå¹¶æ¥æ”¶å…¶è¿”å›ç»“æœ
        cycle_result = pinn.run_training_cycle(
            max_epochs=cycle_max_epochs,
            detect_every=DETECT_EPOCHS,
            collocation_points=current_collocation_points,
            detection_threshold=0.1
        )
        
        # è®¡ç®—æœ¬å‘¨æœŸå®é™…è®­ç»ƒäº†å¤šå°‘è½®
        epochs_this_cycle = (pinn.model.train_state.step or 0) - epochs_before_cycle
        total_epochs_trained += epochs_this_cycle

        # --- [æ–°é€»è¾‘] ---
        # 1. æ›´æ–°åœæ»è®¡æ•°å™¨
        stagnation_this_cycle = cycle_result.get('stagnation_detected', False)
        if stagnation_this_cycle:
            consecutive_stagnation_count += 1
            print(f"INFO: Consecutive stagnation count increased to: {consecutive_stagnation_count}")
        else:
            # ä»»ä½•æˆåŠŸçš„å‘¨æœŸéƒ½ä¼šé‡ç½®è®¡æ•°å™¨
            if consecutive_stagnation_count > 0:
                print(f"INFO: Training successful, resetting stagnation count from {consecutive_stagnation_count} to 0.")
            consecutive_stagnation_count = 0
        
        print(f"\nINFO: æœ¬å‘¨æœŸå®é™…è®­ç»ƒ {epochs_this_cycle} è½®. æ€»è®­ç»ƒè¿›åº¦: {total_epochs_trained}/{TOTAL_EPOCHS}")
        
        # å¦‚æœå·²ç»è®­ç»ƒå¤Ÿäº†ï¼Œå°±æå‰ç»“æŸä¸»å¾ªç¯
        if total_epochs_trained >= TOTAL_EPOCHS:
            print("\nINFO: æ€»è®­ç»ƒè½®æ•°å·²è¾¾åˆ°ç›®æ ‡ï¼Œç»“æŸè‡ªé€‚åº”è®­ç»ƒã€‚")
            break

        # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œå¹²é¢„ (æ³¨å…¥æ•°æ® + å…‹é‡Œé‡‘é‡é‡‡æ ·)
        if consecutive_stagnation_count >= 2:
            print("\n" + "!"*60)
            print("!! è¿ç»­åœæ»ä¸¤æ¬¡ï¼Œè§¦å‘å¹²é¢„æœºåˆ¶ !!")
            print("!"*60)

            # --- å¹²é¢„æªæ–½ 1: æ³¨å…¥æ–°æ•°æ® (å¦‚æœè¿˜æœ‰) ---
            if reserve_data_pools:
                print("\nPHASE A: æ³¨å…¥æ–°çš„å‚¨å¤‡è®­ç»ƒæ•°æ®...")
                data_injection_epoch = pinn.model.train_state.step or 0
                data_to_inject = reserve_data_pools.pop(0)
                pinn.inject_new_data(data_to_inject)
                print("PHASE A: âœ… æ–°æ•°æ®æ³¨å…¥å®Œæˆã€‚")
                
                # [æ–°å¢] è®°å½•æ•°æ®æ³¨å…¥äº‹ä»¶
                important_events.append((
                    data_injection_epoch, 
                    'data_injection', 
                    f'æ•°æ®æ³¨å…¥ (+{len(data_to_inject)}ç‚¹)'
                ))
            else:
                print("\nWARNING: å·²æ— æ›´å¤šå‚¨å¤‡æ•°æ®å¯æ³¨å…¥ã€‚")

            # --- å¹²é¢„æªæ–½ 2: å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ · ---
            print("\nPHASE B: å¼€å§‹å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ ·...")
            
            # æ®‹å·®"ä¾¦å¯Ÿ"
            scout_points = (np.random.rand(NUM_RESIDUAL_SCOUT_POINTS, 3) *
                            (world_max - world_min) + world_min)
            true_residuals = pinn.compute_pde_residual(scout_points)
            print(f"    - çœŸå®PDEæ®‹å·®ç»Ÿè®¡ (åœ¨ {len(scout_points)} ä¸ªä¾¦å¯Ÿç‚¹ä¸Š):")
            print(f"      - Max={np.max(true_residuals):.4e}, "
                  f"Min={np.min(true_residuals):.4e}, "
                  f"Mean={np.mean(true_residuals):.4e}, "
                  f"Std={np.std(true_residuals):.4e}")
            
            # å…‹é‡Œé‡‘ä»£ç†å»ºæ¨¡
            kriging.fit(scout_points, true_residuals)

            # è‡ªé€‚åº”é‡‡æ ·
            kriging_epoch = pinn.model.train_state.step or 0
            num_collocation_to_generate = pinn.data.num_domain
            print(f"INFO: Dynamically calculated {num_collocation_to_generate} collocation points to generate.")

            current_collocation_points = sampler.generate_new_collocation_points(
                kriging_model=kriging,
                num_points_to_sample=num_collocation_to_generate,
                force_exploration=True  # å› ä¸ºåœæ»äº†ï¼Œæ‰€ä»¥å¼ºåˆ¶æ¢ç´¢
            )
            print("PHASE B: âœ… æ–°çš„è‡ªé€‚åº”é…ç½®ç‚¹å·²ç”Ÿæˆã€‚")
            
            # [æ–°å¢] è®°å½•å…‹é‡Œé‡‘åº”ç”¨äº‹ä»¶
            important_events.append((
                kriging_epoch, 
                'kriging_resampling', 
                'å…‹é‡Œé‡‘å¼•å¯¼é‡é‡‡æ ·'
            ))

            # --- å¹²é¢„åé‡ç½®è®¡æ•°å™¨ ---
            print("\nINFO: å¹²é¢„æªæ–½å·²æ‰§è¡Œï¼Œé‡ç½®åœæ»è®¡æ•°å™¨ã€‚")
            consecutive_stagnation_count = 0
            print("-" * 60)
        
        else:
            # å¦‚æœæ²¡æœ‰è¾¾åˆ°å¹²é¢„é˜ˆå€¼ï¼Œåˆ™ä¸æ‰§è¡Œå…‹é‡Œé‡‘é‡‡æ ·ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰é…ç½®ç‚¹
            print("\nINFO: æœªè¾¾åˆ°å¹²é¢„é˜ˆå€¼ï¼Œä¸‹ä¸€å‘¨æœŸå°†ç»§ç»­ä½¿ç”¨å½“å‰é…ç½®ç‚¹ã€‚")
            # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ›´æ–° current_collocation_points
            pass

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

    # --- 4. è®­ç»ƒåŸå§‹PINNä½œä¸ºå¯¹æ¯”æ¨¡å‹ ---
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒåŸå§‹PINNä½œä¸ºå¯¹æ¯”åŸºçº¿")
    print("="*60 + "\n")

    # [æ–°] ä¸ºåŸºçº¿æ¨¡å‹å‡†å¤‡å®Œæ•´çš„è®­ç»ƒæ•°æ®
    # ç›´æ¥å°†ç¬¬ä¸€æ¬¡åŠ è½½æ—¶åˆ†å‰²å¥½çš„æ‰€æœ‰æ•°æ®å—åˆå¹¶èµ·æ¥
    all_training_blocks = [main_train_set] + reserve_data_pools
    full_training_data = np.vstack(all_training_blocks)

    pinn_baseline = PINNModel(
        dose_data=dose_data, 
        training_data=full_training_data, # [æ–°] ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS
    )
    # ä¸ºåŸå§‹PINNç”Ÿæˆä¸€æ¬¡æ€§çš„ã€å›ºå®šçš„é…ç½®ç‚¹
    baseline_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                   (world_max - world_min) + world_min)

    # åŸå§‹PINNä½¿ç”¨å›ºå®šçš„è®­ç»ƒå‘¨æœŸ
    print("INFO: (Baseline PINN) Setting collocation points...")
    # [ä¿®æ­£] å°†ç”Ÿæˆçš„é…ç½®ç‚¹æ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
    num_bc_points_base = pinn_baseline.data.bcs[0].points.shape[0]
    if pinn_baseline.model.train_state.X_train is None:
        pinn_baseline.model.train(iterations=0)
    start_index_base = num_bc_points_base
    end_index_base = len(pinn_baseline.model.train_state.X_train) - len(pinn_baseline.data.anchors)
    pinn_baseline.model.train_state.X_train[start_index_base:end_index_base] = baseline_collocation_points

    print("INFO: (Baseline PINN) Starting training...")
    pinn_baseline.model.train(iterations=TOTAL_EPOCHS, display_every=5)
    
    print("\n" + "="*60)
    print("ğŸ‰ åŸå§‹PINNè®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

    # --- 5. æœ€ç»ˆç»“æœè¯„ä¼°ä¸å¯¹æ¯” ---
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»“æœè¯„ä¼°ä¸å¯¹æ¯”")
    print("="*60 + "\n")

    test_points = test_data[:, :3]
    true_values = test_data[:, 3]

    print("æ­£åœ¨è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½...")
    adaptive_preds = pinn.predict(test_points)
    baseline_preds = pinn_baseline.predict(test_points)

    def calculate_mre(y_true, y_pred):
        return np.mean(np.abs(y_true - y_pred) / (y_true + 1e-10))

    mre_adaptive = calculate_mre(true_values, adaptive_preds)
    mre_baseline = calculate_mre(true_values, baseline_preds)

    print(f"\n{'æ¨¡å‹':<28} | {'å¹³å‡ç›¸å¯¹è¯¯å·® (MRE) on Test Set':<30}")
    print("-" * 65)
    print(f"{'Krigingå¼•å¯¼çš„è‡ªé€‚åº”PINN':<28} | {mre_adaptive:<30.6%}")
    print(f"{'åŸå§‹PINN (å›ºå®šé‡‡æ ·)':<28} | {mre_baseline:<30.6%}")
    print("-" * 65)
    
    # è¾“å‡ºé‡è¦äº‹ä»¶æ‘˜è¦
    if important_events:
        print(f"\nğŸ“‹ è®­ç»ƒè¿‡ç¨‹é‡è¦äº‹ä»¶æ‘˜è¦:")
        print("-" * 50)
        for epoch, event_type, description in important_events:
            event_name = "ğŸ”„ å…‹é‡Œé‡‘é‡é‡‡æ ·" if event_type == 'kriging_resampling' else "ğŸ“Š æ•°æ®æ³¨å…¥"
            print(f"  Epoch {epoch:4d}: {event_name} - {description}")
        print("-" * 50)

    # --- 6. ç»˜åˆ¶å¯¹æ¯”å›¾ ---
    print("\n" + "="*60)
    print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹MREå¯¹æ¯”å›¾")
    print("="*60 + "\n")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # æ‰“å°å°†è¦æ ‡æ³¨çš„äº‹ä»¶
    if important_events:
        print(f"INFO: å‡†å¤‡åœ¨å›¾è¡¨ä¸­æ ‡æ³¨ {len(important_events)} ä¸ªé‡è¦æ—¶é—´ç‚¹:")
        for epoch, event_type, description in important_events:
            print(f"  - Epoch {epoch}: {description}")
    
    # ç»˜åˆ¶è‡ªé€‚åº”PINNçš„MREå†å²
    if pinn.epoch_history and pinn.mre_history:
        ax.plot(pinn.epoch_history, pinn.mre_history, 
                label='Krigingå¼•å¯¼çš„è‡ªé€‚åº”PINN', linewidth=2, alpha=0.8, color='blue')
    
    # ç»˜åˆ¶åŸºçº¿PINNçš„MREå†å²
    if pinn_baseline.epoch_history and pinn_baseline.mre_history:
        ax.plot(pinn_baseline.epoch_history, pinn_baseline.mre_history, 
                label='åŸå§‹PINN (å›ºå®šé‡‡æ ·)', linewidth=2, alpha=0.8, color='red')
    
    # æ·»åŠ é‡è¦äº‹ä»¶æ ‡æ³¨
    if important_events:
        print(f"INFO: æ ‡æ³¨ {len(important_events)} ä¸ªé‡è¦æ—¶é—´ç‚¹...")
        
        # å®šä¹‰äº‹ä»¶ç±»å‹çš„é¢œè‰²å’Œæ ·å¼
        event_styles = {
            'data_injection': {'color': 'green', 'linestyle': '--', 'alpha': 0.7},
            'kriging_resampling': {'color': 'orange', 'linestyle': '-.', 'alpha': 0.7}
        }
        
        for i, (epoch, event_type, description) in enumerate(important_events):
            style = event_styles.get(event_type, {'color': 'gray', 'linestyle': '-', 'alpha': 0.5})
            
            # ç»˜åˆ¶å‚ç›´çº¿
            ax.axvline(x=epoch, **style, linewidth=2)
            
            # è·å–å½“å‰yè½´èŒƒå›´æ¥å®šä½æ–‡æœ¬
            y_min, y_max = ax.get_ylim()
            y_pos = y_max * (0.8 - (i % 3) * 0.15)  # é”™å¼€æ ‡æ³¨ä½ç½®é¿å…é‡å 
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
            ax.annotate(
                f'{description}\n(Epoch {epoch})',
                xy=(epoch, y_pos),
                xytext=(10, 10),
                textcoords='offset points',
                bbox=dict(boxstyle='round,pad=0.3', facecolor=style['color'], alpha=0.3),
                fontsize=9,
                ha='left'
            )
        
        # æ·»åŠ å›¾ä¾‹è¯´æ˜
        from matplotlib.lines import Line2D
        legend_elements = [
            Line2D([0], [0], color='green', linestyle='--', label='æ•°æ®æ³¨å…¥'),
            Line2D([0], [0], color='orange', linestyle='-.', label='å…‹é‡Œé‡‘é‡é‡‡æ ·')
        ]
        
        # åˆ›å»ºç¬¬äºŒä¸ªå›¾ä¾‹
        second_legend = ax.legend(handles=legend_elements, loc='upper right', 
                                 fontsize=10, title='é‡è¦äº‹ä»¶', title_fontsize=11)
        ax.add_artist(second_legend)  # ä¿æŒåŸæœ‰å›¾ä¾‹
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('è®­ç»ƒè½®æ•° (Epochs)', fontsize=12)
    ax.set_ylabel('å¹³å‡ç›¸å¯¹è¯¯å·® (MRE)', fontsize=12)
    ax.set_title('è‡ªé€‚åº”PINN vs åŸºçº¿PINN: è®­ç»ƒè¿‡ç¨‹MREå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(loc='center right', fontsize=11)  # è°ƒæ•´åŸå›¾ä¾‹ä½ç½®
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡æ›´å¥½åœ°æ˜¾ç¤ºè¯¯å·®å˜åŒ–
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path(__file__).parent / "results"
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / "mre_comparison.png", dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / "mre_comparison.pdf", bbox_inches='tight')
    
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   - PNGæ ¼å¼: {output_dir / 'mre_comparison.png'}")
    print(f"   - PDFæ ¼å¼: {output_dir / 'mre_comparison.pdf'}")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()

if __name__ == "__main__":
    main() 