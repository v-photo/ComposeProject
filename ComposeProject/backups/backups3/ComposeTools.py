"""
GPU-Accelerated Block Kriging Ã— PINN è€¦åˆé‡å»ºå·¥å…·æ¨¡å—
GPU-Accelerated Block Kriging Ã— PINN Coupling Reconstruction Tools

åŠŸèƒ½æ¦‚è¿° (Functionality Overview):
- é€šç”¨å·¥å…· (Common Tools): æ•°æ®æ ‡å‡†åŒ–ã€è¯¯å·®ç»Ÿè®¡ã€å¯è§†åŒ–
- æ–¹æ¡ˆ1ä¸“ç”¨ (Mode 1 Specific): PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ
- æ–¹æ¡ˆ2ä¸“ç”¨ (Mode 2 Specific): Kriging ROIæ ·æœ¬æ‰©å…… â†’ PINNé‡è®­ç»ƒ

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
import time
from dataclasses import dataclass
from pathlib import Path
import pickle

# ==================== è€¦åˆé¡¹ç›®åŸæœ‰å·¥å…·å’Œæ¨¡å—å¯¼å…¥ ====================
from PINN.pinn_core import SimulationConfig, PINNTrainer, ResultAnalyzer
from PINN.data_processing import DataLoader
from PINN.visualization import Visualizer # <--- ä¿®æ”¹è¿™é‡Œ
from PINN.tools import setup_deepxde_backend
from PINN.dataAnalysis import get_data

# å°è¯•å¯¼å…¥æ‰€éœ€çš„ç¬¬ä¸‰æ–¹åº“
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    warnings.warn("PyTorchä¸å¯ç”¨ï¼Œéƒ¨åˆ†GPUåŠ é€ŸåŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import cupy as cp
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    warnings.warn("CuPyä¸å¯ç”¨ï¼ŒGPUåŠ é€ŸåŠŸèƒ½å°†è¢«ç¦ç”¨")

# å¯¼å…¥ç°æœ‰æ¨¡å— - éœ€è¦ç¡®ä¿è·¯å¾„æ­£ç¡®
current_dir = Path(__file__).parent
project_root = current_dir.parent

# æ·»åŠ Krigingæ¨¡å—è·¯å¾„
sys.path.insert(0, str(project_root / "Kriging"))

try:
    # å¯¼å…¥Krigingæ¨¡å—
    from myKriging import training as kriging_training, testing as kriging_testing
    from myPyKriging3D import MyOrdinaryKriging3D
    KRIGING_AVAILABLE = True
    print("âœ… Krigingæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    KRIGING_AVAILABLE = False
    warnings.warn(f"Krigingæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# æ·»åŠ PINNæ¨¡å—è·¯å¾„
sys.path.insert(0, str(project_root / "PINN"))
try:
    from PINN.pinn_core import SimulationConfig, PINNTrainer, ResultAnalyzer
    from PINN.data_processing import DataLoader
    from PINN.visualization import Visualizer # <--- ä¿®æ”¹è¿™é‡Œ
    from PINN.tools import setup_deepxde_backend
    from PINN.dataAnalysis import get_data
    PINN_AVAILABLE = True
    print("âœ… PINNæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    PINN_AVAILABLE = False
    warnings.warn(f"PINNæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# ==================== å…¨å±€å¸¸é‡ä¸é…ç½® ====================
# Global Constants and Configuration

EPSILON = 1e-30  # æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
DEFAULT_METRICS = ['MAE', 'RMSE', 'MAPE', 'R2', 'MRE']

@dataclass
class ComposeConfig:
    """
    è€¦åˆç³»ç»Ÿå…¨å±€é…ç½®ç±»
    Global configuration for the coupling system
    """
    # é€šç”¨é…ç½® Common settings
    gpu_enabled: bool = True
    verbose: bool = True
    random_seed: int = 42
    
    # Krigingé…ç½® Kriging settings
    kriging_variogram_model: str = "exponential"
    kriging_block_size: int = 10000
    kriging_enable_uncertainty: bool = True  # æ³¨æ„ï¼šå½“å‰å®ç°å¯èƒ½ä¸å®Œå…¨æ”¯æŒ
    
    # è€¦åˆé…ç½® Coupling settings
    fusion_weight: float = 0.5  # æ–¹æ¡ˆ1ä¸­çš„æƒé‡Ï‰
    roi_detection_strategy: str = 'high_density'  # æ–¹æ¡ˆ2ä¸­çš„ROIæ£€æµ‹ç­–ç•¥
    sample_augment_factor: float = 2.0  # æ–¹æ¡ˆ2ä¸­çš„æ ·æœ¬æ‰©å……å€æ•°
    
# ==================== é€šç”¨å·¥å…· (Common Tools) ====================
class MetricsCalculator:
    """
    è¯¯å·®ç»Ÿè®¡è®¡ç®—å™¨
    Error metrics calculator
    """
    
    @staticmethod
    def compute_metrics(true_values: np.ndarray, 
                       pred_values: np.ndarray,
                       metrics: List[str] = None) -> Dict[str, float]:
        """
        è®¡ç®—é¢„æµ‹è¯¯å·®æŒ‡æ ‡
        Compute prediction error metrics
        
        Args:
            true_values: çœŸå®å€¼
            pred_values: é¢„æµ‹å€¼  
            metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨
            
        Returns:
            Dict[metric_name, metric_value]
        """
        if metrics is None:
            metrics = DEFAULT_METRICS
            
        # ç¡®ä¿è¾“å…¥ä¸ºnumpyæ•°ç»„
        true_values = np.asarray(true_values).flatten()
        pred_values = np.asarray(pred_values).flatten()
        
        if len(true_values) != len(pred_values):
            raise ValueError("çœŸå®å€¼å’Œé¢„æµ‹å€¼çš„é•¿åº¦ä¸åŒ¹é…")
        
        results = {}
        
        # è®¡ç®—æ®‹å·®
        residuals = pred_values - true_values
        # å¹³å‡ç›¸å¯¹è¯¯å·® Mean Relative Error
        if 'MRE' in metrics:
            results['MRE'] = np.mean(np.abs(residuals / true_values))
        
        # å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute Error
        if 'MAE' in metrics:
            results['MAE'] = np.mean(np.abs(residuals))
        
        # å‡æ–¹æ ¹è¯¯å·® Root Mean Square Error  
        if 'RMSE' in metrics:
            results['RMSE'] = np.sqrt(np.mean(residuals**2))
        
        # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® Mean Absolute Percentage Error
        # åªåœ¨éé›¶çœŸå€¼å¤„è®¡ç®—
        if 'MAPE' in metrics:
            nonzero_mask = np.abs(true_values) > EPSILON
            if np.any(nonzero_mask):
                mape_values = np.abs(residuals[nonzero_mask] / true_values[nonzero_mask])
                results['MAPE'] = np.mean(mape_values) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            else:
                results['MAPE'] = float('inf')
        
        # å†³å®šç³»æ•° R-squared
        if 'R2' in metrics:
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((true_values - np.mean(true_values))**2)
            results['R2'] = 1 - (ss_res / (ss_tot + EPSILON))
        
        # ç›¸å…³ç³»æ•° Pearson correlation
        if 'CORR' in metrics:
            correlation_matrix = np.corrcoef(true_values, pred_values)
            results['CORR'] = correlation_matrix[0, 1] if not np.isnan(correlation_matrix[0, 1]) else 0.0
        
        return results
    
    @staticmethod
    def compute_relative_error_stats(true_values: np.ndarray, 
                                   pred_values: np.ndarray,
                                   percentiles: List[float] = None) -> Dict[str, float]:
        """
        è®¡ç®—ç›¸å¯¹è¯¯å·®çš„ç»Ÿè®¡åˆ†å¸ƒ
        Compute relative error statistics
        """
        if percentiles is None:
            percentiles = [10, 25, 50, 75, 90, 95, 99]
        
        # åªåœ¨éé›¶çœŸå€¼å¤„è®¡ç®—ç›¸å¯¹è¯¯å·®
        nonzero_mask = np.abs(true_values) > EPSILON
        if not np.any(nonzero_mask):
            return {f'P{p}': float('inf') for p in percentiles}
        
        relative_errors = np.abs((pred_values[nonzero_mask] - true_values[nonzero_mask]) 
                                / true_values[nonzero_mask]) * 100
        
        stats = {}
        for p in percentiles:
            stats[f'P{p}'] = np.percentile(relative_errors, p)
        
        stats['mean_rel_error'] = np.mean(relative_errors)
        stats['std_rel_error'] = np.std(relative_errors)
        
        return stats

class VisualizationTools:
    """
    å¯è§†åŒ–å·¥å…·é›†
    Visualization utilities
    """
    
    @staticmethod
    def plot_comparison_2d_slice(true_field: np.ndarray,
                               pred_field: np.ndarray, 
                               slice_axis: int = 2,
                               slice_idx: Optional[int] = None,
                               uncertainty_field: Optional[np.ndarray] = None,
                               save_path: Optional[str] = None,
                               title_prefix: str = "") -> plt.Figure:
        """
        ç»˜åˆ¶2Dåˆ‡ç‰‡å¯¹æ¯”å›¾
        Plot 2D slice comparison
        
        Args:
            true_field: çœŸå®åœº (nx, ny, nz)
            pred_field: é¢„æµ‹åœº (nx, ny, nz)
            slice_axis: åˆ‡ç‰‡è½´ (0=x, 1=y, 2=z)
            slice_idx: åˆ‡ç‰‡ç´¢å¼•ï¼ŒNoneåˆ™ä½¿ç”¨ä¸­é—´åˆ‡ç‰‡
            uncertainty_field: ä¸ç¡®å®šåº¦åœºï¼ˆå¯é€‰ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            title_prefix: æ ‡é¢˜å‰ç¼€
            
        Returns:
            matplotlib Figureå¯¹è±¡
        """
        if slice_idx is None:
            slice_idx = true_field.shape[slice_axis] // 2
        
        # æå–åˆ‡ç‰‡
        if slice_axis == 0:
            true_slice = true_field[slice_idx, :, :]
            pred_slice = pred_field[slice_idx, :, :]
            uncertainty_slice = uncertainty_field[slice_idx, :, :] if uncertainty_field is not None else None
        elif slice_axis == 1:
            true_slice = true_field[:, slice_idx, :]
            pred_slice = pred_field[:, slice_idx, :]
            uncertainty_slice = uncertainty_field[:, slice_idx, :] if uncertainty_field is not None else None
        else:  # slice_axis == 2
            true_slice = true_field[:, :, slice_idx]
            pred_slice = pred_field[:, :, slice_idx]
            uncertainty_slice = uncertainty_field[:, :, slice_idx] if uncertainty_field is not None else None
        
        # åˆ›å»ºå­å›¾å¸ƒå±€
        n_plots = 3 if uncertainty_slice is not None else 2
        fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))
        if n_plots == 2:
            axes = [axes[0], axes[1]]
        
        # ç»˜åˆ¶çœŸå®åœº
        im1 = axes[0].imshow(true_slice.T, origin='lower', aspect='auto', 
                           norm=LogNorm(vmin=max(true_slice.min(), EPSILON), vmax=true_slice.max()))
        axes[0].set_title(f'{title_prefix}çœŸå®åœº (è½´{slice_axis}, åˆ‡ç‰‡{slice_idx})')
        axes[0].set_xlabel('X' if slice_axis != 0 else ('Y' if slice_axis == 2 else 'Z'))
        axes[0].set_ylabel('Y' if slice_axis != 1 else ('X' if slice_axis == 2 else 'Z'))
        plt.colorbar(im1, ax=axes[0])
        
        # ç»˜åˆ¶é¢„æµ‹åœº
        im2 = axes[1].imshow(pred_slice.T, origin='lower', aspect='auto',
                           norm=LogNorm(vmin=max(pred_slice.min(), EPSILON), vmax=pred_slice.max()))
        axes[1].set_title(f'{title_prefix}é¢„æµ‹åœº')
        axes[1].set_xlabel('X' if slice_axis != 0 else ('Y' if slice_axis == 2 else 'Z'))
        axes[1].set_ylabel('Y' if slice_axis != 1 else ('X' if slice_axis == 2 else 'Z'))
        plt.colorbar(im2, ax=axes[1])
        
        # ç»˜åˆ¶ä¸ç¡®å®šåº¦åœºï¼ˆå¦‚æœæä¾›ï¼‰
        if uncertainty_slice is not None:
            im3 = axes[2].imshow(uncertainty_slice.T, origin='lower', aspect='auto')
            axes[2].set_title(f'{title_prefix}ä¸ç¡®å®šåº¦')
            axes[2].set_xlabel('X' if slice_axis != 0 else ('Y' if slice_axis == 2 else 'Z'))
            axes[2].set_ylabel('Y' if slice_axis != 1 else ('X' if slice_axis == 2 else 'Z'))
            plt.colorbar(im3, ax=axes[2])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            
        return fig
    
    @staticmethod
    def plot_residual_analysis(residuals: np.ndarray,
                             coordinates: Optional[np.ndarray] = None,
                             save_path: Optional[str] = None) -> plt.Figure:
        """
        æ®‹å·®åˆ†æå¯è§†åŒ–
        Residual analysis visualization
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æ®‹å·®ç›´æ–¹å›¾
        axes[0, 0].hist(residuals, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(np.mean(residuals), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(residuals):.2e}')
        axes[0, 0].axvline(np.median(residuals), color='orange', linestyle='--', label=f'ä¸­ä½æ•°: {np.median(residuals):.2e}')
        axes[0, 0].set_xlabel('æ®‹å·®å€¼')
        axes[0, 0].set_ylabel('é¢‘ç‡')
        axes[0, 0].set_title('æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Q-Qå›¾
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('æ®‹å·®Q-Qå›¾ (æ­£æ€æ€§æ£€éªŒ)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ®‹å·®ç»å¯¹å€¼vsé¢„æµ‹å€¼ï¼ˆå¦‚æœæœ‰åæ ‡ä¿¡æ¯ï¼‰
        if coordinates is not None and coordinates.shape[0] == len(residuals):
            # ä½¿ç”¨zåæ ‡ä½œä¸ºå‚è€ƒ
            z_coords = coordinates[:, 2]
            scatter = axes[1, 0].scatter(z_coords, np.abs(residuals), alpha=0.6, c=np.abs(residuals), cmap='viridis')
            axes[1, 0].set_xlabel('Zåæ ‡')
            axes[1, 0].set_ylabel('æ®‹å·®ç»å¯¹å€¼')
            axes[1, 0].set_title('æ®‹å·®ç»å¯¹å€¼ vs Zåæ ‡')
            plt.colorbar(scatter, ax=axes[1, 0])
        else:
            # æ®‹å·®ç»å¯¹å€¼vsç´¢å¼•
            axes[1, 0].plot(np.abs(residuals), 'o', alpha=0.6, markersize=3)
            axes[1, 0].set_xlabel('æ ·æœ¬ç´¢å¼•')
            axes[1, 0].set_ylabel('æ®‹å·®ç»å¯¹å€¼')
            axes[1, 0].set_title('æ®‹å·®ç»å¯¹å€¼åˆ†å¸ƒ')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ®‹å·®ç»Ÿè®¡æ‘˜è¦
        axes[1, 1].axis('off')
        stats_text = f"""
æ®‹å·®ç»Ÿè®¡æ‘˜è¦:

åŸºæœ¬ç»Ÿè®¡é‡:
â€¢ æ ·æœ¬æ•°é‡: {len(residuals)}
â€¢ å‡å€¼: {np.mean(residuals):.4e}
â€¢ æ ‡å‡†å·®: {np.std(residuals):.4e}
â€¢ æœ€å°å€¼: {np.min(residuals):.4e}
â€¢ æœ€å¤§å€¼: {np.max(residuals):.4e}

åˆ†ä½æ•°:
â€¢ 25%: {np.percentile(residuals, 25):.4e}
â€¢ 50%: {np.percentile(residuals, 50):.4e}
â€¢ 75%: {np.percentile(residuals, 75):.4e}

è´¨é‡æŒ‡æ ‡:
â€¢ MAE: {np.mean(np.abs(residuals)):.4e}
â€¢ RMSE: {np.sqrt(np.mean(residuals**2)):.4e}
â€¢ ååº¦: {stats.skew(residuals):.4f}
â€¢ å³°åº¦: {stats.kurtosis(residuals):.4f}
        """
        axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=10, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        
        if save_path:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            
        return fig

    @staticmethod
    def plot_pinn_error_analysis(train_errors: np.ndarray, 
                                train_points: np.ndarray,
                                pinn_predictions: np.ndarray,
                                true_values: np.ndarray,
                                save_path: Optional[str] = None) -> plt.Figure:
        """
        PINNè¯¯å·®æ·±åº¦åˆ†æå¯è§†åŒ–
        PINN error deep analysis visualization
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è¯¯å·®vsçœŸå®å€¼æ•£ç‚¹å›¾
        axes[0, 0].scatter(true_values, train_errors, alpha=0.6, c='blue', s=20)
        axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=1)
        axes[0, 0].set_xlabel('çœŸå®å€¼')
        axes[0, 0].set_ylabel('é¢„æµ‹è¯¯å·®')
        axes[0, 0].set_title('è¯¯å·® vs çœŸå®å€¼')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(true_values, train_errors, 1)
        p = np.poly1d(z)
        axes[0, 0].plot(true_values, p(true_values), "r--", alpha=0.8, linewidth=1)
        
        # 2. è¯¯å·®vsé¢„æµ‹å€¼æ•£ç‚¹å›¾
        axes[0, 1].scatter(pinn_predictions, train_errors, alpha=0.6, c='green', s=20)
        axes[0, 1].axhline(0, color='red', linestyle='--', linewidth=1)
        axes[0, 1].set_xlabel('PINNé¢„æµ‹å€¼')
        axes[0, 1].set_ylabel('é¢„æµ‹è¯¯å·®')
        axes[0, 1].set_title('è¯¯å·® vs PINNé¢„æµ‹å€¼')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. 3Dç©ºé—´è¯¯å·®åˆ†å¸ƒ
        ax_3d = fig.add_subplot(2, 3, 3, projection='3d')
        scatter = ax_3d.scatter(train_points[:, 0], train_points[:, 1], train_points[:, 2], 
                               c=np.abs(train_errors), cmap='hot', s=30, alpha=0.7)
        ax_3d.set_xlabel('X')
        ax_3d.set_ylabel('Y') 
        ax_3d.set_zlabel('Z')
        ax_3d.set_title('3Dç©ºé—´è¯¯å·®åˆ†å¸ƒ')
        plt.colorbar(scatter, ax=ax_3d, shrink=0.8)
        
        # 4. è¯¯å·®ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
        sorted_errors = np.sort(np.abs(train_errors))
        y_vals = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)
        axes[1, 0].plot(sorted_errors, y_vals, linewidth=2, color='purple')
        axes[1, 0].set_xlabel('è¯¯å·®ç»å¯¹å€¼')
        axes[1, 0].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
        axes[1, 0].set_title('è¯¯å·®ç´¯ç§¯åˆ†å¸ƒå‡½æ•°')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ å…³é”®ç™¾åˆ†ä½çº¿
        percentiles = [50, 80, 90, 95]
        colors = ['blue', 'orange', 'red', 'darkred']
        for p, color in zip(percentiles, colors):
            error_val = np.percentile(np.abs(train_errors), p)
            axes[1, 0].axvline(error_val, color=color, linestyle='--', alpha=0.7, 
                              label=f'{p}%: {error_val:.2e}')
        axes[1, 0].legend()
        
        # 5. é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾
        axes[1, 1].scatter(true_values, pinn_predictions, alpha=0.6, c='cyan', s=20)
        
        # å®Œç¾é¢„æµ‹çº¿
        min_val = min(np.min(true_values), np.min(pinn_predictions))
        max_val = max(np.max(true_values), np.max(pinn_predictions))
        axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
        
        axes[1, 1].set_xlabel('çœŸå®å€¼')
        axes[1, 1].set_ylabel('PINNé¢„æµ‹å€¼')
        axes[1, 1].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. è¯¯å·®ç»Ÿè®¡æ‘˜è¦è¡¨æ ¼
        axes[1, 2].axis('off')
        
        # è®¡ç®—ç›¸å…³æ€§
        from scipy.stats import pearsonr, spearmanr
        pearson_corr, _ = pearsonr(true_values, pinn_predictions)
        spearman_corr, _ = spearmanr(true_values, pinn_predictions)
        
        stats_text = f"""
PINNé¢„æµ‹æ€§èƒ½è¯¦ç»†åˆ†æ:

åŸºæœ¬è¯¯å·®ç»Ÿè®¡:
â€¢ MAE: {np.mean(np.abs(train_errors)):.4e}
â€¢ RMSE: {np.sqrt(np.mean(train_errors**2)):.4e}
â€¢ MAPE: {np.mean(np.abs(train_errors)/(np.abs(true_values)+1e-8))*100:.2f}%
â€¢ æœ€å¤§è¯¯å·®: {np.max(np.abs(train_errors)):.4e}

ç›¸å…³æ€§åˆ†æ:
â€¢ Pearsonç›¸å…³ç³»æ•°: {pearson_corr:.4f}
â€¢ Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f}
â€¢ RÂ²å†³å®šç³»æ•°: {1 - np.sum(train_errors**2)/np.sum((true_values-np.mean(true_values))**2):.4f}

è¯¯å·®åˆ†å¸ƒ:
â€¢ è¯¯å·®å‡å€¼: {np.mean(train_errors):.4e}
â€¢ è¯¯å·®æ ‡å‡†å·®: {np.std(train_errors):.4e}
â€¢ æ­£åè¯¯å·®æ¯”ä¾‹: {np.sum(train_errors>0)/len(train_errors)*100:.1f}%
â€¢ è´Ÿåè¯¯å·®æ¯”ä¾‹: {np.sum(train_errors<0)/len(train_errors)*100:.1f}%

æ•°æ®èŒƒå›´:
â€¢ çœŸå®å€¼èŒƒå›´: [{np.min(true_values):.2e}, {np.max(true_values):.2e}]
â€¢ é¢„æµ‹å€¼èŒƒå›´: [{np.min(pinn_predictions):.2e}, {np.max(pinn_predictions):.2e}]
        """
        
        axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes,
                        fontsize=9, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        plt.tight_layout()
        
        if save_path:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            
        return fig

# ==================== æ¨¡å‹é€‚é…å™¨ (Model Adapters) ====================
class KrigingAdapter:
    """
    Krigingæ¨¡å—çš„æ ‡å‡†åŒ–æ¥å£é€‚é…å™¨
    Standardized interface adapter for Kriging module
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.model = None
        self.is_fitted = False
        
    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> 'KrigingAdapter':
        """
        æ ‡å‡†åŒ–çš„fitæ¥å£
        Standardized fit interface
        
        Args:
            X: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            y: è®­ç»ƒç‚¹æ•°å€¼ (N,)
            **kwargs: é¢å¤–çš„krigingå‚æ•°
            
        Returns:
            self
        """
        if not KRIGING_AVAILABLE:
            raise RuntimeError("Krigingæ¨¡å—ä¸å¯ç”¨")
            
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼ˆå…¼å®¹ç°æœ‰æ¥å£ï¼‰
        df = pd.DataFrame({
            'x': X[:, 0],
            'y': X[:, 1], 
            'z': X[:, 2],
            'target': y
        })
        
        # ä½¿ç”¨ç°æœ‰çš„trainingå‡½æ•°
        variogram_model = kwargs.get('variogram_model', self.config.kriging_variogram_model)
        self.model = kriging_training(
            df=df,
            variogram_model=variogram_model,
            nlags=kwargs.get('nlags', 8),
            enable_plotting=kwargs.get('enable_plotting', False),
            weight=kwargs.get('weight', False),
            uk=kwargs.get('uk', False),
            cpu_on=not self.config.gpu_enabled
        )
        
        self.is_fitted = True
        return self
    
    def predict(self, X: np.ndarray, return_std: bool = False, **kwargs) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        æ ‡å‡†åŒ–çš„predictæ¥å£
        Standardized predict interface
        
        Args:
            X: é¢„æµ‹ç‚¹åæ ‡ (N, 3)
            return_std: æ˜¯å¦è¿”å›æ ‡å‡†å·®ï¼ˆä¸ç¡®å®šåº¦ï¼‰
            **kwargs: é¢å¤–çš„é¢„æµ‹å‚æ•°
            
        Returns:
            predictions æˆ– (predictions, std)
        """
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")
            
        if not KRIGING_AVAILABLE:
            raise RuntimeError("Krigingæ¨¡å—ä¸å¯ç”¨")
        
        # å°†é¢„æµ‹ç‚¹è½¬æ¢ä¸ºDataFrameæ ¼å¼
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æä¾›è™šæ‹Ÿçš„targetåˆ—ï¼Œä½†ä¸ä¼šè¢«ä½¿ç”¨
        df_pred = pd.DataFrame({
            'x': X[:, 0],
            'y': X[:, 1],
            'z': X[:, 2], 
            'target': np.zeros(X.shape[0])  # è™šæ‹Ÿå€¼
        })
        
        # ä½¿ç”¨ç°æœ‰çš„testingå‡½æ•°è¿›è¡Œé¢„æµ‹
        predictions, _ = kriging_testing(
            df=df_pred,
            model=self.model,
            block_size=kwargs.get('block_size', self.config.kriging_block_size),
            cpu_on=not self.config.gpu_enabled,
            style=kwargs.get('style', "gpu_b"),
            multi_process=kwargs.get('multi_process', False),
            print_time=kwargs.get('print_time', False),
            torch_ac=kwargs.get('torch_ac', False),
            compute_precision=False  # å…³é—­ç²¾åº¦è®¡ç®—é¿å…æ··æ·†
        )
        
        if return_std and self.config.kriging_enable_uncertainty:
            # æ³¨æ„ï¼šå½“å‰å®ç°å¯èƒ½ä¸å®Œå…¨æ”¯æŒå…¨å±€ÏƒÂ²è¾“å‡º
            # TODO: éœ€è¦ä¿®æ”¹ç°æœ‰Krigingä»£ç ä»¥æ­£ç¡®è¿”å›ä¸ç¡®å®šåº¦
            warnings.warn("å½“å‰Krigingå®ç°æš‚ä¸å®Œå…¨æ”¯æŒå…¨å±€ÏƒÂ²è¾“å‡ºï¼Œè¿”å›çš„ä¸ç¡®å®šåº¦å¯èƒ½ä¸å‡†ç¡®")
            
            # ä¸´æ—¶æ–¹æ¡ˆï¼šä½¿ç”¨executeæ–¹æ³•ç›´æ¥è·å–æ–¹å·®
            try:
                _, variances = self.model.execute(
                    style='points',
                    xpoints=X[:, 0],
                    ypoints=X[:, 1], 
                    zpoints=X[:, 2],
                    block_size=kwargs.get('block_size', self.config.kriging_block_size),
                    cpu_on=not self.config.gpu_enabled
                )
                std_values = np.sqrt(np.maximum(variances, 0))  # ç¡®ä¿éè´Ÿ
                return predictions, std_values
            except Exception as e:
                warnings.warn(f"è·å–ä¸ç¡®å®šåº¦å¤±è´¥: {e}ï¼Œè¿”å›é›¶ä¸ç¡®å®šåº¦")
                return predictions, np.zeros_like(predictions)
        else:
            return predictions

class PINNAdapter:
    """
    PINNæ¨¡å‹çš„æ ‡å‡†åŒ–æ¥å£é€‚é…å™¨
    Standardized interface adapter for the PINN model
    """
    
    def __init__(self, physical_params: Dict, config: ComposeConfig = None):
        """
        åˆå§‹åŒ–PINNé€‚é…å™¨
        """
        self.config = config or ComposeConfig()
        if not PINN_AVAILABLE:
            raise RuntimeError("PINNæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºPINNAdapter")
        if not physical_params:
            raise ValueError("PINNAdapteréœ€è¦ä¸€ä¸ªåŒ…å«ç‰©ç†å‚æ•°çš„å­—å…¸ 'physical_params'")
        
        self.trainer = PINNTrainer(physical_params=physical_params)
        self.dose_data = None  # ç”¨äºå­˜å‚¨åŠ è½½çš„æ•°æ®
        self.is_fitted = False

    def fit_from_memory(self,
                        train_points: np.ndarray,
                        train_values: np.ndarray,
                        dose_data: Dict, 
                        sample_weights: Optional[np.ndarray] = None,
                        **kwargs) -> 'PINNAdapter':
        """
        ä½¿ç”¨å†…å­˜ä¸­çš„è®­ç»ƒæ•°æ®ç‚¹è®­ç»ƒPINNæ¨¡å‹ã€‚
        ä¼šè‡ªåŠ¨å¤„ç†å¯¹æ•°è½¬æ¢ã€‚æ­¤æ–¹æ³•ä¸“ä¸ºè€¦åˆå·¥ä½œæµè®¾è®¡ã€‚
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: è®­ç»ƒç‚¹æ•°å€¼ (N,)
            dose_data: å‰‚é‡æ•°æ®å­—å…¸
            sample_weights: æ ·æœ¬æƒé‡ (N,)ï¼Œå¯é€‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            self
        """
        print("INFO: å¼€å§‹æ‰§è¡Œ PINNAdapter.fit_from_memory()")
        
        # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡ (è½¬æ¢ç‰©ç†å€¼ä¸ºå¯¹æ•°å€¼)
        print(f"      - æ­¥éª¤1: è½¬æ¢ {len(train_values)} ä¸ªè®­ç»ƒç‚¹çš„ç‰©ç†å€¼ä¸ºå¯¹æ•°å€¼...")
        sampled_log_doses = np.log(np.maximum(train_values, EPSILON))
        print("      - å¯¹æ•°è½¬æ¢å®Œæˆã€‚")

        # æ­¥éª¤ 2: åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        print("      - æ­¥éª¤2: åˆ›å»ºå¹¶è®­ç»ƒPINNæ¨¡å‹...")
        network_config = kwargs.get('network_config', {'layers': [3] + [32] * 4 + [1], 'activation': 'tanh'})
        include_source = kwargs.get('include_source', False)
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ ·æœ¬æƒé‡
        if sample_weights is not None:
            print(f"      - æ£€æµ‹åˆ°æ ·æœ¬æƒé‡ï¼Œå°†ç”¨äºè®­ç»ƒ (æƒé‡èŒƒå›´: [{np.min(sample_weights):.4f}, {np.max(sample_weights):.4f}])")
            # ç¡®ä¿æƒé‡é•¿åº¦ä¸æ ·æœ¬æ•°é‡åŒ¹é…
            if len(sample_weights) != len(train_points):
                raise ValueError(f"æ ·æœ¬æƒé‡é•¿åº¦ ({len(sample_weights)}) ä¸è®­ç»ƒç‚¹æ•°é‡ ({len(train_points)}) ä¸åŒ¹é…")
            
            # å­˜å‚¨æ ·æœ¬æƒé‡ä¾›åç»­ä½¿ç”¨ï¼ˆå¦‚æœPINNæ¨¡å—æ”¯æŒï¼‰
            self._sample_weights = sample_weights
        else:
            print("      - æœªæä¾›æ ·æœ¬æƒé‡ï¼Œä½¿ç”¨å‡åŒ€æƒé‡")
            self._sample_weights = None
            
        self.trainer.create_pinn_model(
            dose_data=dose_data,
            sampled_points_xyz=train_points,
            sampled_log_doses_values=sampled_log_doses,
            include_source=include_source,
            network_config=network_config
        )
        
        epochs = kwargs.get('epochs', 10000)
        use_lbfgs = kwargs.get('use_lbfgs', True)
        loss_weights = kwargs.get('loss_weights', [1, 100])
        
        # å¦‚æœæœ‰æ ·æœ¬æƒé‡ï¼Œå°è¯•é€šè¿‡å…¶ä»–æ–¹å¼åº”ç”¨
        if self._sample_weights is not None:
            print("      - æ³¨æ„: å½“å‰PINNæ¨¡å—ä¸ç›´æ¥æ”¯æŒæ ·æœ¬æƒé‡ï¼Œå°†é€šè¿‡å…¶ä»–æ–¹å¼å®ç°")
            # è¿™é‡Œå¯ä»¥æ·»åŠ é€‚ç”¨äºæ‚¨PINNæ¨¡å—çš„æƒé‡å®ç°æ–¹å¼
            # ä¾‹å¦‚ï¼šé€šè¿‡ä¿®æ”¹æŸå¤±å‡½æ•°ã€é‡å¤æ•°æ®ç‚¹ç­‰
        
        self.trainer.train(
            epochs=epochs, 
            use_lbfgs=use_lbfgs, 
            loss_weights=loss_weights
        )
        
        self.is_fitted = True
        print("INFO: PINNAdapter.fit_from_memory() å®Œæˆ")
        return self

    def predict(self, prediction_points: np.ndarray) -> np.ndarray:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„PINNæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚
        æ ¹æ®çº¦å®šï¼Œæ­¤æ–¹æ³•ç›´æ¥è¿”å›æœ€ç»ˆçš„ç‰©ç†å‰‚é‡å€¼(çº¿æ€§å°ºåº¦)ã€‚
        """
        if not self.is_fitted:
            raise RuntimeError("PINNæ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")
            
        # æ ¹æ®çº¦å®šï¼Œtrainer.predict()è¿”å›çš„å°±æ˜¯æœ€ç»ˆçš„ç‰©ç†å‰‚é‡
        predicted_doses = self.trainer.predict(prediction_points)
        
        return predicted_doses.flatten()

def validate_compose_environment() -> Dict[str, bool]:
    """
    éªŒè¯è€¦åˆé¡¹ç›®çš„æ ¸å¿ƒä¾èµ–æ˜¯å¦å¯ç”¨
    Validate the core dependencies for the coupling project
    
    Returns:
        Dict of availability status for each component
    """
    status = {
        'Kriging': KRIGING_AVAILABLE,
        'CuPy': CUPY_AVAILABLE,
        'PyTorch': TORCH_AVAILABLE
    }
    
    print("\n=== ç¯å¢ƒæ£€æŸ¥ç»“æœ Environment Check ===")
    for component, available in status.items():
        status_str = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
        print(f"{component}: {status_str}")
    
    return status 

# ==================== æ–¹æ¡ˆ1ä¸“ç”¨åŠŸèƒ½ (Mode 1 Specific) ====================
# PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ
class Mode1ResidualKriging:
    """
    æ–¹æ¡ˆ1: æ®‹å·®å…‹é‡Œé‡‘æ’å€¼ä¸“ç”¨å·¥å…·
    Mode 1: Residual Kriging specific tools
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.kriging_adapter = KrigingAdapter(config)
        
    def compute_residuals(self, 
                         train_points: np.ndarray,
                         train_values: np.ndarray, 
                         pinn_predictions: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—PINNé¢„æµ‹ä¸çœŸå®å€¼çš„æ®‹å·®
        Compute residuals between PINN predictions and true values
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: çœŸå®è®­ç»ƒå€¼ (N,)
            pinn_predictions: PINNåœ¨è®­ç»ƒç‚¹çš„é¢„æµ‹å€¼ (N,)
            
        Returns:
            residuals: æ®‹å·® = çœŸå®å€¼ - PINNé¢„æµ‹å€¼ (N,)
        """
        if len(train_values) != len(pinn_predictions):
            raise ValueError("çœŸå®å€¼å’ŒPINNé¢„æµ‹å€¼çš„é•¿åº¦ä¸åŒ¹é…")
            
        residuals = train_values - pinn_predictions
        
        # æ£€æŸ¥å¹¶ä¿®å¤å¼‚å¸¸å€¼
        valid_mask = np.isfinite(residuals)
        if not np.all(valid_mask):
            print(f"       âš ï¸ å‘ç° {np.sum(~valid_mask)} ä¸ªæ— æ•ˆæ®‹å·®å€¼ï¼Œå°†è¿›è¡Œä¿®å¤")
            residuals = residuals[valid_mask]
            train_points = train_points[valid_mask]
            train_values = train_values[valid_mask]
            pinn_predictions = pinn_predictions[valid_mask]
        
        # æ£€æŸ¥æ®‹å·®çš„æ•°å€¼ç‰¹æ€§
        residual_std = np.std(residuals)
        residual_range = np.max(residuals) - np.min(residuals)
        
        if residual_std < 1e-10 or residual_range < 1e-10:
            # å¦‚æœæ®‹å·®å˜åŒ–æå°ï¼Œè¯´æ˜PINNé¢„æµ‹è¿‡äºä¸€è‡´ï¼Œéœ€è¦æ·»åŠ ç©ºé—´ç»“æ„
            print("       ğŸ”§ æ£€æµ‹åˆ°æ®‹å·®ç©ºé—´å˜åŒ–è¿‡å°ï¼Œæ·»åŠ åŸºäºä½ç½®çš„å¾®æ‰°ä»¥æ”¹å–„Krigingå»ºæ¨¡")
            
            # åŸºäºç©ºé—´ä½ç½®æ·»åŠ å¾®æ‰°ï¼Œä¿æŒç©ºé—´ç›¸å…³æ€§
            spatial_weights = np.linalg.norm(train_points - np.mean(train_points, axis=0), axis=1)
            spatial_weights = (spatial_weights - np.min(spatial_weights)) / (np.max(spatial_weights) - np.min(spatial_weights) + 1e-10)
            
            # æ·»åŠ ä¸ç©ºé—´ä½ç½®ç›¸å…³çš„å¾®æ‰°
            base_residual = np.mean(residuals)
            perturbation_scale = max(abs(base_residual) * 0.05, np.std(train_values) * 0.01, 1e-3)
            
            # ä½¿ç”¨ç©ºé—´æƒé‡ç”Ÿæˆå…·æœ‰ç©ºé—´ç»“æ„çš„å¾®æ‰°
            spatial_perturbation = perturbation_scale * (spatial_weights - 0.5) * 2
            random_perturbation = np.random.normal(0, perturbation_scale * 0.1, len(residuals))
            
            residuals = residuals + spatial_perturbation + random_perturbation
        
        # å¯¹æ®‹å·®è¿›è¡Œåˆç†æ€§æ£€æŸ¥å’Œè£å‰ª
        residuals = np.clip(residuals, -1e6, 1e6)
        
        if self.config.verbose:
            print(f"       ğŸ“Š æ®‹å·®ç»Ÿè®¡: å‡å€¼={np.mean(residuals):.4e}, æ ‡å‡†å·®={np.std(residuals):.4e}")
            print(f"       ğŸ“ˆ æ®‹å·®èŒƒå›´: [{np.min(residuals):.4e}, {np.max(residuals):.4e}]")
            
        return residuals
        
    def residual_kriging(self,
                        train_points: np.ndarray,
                        train_values: np.ndarray,
                        pinn_predictions: np.ndarray,
                        prediction_points: np.ndarray,
                        return_uncertainty: bool = True,
                        **kriging_params) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        å¯¹æ®‹å·®è¿›è¡Œå…‹é‡Œé‡‘æ’å€¼
        Perform Kriging interpolation on residuals
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: çœŸå®è®­ç»ƒå€¼ (N,)
            pinn_predictions: PINNåœ¨è®­ç»ƒç‚¹çš„é¢„æµ‹å€¼ (N,)
            prediction_points: é¢„æµ‹ç‚¹åæ ‡ (M, 3)
            return_uncertainty: æ˜¯å¦è¿”å›ä¸ç¡®å®šåº¦
            **kriging_params: å…‹é‡Œé‡‘å‚æ•°
            
        Returns:
            residual_predictions: æ®‹å·®é¢„æµ‹ (M,)
            å¦‚æœreturn_uncertainty=True: (residual_predictions, residual_std)
        """
        # è®¡ç®—æ®‹å·®
        print(f"       ğŸ§® è®¡ç®—æ®‹å·® = çœŸå®å€¼ - PINNé¢„æµ‹å€¼...")
        residuals = self.compute_residuals(train_points, train_values, pinn_predictions)
        
        # è®­ç»ƒæ®‹å·®å…‹é‡Œé‡‘æ¨¡å‹
        print(f"       ğŸ—ï¸ è®­ç»ƒæ®‹å·®å…‹é‡Œé‡‘æ¨¡å‹ (å˜å¼‚å‡½æ•°: {kriging_params.get('variogram_model', 'exponential')})...")
        self.kriging_adapter.fit(train_points, residuals, **kriging_params)
        print(f"       âœ… æ®‹å·®å…‹é‡Œé‡‘æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # ==================== çº¯è°ƒè¯•ä»£ç : æµ‹è¯•Krigingå¯¹è®­ç»ƒæ®‹å·®çš„è‡ªé¢„æµ‹ç²¾åº¦ ====================
        try:
            print("\n" + "-"*20 + " DEBUG: Kriging Self-Prediction Test " + "-"*20)
            # ä½¿ç”¨åˆšåˆšè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåœ¨è®­ç»ƒç‚¹ä¸Šè¿›è¡Œé¢„æµ‹
            kriging_train_pred = self.kriging_adapter.predict(train_points, return_std=False)
            
            # è®¡ç®—Krigingé¢„æµ‹å€¼ä¸çœŸå®æ®‹å·®ä¹‹é—´çš„å¹³å‡ç›¸å¯¹è¯¯å·® (MRE)
            abs_true_residuals = np.abs(residuals)
            
            # é¿å…é™¤ä»¥é›¶
            valid_mask = abs_true_residuals > EPSILON
            if np.any(valid_mask):
                relative_errors = np.abs(residuals[valid_mask] - kriging_train_pred[valid_mask]) / abs_true_residuals[valid_mask]
                mre = np.mean(relative_errors)
                print(f"âœ… Kriging on Training Residuals MRE: {mre:.6f}")
                
                # é¢å¤–æä¾›ä¸€äº›ç»Ÿè®¡
                print(f"  - Test points count: {np.sum(valid_mask)}")
                print(f"  - True Residuals (on test points): Mean={np.mean(residuals[valid_mask]):.4e}, Std={np.std(residuals[valid_mask]):.4e}")
                print(f"  - Predicted Residuals (on test points): Mean={np.mean(kriging_train_pred[valid_mask]):.4e}, Std={np.std(kriging_train_pred[valid_mask]):.4e}")

            else:
                print("âš ï¸ Kriging MRE test skipped: All true residual values are close to zero.")

            # --- æ–°å¢ï¼šçœŸå®æ®‹å·®åˆ†å¸ƒåˆ†æ ---
            print("\n" + "--- True Residuals Distribution Analysis ---")
            zero_count = np.sum(np.isclose(residuals, 0))
            positive_count = np.sum(residuals > 0)
            negative_count = np.sum(residuals < 0)
            print(f"  - Total residuals: {len(residuals)}")
            print(f"  - Zero values (or close to zero): {zero_count}")
            print(f"  - Positive values: {positive_count}")
            print(f"  - Negative values: {negative_count}")

            if positive_count > 0:
                pos_residuals = residuals[residuals > 0]
                bins = [0, 1e-2, 1e-1, 1, 10, np.inf]
                hist, bin_edges = np.histogram(pos_residuals, bins=bins)
                print("  - Positive residuals breakdown:")
                for i in range(len(hist)):
                    print(f"    - Range ({bin_edges[i]:.1e}, {bin_edges[i+1]:.1e}]: {hist[i]} points")
                    
            if negative_count > 0:
                neg_residuals_abs = np.abs(residuals[residuals < 0])
                bins = [0, 1e-2, 1e-1, 1, 10, np.inf]
                hist, bin_edges = np.histogram(neg_residuals_abs, bins=bins)
                print("  - Negative residuals (absolute value) breakdown:")
                for i in range(len(hist)):
                    print(f"    - Range ({bin_edges[i]:.1e}, {bin_edges[i+1]:.1e}]: {hist[i]} points")
            # --- åˆ†å¸ƒåˆ†æç»“æŸ ---

            print("-"*(40 + len(" DEBUG: Kriging Self-Prediction Test ")) + "\n")
        except Exception as e:
            print(f"âŒ DEBUG: Failed to test Kriging self-prediction: {e}")
        # ========================== è°ƒè¯•ä»£ç ç»“æŸ ==========================
        
        # é¢„æµ‹æ®‹å·®
        if return_uncertainty and self.config.kriging_enable_uncertainty:
            residual_pred, residual_std = self.kriging_adapter.predict(
                prediction_points, return_std=True
            )
            return residual_pred, residual_std
        else:
            residual_pred = self.kriging_adapter.predict(prediction_points, return_std=False)
            if return_uncertainty:
                # å¦‚æœè¯·æ±‚ä¸ç¡®å®šåº¦ä½†ä¸å¯ç”¨ï¼Œè¿”å›é›¶ä¸ç¡®å®šåº¦
                return residual_pred, np.zeros_like(residual_pred)
            else:
                return residual_pred

class Mode1Fusion:
    """
    æ–¹æ¡ˆ1: åŠ æƒèåˆå·¥å…·
    Mode 1: Weighted fusion tools
    """
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.kriging_adapter = KrigingAdapter(config)
    
    @staticmethod
    def fuse_residual(pinn_pred: np.ndarray,
                     kriging_residual: np.ndarray, 
                     weight: float = 0.5,
                     uncertainty: Optional[np.ndarray] = None) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        åŠ æƒèåˆPINNé¢„æµ‹å’Œæ®‹å·®é¢„æµ‹
        Weighted fusion of PINN predictions and residual predictions
        
        Args:
            pinn_pred: PINNé¢„æµ‹å€¼ (N,)
            kriging_residual: Krigingæ®‹å·®é¢„æµ‹å€¼ (N,)  
            weight: æ®‹å·®æƒé‡ Ï‰ âˆˆ (0,1), æœ€ç»ˆé¢„æµ‹ = PINN + Ï‰Ã—æ®‹å·®
            uncertainty: Krigingæ®‹å·®çš„ä¸ç¡®å®šåº¦ (N,) [å¯é€‰]
            
        Returns:
            fused_prediction: èåˆé¢„æµ‹ (N,)
            å¦‚æœæä¾›uncertainty: (fused_prediction, confidence_bounds)
        """
        if len(pinn_pred) != len(kriging_residual):
            raise ValueError("PINNé¢„æµ‹å’Œæ®‹å·®é¢„æµ‹çš„é•¿åº¦ä¸åŒ¹é…")
            
        if not 0 < weight < 1:
            warnings.warn(f"æƒé‡ {weight} ä¸åœ¨æ¨èèŒƒå›´ (0,1) å†…")
        
        # åŠ æƒèåˆ
        fused_pred = pinn_pred + weight * kriging_residual
        
        if uncertainty is not None:
            # è®¡ç®—ç½®ä¿¡ç•Œ (å‡è®¾PINNæ— ä¸ç¡®å®šåº¦ï¼Œåªè€ƒè™‘Krigingæ®‹å·®çš„ä¸ç¡®å®šåº¦)
            # 95%ç½®ä¿¡ç•Œ â‰ˆ Â±1.96Ïƒ  
            confidence_bounds = weight * 1.96 * uncertainty
            return fused_pred, confidence_bounds
        else:
            return fused_pred
    
    @staticmethod
    def adaptive_weight_strategy(residuals: np.ndarray,
                               kriging_std: Optional[np.ndarray] = None,
                               strategy: str = 'variance_based') -> np.ndarray:
        """
        è‡ªé€‚åº”æƒé‡ç­–ç•¥
        Adaptive weighting strategy
        
        Args:
            residuals: æ®‹å·®å€¼ (N,)
            kriging_std: Krigingæ ‡å‡†å·® (N,) [å¯é€‰]
            strategy: æƒé‡ç­–ç•¥ ('variance_based', 'magnitude_based', 'uniform')
            
        Returns:
            weights: è‡ªé€‚åº”æƒé‡ (N,)
        """
        n_points = len(residuals)
        
        if strategy == 'uniform':
            return np.full(n_points, 0.5)
        
        elif strategy == 'magnitude_based':
            # åŸºäºæ®‹å·®å¹…åº¦ï¼šæ®‹å·®è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜
            abs_residuals = np.abs(residuals)
            max_residual = np.max(abs_residuals) 
            weights = 0.1 + 0.8 * (abs_residuals / (max_residual + EPSILON))
            return np.clip(weights, 0.1, 0.9)
        
        elif strategy == 'variance_based' and kriging_std is not None:
            # åŸºäºKrigingä¸ç¡®å®šåº¦ï¼šä¸ç¡®å®šåº¦è¶Šå°ï¼Œæƒé‡è¶Šé«˜
            normalized_std = kriging_std / (np.max(kriging_std) + EPSILON)
            weights = 0.1 + 0.8 * (1 - normalized_std)  # åæ¯”å…³ç³»
            return np.clip(weights, 0.1, 0.9)
        
        else:
            warnings.warn(f"ä¸æ”¯æŒçš„æƒé‡ç­–ç•¥ '{strategy}' æˆ–ç¼ºå°‘å¿…è¦æ•°æ®ï¼Œä½¿ç”¨å‡åŒ€æƒé‡")
            return np.full(n_points, 0.5)

# ==================== æ–¹æ¡ˆ2ä¸“ç”¨åŠŸèƒ½ (Mode 2 Specific) ====================  
# Krigingåœ¨ROIç”Ÿæˆæ–°æ ·æœ¬ â†’ æ‰©å……æ•°æ® â†’ é‡æ–°è®­ç»ƒPINN

class Mode2ROIDetector:
    """
    æ–¹æ¡ˆ2: æ„Ÿå…´è¶£åŒºåŸŸ(ROI)æ£€æµ‹å™¨
    Mode 2: Region of Interest (ROI) detector
    """
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.kriging_adapter = KrigingAdapter(config)
    
    @staticmethod
    def detect_roi(train_points: np.ndarray,
                  train_values: np.ndarray,
                  roi_strategy: str = 'high_density',
                  **strategy_params) -> Dict[str, np.ndarray]:
        """
        æ£€æµ‹ç›¸å…³åŒºåŸŸ (Region of Interest)
        Detect region of interest for sample augmentation
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: è®­ç»ƒç‚¹æ•°å€¼ (N,)
            roi_strategy: ROIæ£€æµ‹ç­–ç•¥
            **strategy_params: ç­–ç•¥ç›¸å…³å‚æ•°
            
        Returns:
            roi_bounds: ROIè¾¹ç•Œä¿¡æ¯ {'min': [x,y,z], 'max': [x,y,z], 'mask': bool_array}
        """
        if roi_strategy == 'high_density':
            return Mode2ROIDetector._detect_high_density_roi(
                train_points, train_values, **strategy_params
            )
        elif roi_strategy == 'high_value':
            return Mode2ROIDetector._detect_high_value_roi(
                train_points, train_values, **strategy_params
            )
        elif roi_strategy == 'bounding_box':
            return Mode2ROIDetector._detect_bounding_box_roi(
                train_points, train_values, **strategy_params
            )
        elif roi_strategy == 'gradient_aware':
            return Mode2ROIDetector._detect_gradient_aware_roi(
                train_points, train_values, **strategy_params
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ROIç­–ç•¥: {roi_strategy}")
    
    @staticmethod
    def _detect_high_density_roi(train_points: np.ndarray,
                               train_values: np.ndarray,
                               density_percentile: float = 75,
                               expansion_factor: float = 1.2) -> Dict[str, np.ndarray]:
        """é«˜å¯†åº¦åŒºåŸŸæ£€æµ‹ç­–ç•¥"""
        from scipy.spatial import cKDTree
        
        # è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦
        tree = cKDTree(train_points)
        # è®¡ç®—åˆ°ç¬¬5è¿‘é‚»çš„è·ç¦»ä½œä¸ºå¯†åº¦çš„é€†æŒ‡æ ‡
        k = min(5, len(train_points) - 1)
        distances, _ = tree.query(train_points, k=k+1)  # k+1å› ä¸ºåŒ…å«è‡ªèº«
        local_density = 1 / (np.mean(distances[:, 1:], axis=1) + EPSILON)  # æ’é™¤è‡ªèº«
        
        # é€‰æ‹©é«˜å¯†åº¦ç‚¹
        density_threshold = np.percentile(local_density, density_percentile)
        high_density_mask = local_density >= density_threshold
        
        if not np.any(high_density_mask):
            # å¦‚æœæ²¡æœ‰é«˜å¯†åº¦ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰ç‚¹
            high_density_mask = np.ones(len(train_points), dtype=bool)
        
        roi_points = train_points[high_density_mask]
        
        # è®¡ç®—ROIè¾¹ç•Œ
        roi_min = np.min(roi_points, axis=0)
        roi_max = np.max(roi_points, axis=0)
        
        # æ‰©å±•è¾¹ç•Œ
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        return {
            'min': roi_min,
            'max': roi_max, 
            'mask': high_density_mask,
            'density_scores': local_density
        }
    
    @staticmethod
    def _detect_high_value_roi(train_points: np.ndarray,
                             train_values: np.ndarray,
                             value_percentile: float = 80,
                             expansion_factor: float = 1.5) -> Dict[str, np.ndarray]:
        """é«˜æ•°å€¼åŒºåŸŸæ£€æµ‹ç­–ç•¥"""
        # é€‰æ‹©é«˜æ•°å€¼ç‚¹
        value_threshold = np.percentile(train_values, value_percentile)
        high_value_mask = train_values >= value_threshold
        
        if not np.any(high_value_mask):
            # å¦‚æœæ²¡æœ‰é«˜æ•°å€¼ç‚¹ï¼Œä½¿ç”¨æ•°å€¼å¤§äº0çš„ç‚¹
            high_value_mask = train_values > 0
            
        if not np.any(high_value_mask):
            # å¦‚æœä»ç„¶æ²¡æœ‰ï¼Œä½¿ç”¨æ‰€æœ‰ç‚¹
            high_value_mask = np.ones(len(train_points), dtype=bool)
        
        roi_points = train_points[high_value_mask]
        
        # è®¡ç®—ROIè¾¹ç•Œå¹¶æ‰©å±•
        roi_min = np.min(roi_points, axis=0)
        roi_max = np.max(roi_points, axis=0)
        
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        return {
            'min': roi_min,
            'max': roi_max,
            'mask': high_value_mask,
            'value_scores': train_values
        }
    
    @staticmethod
    def _detect_bounding_box_roi(train_points: np.ndarray,
                               train_values: np.ndarray,
                               expansion_factor: float = 1.1) -> Dict[str, np.ndarray]:
        """åŒ…å›´ç›’ROIæ£€æµ‹ç­–ç•¥"""
        # ä½¿ç”¨æ‰€æœ‰è®­ç»ƒç‚¹çš„åŒ…å›´ç›’
        roi_min = np.min(train_points, axis=0)
        roi_max = np.max(train_points, axis=0)
        
        # è½»å¾®æ‰©å±•
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        # æ‰€æœ‰ç‚¹éƒ½åœ¨ROIå†…
        all_points_mask = np.ones(len(train_points), dtype=bool)
        
        return {
            'min': roi_min,
            'max': roi_max,
            'mask': all_points_mask,
            'bounding_box': True
        }
        
    @staticmethod
    def _detect_gradient_aware_roi(train_points: np.ndarray,
                                  train_values: np.ndarray,
                                  pinn_predictions: Optional[np.ndarray] = None,
                                  gradient_percentile: float = 90,
                                  expansion_factor: float = 1.2) -> Dict[str, np.ndarray]:
        """
        åŸºäºæ¢¯åº¦çš„ROIæ£€æµ‹ç­–ç•¥
        é€šè¿‡è®¡ç®—åœºå€¼çš„ç©ºé—´æ¢¯åº¦æ¥æ£€æµ‹é«˜æ¢¯åº¦åŒºåŸŸä½œä¸ºROI
        
        Args:
            train_points: è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: è®­ç»ƒç‚¹çœŸå®å€¼ (N,)
            pinn_predictions: PINNåœ¨è®­ç»ƒç‚¹çš„é¢„æµ‹å€¼ï¼ˆå¯é€‰ï¼‰(N,)
            gradient_percentile: æ¢¯åº¦é˜ˆå€¼çš„ç™¾åˆ†ä½æ•°
            expansion_factor: ROIæ‰©å±•ç³»æ•°
            
        Returns:
            roi_bounds: ROIè¾¹ç•Œä¿¡æ¯
        """
        from scipy.spatial import cKDTree
        
        # ä½¿ç”¨PINNé¢„æµ‹å’ŒçœŸå®å€¼çš„å·®å¼‚åœºï¼Œå¦‚æœæä¾›çš„è¯
        values_to_analyze = train_values
        if pinn_predictions is not None:
            # ä½¿ç”¨è¯¯å·®åœºçš„ç»å¯¹å€¼
            values_to_analyze = np.abs(train_values - pinn_predictions)
            
        # æ„å»ºKDæ ‘ä»¥å¯»æ‰¾è¿‘é‚»ç‚¹
        tree = cKDTree(train_points)
        
        # è®¡ç®—æ•°å€¼æ¢¯åº¦
        k = min(15, len(train_points) - 1)  # ç”¨äºæ¢¯åº¦è®¡ç®—çš„è¿‘é‚»æ•°
        gradients = []
        
        for i in range(len(train_points)):
            # æ‰¾åˆ°è¿‘é‚»ç‚¹
            distances, indices = tree.query(train_points[i], k=k+1)
            neighbors = train_points[indices[1:]]  # æ’é™¤è‡ªèº«
            neighbor_values = values_to_analyze[indices[1:]]
            
            if len(neighbors) < 3:  # æ¢¯åº¦è®¡ç®—éœ€è¦è‡³å°‘3ä¸ªç‚¹
                gradients.append(0)
                continue
                
            # ä½¿ç”¨å‘é‡å‡å»ä¸­å¿ƒç‚¹ï¼Œå½¢æˆä¸€ä¸ªå±€éƒ¨åæ ‡ç³»
            local_coords = neighbors - train_points[i]
            
            # ç®€å•è®¡ç®—æ¢¯åº¦: æ•°å€¼å˜åŒ– / è·ç¦»å˜åŒ–
            value_diffs = neighbor_values - values_to_analyze[i]
            
            # è®¡ç®—æ¯ä¸ªæ–¹å‘çš„å•ä½å‘é‡å’Œå¯¹åº”æ¢¯åº¦
            gradient_magnitudes = []
            for j, coord in enumerate(local_coords):
                dist = np.linalg.norm(coord)
                if dist > EPSILON:
                    # æ²¿è¯¥æ–¹å‘çš„æ¢¯åº¦å¤§å°
                    grad_magnitude = abs(value_diffs[j] / dist)
                    gradient_magnitudes.append(grad_magnitude)
            
            # å–å¹³å‡æ¢¯åº¦å¤§å°
            if gradient_magnitudes:
                gradients.append(np.mean(gradient_magnitudes))
            else:
                gradients.append(0)
        
        gradients = np.array(gradients)
        
        # è®¡ç®—å±€éƒ¨æ›²ç‡ï¼ˆè¿‘ä¼¼Hessianï¼‰
        curvature = np.zeros_like(gradients)
        for i in range(len(train_points)):
            distances, indices = tree.query(train_points[i], k=min(8, k+1)) 
            neighbor_gradients = gradients[indices[1:]]  # æ’é™¤è‡ªèº«
            if len(neighbor_gradients) > 0:
                # æ¢¯åº¦å˜åŒ–ä½œä¸ºæ›²ç‡åº¦é‡
                curvature[i] = np.std(neighbor_gradients)
                
        # ç»„åˆæ¢¯åº¦å’Œæ›²ç‡ä¿¡æ¯
        importance_score = gradients + 0.5 * curvature
        
        # é€‰æ‹©é«˜é‡è¦æ€§åŒºåŸŸ
        threshold = np.percentile(importance_score, gradient_percentile)
        high_gradient_mask = importance_score >= threshold
        
        if not np.any(high_gradient_mask):
            # å¦‚æœæ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰ç‚¹
            high_gradient_mask = np.ones(len(train_points), dtype=bool)
            print("   âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°é«˜æ¢¯åº¦åŒºåŸŸï¼Œä½¿ç”¨æ‰€æœ‰ç‚¹ä½œä¸ºROI")
        
        roi_points = train_points[high_gradient_mask]
        
        # è®¡ç®—ROIè¾¹ç•Œ
        roi_min = np.min(roi_points, axis=0)
        roi_max = np.max(roi_points, axis=0)
        
        # æ‰©å±•è¾¹ç•Œ
        roi_center = (roi_min + roi_max) / 2
        roi_size = (roi_max - roi_min) * expansion_factor
        roi_min = roi_center - roi_size / 2
        roi_max = roi_center + roi_size / 2
        
        return {
            'min': roi_min,
            'max': roi_max,
            'mask': high_gradient_mask,
            'importance_scores': importance_score,
            'gradients': gradients,
            'curvature': curvature
        }

class Mode2SampleAugmentor:
    """
    æ–¹æ¡ˆ2: æ ·æœ¬æ‰©å……å™¨  
    Mode 2: Sample augmentor using Kriging
    """
    
    def __init__(self, config: ComposeConfig = None):
        self.config = config or ComposeConfig()
        self.kriging_adapter = KrigingAdapter(config)
        
    def augment_by_kriging(self,
                          train_points: np.ndarray,
                          train_values: np.ndarray,
                          roi_bounds: Dict[str, np.ndarray],
                          augment_factor: float = 2.0,
                          sampling_strategy: str = 'adaptive',  # é»˜è®¤æ”¹ä¸ºadaptive
                          **kriging_params) -> Tuple[np.ndarray, np.ndarray]:
        """
        åœ¨ROIå†…ç”¨Krigingç”Ÿæˆæ–°æ ·æœ¬
        Generate new samples in ROI using Kriging
        
        Args:
            train_points: åŸå§‹è®­ç»ƒç‚¹åæ ‡ (N, 3)
            train_values: åŸå§‹è®­ç»ƒå€¼ (N,)
            roi_bounds: ROIè¾¹ç•Œä¿¡æ¯
            augment_factor: æ‰©å……å€æ•° (æ–°æ ·æœ¬æ•° = åŸæ ·æœ¬æ•° Ã— (augment_factor - 1))
            sampling_strategy: é‡‡æ ·ç­–ç•¥ ('grid', 'random', 'adaptive', 'sobol', 'lhs')
            **kriging_params: Krigingå‚æ•°
            
        Returns:
            augmented_points: æ‰©å……åçš„åæ ‡ (N+M, 3)
            augmented_values: æ‰©å……åçš„æ•°å€¼ (N+M,)
        """
        # è®­ç»ƒKrigingæ¨¡å‹
        self.kriging_adapter.fit(train_points, train_values, **kriging_params)
        
        # ç”ŸæˆROIå†…çš„æ–°é‡‡æ ·ç‚¹
        n_original = len(train_points) 
        n_new = int(n_original * (augment_factor - 1.0))
        
        if n_new <= 0:
            warnings.warn("æ‰©å……å€æ•°å¤ªå°ï¼Œæ²¡æœ‰ç”Ÿæˆæ–°æ ·æœ¬")
            return train_points, train_values
        
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆæ–°é‡‡æ ·ç‚¹
        new_points = self._generate_sampling_points(
            roi_bounds, n_new, sampling_strategy, train_points
        )
        
        # ä½¿ç”¨Krigingé¢„æµ‹æ–°ç‚¹çš„æ•°å€¼
        new_values = self.kriging_adapter.predict(new_points, return_std=False)
        
        # åˆå¹¶åŸå§‹å’Œæ–°ç”Ÿæˆçš„æ ·æœ¬
        augmented_points = np.vstack([train_points, new_points])
        augmented_values = np.concatenate([train_values, new_values])
        
        if self.config.verbose:
            print(f"æ ·æœ¬æ‰©å……å®Œæˆ: {n_original} â†’ {len(augmented_points)} ä¸ªæ ·æœ¬")
            print(f"æ–°æ ·æœ¬æ•°å€¼èŒƒå›´: [{np.min(new_values):.4e}, {np.max(new_values):.4e}]")
            print(f"åŸå§‹æ ·æœ¬æ•°å€¼èŒƒå›´: [{np.min(train_values):.4e}, {np.max(train_values):.4e}]")
        return augmented_points, augmented_values
    
    def _generate_sampling_points(self,
                                roi_bounds: Dict[str, np.ndarray], 
                                n_points: int,
                                strategy: str,
                                existing_points: np.ndarray) -> np.ndarray:
        """åœ¨ROIå†…ç”Ÿæˆé‡‡æ ·ç‚¹"""
        roi_min = roi_bounds['min']
        roi_max = roi_bounds['max']
        
        if strategy == 'grid':
            return self._generate_grid_points(roi_min, roi_max, n_points)
        elif strategy == 'random':
            return self._generate_random_points(roi_min, roi_max, n_points)
        elif strategy == 'adaptive':
            return self._generate_adaptive_points(roi_min, roi_max, n_points, existing_points)
        elif strategy == 'sobol':
            return self._generate_sobol_points(roi_min, roi_max, n_points)
        elif strategy == 'lhs':
            return self._generate_lhs_points(roi_min, roi_max, n_points)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·ç­–ç•¥: {strategy}")
    
    def _generate_grid_points(self, roi_min: np.ndarray, roi_max: np.ndarray, n_points: int) -> np.ndarray:
        """ç”Ÿæˆè§„åˆ™ç½‘æ ¼ç‚¹"""
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ç‚¹æ•°ï¼ˆå°½é‡æ¥è¿‘ç«‹æ–¹ä½“ï¼‰
        points_per_dim = int(np.ceil(n_points ** (1/3)))
        
        x = np.linspace(roi_min[0], roi_max[0], points_per_dim)
        y = np.linspace(roi_min[1], roi_max[1], points_per_dim)
        z = np.linspace(roi_min[2], roi_max[2], points_per_dim)
        
        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
        grid_points = np.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)
        
        # å¦‚æœç”Ÿæˆçš„ç‚¹æ•°è¶…è¿‡éœ€è¦çš„æ•°é‡ï¼Œéšæœºé€‰æ‹©
        if len(grid_points) > n_points:
            indices = np.random.choice(len(grid_points), n_points, replace=False)
            grid_points = grid_points[indices]
        
        return grid_points
    
    def _generate_random_points(self, roi_min: np.ndarray, roi_max: np.ndarray, n_points: int) -> np.ndarray:
        """ç”Ÿæˆéšæœºé‡‡æ ·ç‚¹"""
        random_points = np.random.rand(n_points, 3)
        random_points = roi_min + random_points * (roi_max - roi_min)
        return random_points
    
    def _generate_adaptive_points(self, roi_min: np.ndarray, roi_max: np.ndarray, 
                                n_points: int, existing_points: np.ndarray) -> np.ndarray:
        """
        ç”Ÿæˆè‡ªé€‚åº”é‡‡æ ·ç‚¹ï¼ˆä½¿ç”¨Max-minè·ç¦»ç­–ç•¥ï¼‰
        ç»“åˆKD-Treeå’Œæœ€å¤§æœ€å°è·ç¦»ï¼Œç¡®ä¿æ–°ç‚¹è¿œç¦»å·²æœ‰ç‚¹ï¼ŒåŒæ—¶è¦†ç›–ç©ºé—´
        """
        from scipy.spatial import cKDTree
        
        # æ„å»ºå·²æœ‰ç‚¹çš„KDæ ‘
        tree = cKDTree(existing_points)
        
        # ä½¿ç”¨Max-minç­–ç•¥é€‰æ‹©ç‚¹
        selected_points = []
        
        # ç¬¬ä¸€ä¸ªç‚¹éšæœºé€‰æ‹©
        candidate = roi_min + np.random.rand(3) * (roi_max - roi_min)
        selected_points.append(candidate)
        
        # ç”Ÿæˆå€™é€‰ç‚¹æ± ï¼ˆæ¯”æ‰€éœ€ç‚¹æ•°å¤šï¼Œæé«˜æ•ˆç‡ï¼‰
        pool_size = min(n_points * 10, 10000)  # é¿å…è¿‡å¤§çš„å€™é€‰æ± 
        candidate_pool = roi_min + np.random.rand(pool_size, 3) * (roi_max - roi_min)
        
        # é€ç‚¹æ·»åŠ å‰©ä½™ç‚¹
        pbar = range(n_points - 1)
        if self.config.verbose and n_points > 100:
            try:
                from tqdm import tqdm
                pbar = tqdm(pbar, desc="ç”Ÿæˆè‡ªé€‚åº”é‡‡æ ·ç‚¹")
            except ImportError:
                pass
                
        for _ in pbar:
            if len(selected_points) >= n_points:
                break
                
            # æ›´æ–°KDæ ‘ï¼ŒåŒ…å«å·²æœ‰ç‚¹å’Œå·²é€‰ç‚¹
            current_points = np.vstack([existing_points, selected_points])
            current_tree = cKDTree(current_points)
            
            # è®¡ç®—å€™é€‰æ± ä¸­æ¯ä¸ªç‚¹åˆ°å½“å‰å·²æœ‰ç‚¹çš„æœ€å°è·ç¦»
            distances, _ = current_tree.query(candidate_pool, k=1)
            
            # é€‰æ‹©æœ€å°è·ç¦»æœ€å¤§çš„å€™é€‰ç‚¹ï¼ˆæœ€è¿œç‚¹ï¼‰
            best_idx = np.argmax(distances)
            best_point = candidate_pool[best_idx]
            
            # æ·»åŠ åˆ°å·²é€‰ç‚¹é›†
            selected_points.append(best_point)
            
            # ä»å€™é€‰æ± ä¸­ç§»é™¤å·²é€‰ç‚¹ï¼Œå¹¶è¡¥å……æ–°çš„éšæœºç‚¹
            candidate_pool[best_idx] = roi_min + np.random.rand(3) * (roi_max - roi_min)
        
        return np.array(selected_points)
    
    def _generate_sobol_points(self, roi_min: np.ndarray, roi_max: np.ndarray, n_points: int) -> np.ndarray:
        """
        ä½¿ç”¨Sobolåºåˆ—ç”Ÿæˆä½å·®å¼‚ç‚¹é›†
        Sobolåºåˆ—æ˜¯ä¸€ç§å‡†éšæœºåºåˆ—ï¼Œå…·æœ‰æ›´å‡åŒ€çš„ç©ºé—´è¦†ç›–æ€§
        """
        try:
            from scipy.stats import qmc
            
            # åˆ›å»ºSobolç”Ÿæˆå™¨
            sampler = qmc.Sobol(d=3, scramble=True)
            
            # ç”Ÿæˆ[0,1)^3ç©ºé—´çš„ç‚¹
            sample = sampler.random(n=n_points)
            
            # ç¼©æ”¾åˆ°ROIèŒƒå›´
            points = qmc.scale(sample, roi_min, roi_max)
            
            return points
            
        except (ImportError, AttributeError):
            # å¦‚æœscipyç‰ˆæœ¬ä¸æ”¯æŒqmcï¼Œå›é€€åˆ°éšæœºé‡‡æ ·
            print("   âš ï¸ SciPy qmcæ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°éšæœºé‡‡æ ·")
            return self._generate_random_points(roi_min, roi_max, n_points)
    
    def _generate_lhs_points(self, roi_min: np.ndarray, roi_max: np.ndarray, n_points: int) -> np.ndarray:
        """
        ä½¿ç”¨Latin Hypercubeé‡‡æ ·ç”Ÿæˆç‚¹
        Latin Hypercubeé‡‡æ ·å¯ä»¥ç¡®ä¿åœ¨æ¯ä¸ªç»´åº¦ä¸Šçš„æŠ•å½±åˆ†å¸ƒå‡åŒ€
        """
        try:
            from scipy.stats import qmc
            
            # åˆ›å»ºLatin Hypercubeé‡‡æ ·å™¨
            sampler = qmc.LatinHypercube(d=3)
            
            # ç”Ÿæˆ[0,1)^3ç©ºé—´çš„ç‚¹
            sample = sampler.random(n=n_points)
            
            # ç¼©æ”¾åˆ°ROIèŒƒå›´
            points = qmc.scale(sample, roi_min, roi_max)
            
            return points
            
        except (ImportError, AttributeError):
            # å¦‚æœscipyç‰ˆæœ¬ä¸æ”¯æŒqmcï¼Œå›é€€åˆ°éšæœºé‡‡æ ·
            print("   âš ï¸ SciPy qmcæ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°éšæœºé‡‡æ ·")
            return self._generate_random_points(roi_min, roi_max, n_points)

# ==================== ç«¯åˆ°ç«¯è€¦åˆå·¥ä½œæµ ====================
# End-to-end coupling workflows

class CouplingWorkflow:
    """
    è€¦åˆå·¥ä½œæµä¸»ç¼–æ’å™¨
    Main orchestrator for coupling workflows
    """
    def __init__(self, physical_params: Dict, config: ComposeConfig = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµ
        
        Args:
            physical_params: ç‰©ç†å‚æ•°å­—å…¸ (å¦‚rho, mu)
            config: å…¨å±€é…ç½®å¯¹è±¡
        """
        self.physical_params = physical_params
        self.config = config or ComposeConfig()
        
        if self.config.verbose:
            print("="*20 + " è€¦åˆå·¥ä½œæµåˆå§‹åŒ– " + "="*20)
            print(f"  - ä½¿ç”¨é…ç½®: {self.config}")

        # ä¸ºæ¯ä¸ªæ¨¡å¼åˆå§‹åŒ–ä¸“ç”¨çš„å·¥å…·é›†
        self.mode1_tools = {
            'residual_kriging': Mode1ResidualKriging(config=self.config),
            'fusion': Mode1Fusion(config=self.config)
        }
        
        self.mode2_tools = {
            'roi_detector': Mode2ROIDetector(config=self.config),
            'sample_augmentor': Mode2SampleAugmentor(config=self.config)
        }

        if self.config.verbose:
            print("="*20 + " åˆå§‹åŒ–å®Œæˆ " + "="*20 + "\n")

    def run_mode1_pipeline(self,
                          train_points: np.ndarray,
                          train_values: np.ndarray,
                          prediction_points: np.ndarray,
                          fusion_weight: Optional[float] = None,
                          dose_data: Optional[Dict] = None,
                          **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–¹æ¡ˆ1å®Œæ•´æµç¨‹: PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ
        Execute Mode 1 complete pipeline
        """
        if fusion_weight is None:
            fusion_weight = self.config.fusion_weight
            
        results = {}
        
        # æ­¥éª¤1: è®­ç»ƒPINN
        print("ğŸ”¥ æ­¥éª¤1: è®­ç»ƒPINNæ¨¡å‹...")
        self.pinn_adapter_mode1.fit_from_memory(
            train_points=train_points, 
            train_values=train_values,
            dose_data=dose_data,
            epochs=kwargs.get('epochs'),
            loss_weights=kwargs.get('loss_weights'),
            use_lbfgs=kwargs.get('use_lbfgs')
        )
        
        # æ­¥éª¤2: PINNé¢„æµ‹
        print("ğŸ”® æ­¥éª¤2: PINNå…¨åœºé¢„æµ‹...")
        pinn_train_pred = self.pinn_adapter_mode1.predict(train_points)
        pinn_field_pred = self.pinn_adapter_mode1.predict(prediction_points)
        
        # ==================== æ–°å¢ï¼šè¯¦ç»†PINNè¯¯å·®ç»Ÿè®¡ ====================
        print("\nğŸ“Š æ­¥éª¤2.1: PINNè¯¯å·®åˆ†æ...")
        
        # è®­ç»ƒç‚¹è¯¯å·®ç»Ÿè®¡
        train_errors = train_values - pinn_train_pred
        train_metrics = {
            'è®­ç»ƒé›†MAE': np.mean(np.abs(train_errors)),
            'è®­ç»ƒé›†RMSE': np.sqrt(np.mean(train_errors**2)),
            'è®­ç»ƒé›†MRE': np.mean(np.abs(train_errors) / (np.abs(train_values) + EPSILON)),
            'è®­ç»ƒé›†æœ€å¤§è¯¯å·®': np.max(np.abs(train_errors)),
            'è®­ç»ƒé›†RÂ²': 1 - np.sum(train_errors**2) / np.sum((train_values - np.mean(train_values))**2)
        }
        
        print("   ğŸ¯ PINNè®­ç»ƒé›†æ€§èƒ½:")
        for metric, value in train_metrics.items():
            print(f"      {metric}: {value:.4f}")
        
        # é¢„æµ‹å€¼ç»Ÿè®¡ä¿¡æ¯  
        print("   ğŸ” PINNé¢„æµ‹ç»Ÿè®¡: èŒƒå›´[{:.2e}, {:.2e}]".format(np.min(pinn_train_pred), np.max(pinn_train_pred)))
        print("   ğŸ“Š æœ‰æ•ˆé¢„æµ‹æ•°é‡: {}".format(len(pinn_train_pred)))
        
        # é¢„æµ‹ç‚¹é¢„æµ‹å€¼ç»Ÿè®¡
        print("   ğŸ” PINNé¢„æµ‹ç»Ÿè®¡: èŒƒå›´[{:.2e}, {:.2e}]".format(np.min(pinn_field_pred), np.max(pinn_field_pred)))
        print("   ğŸ“Š æœ‰æ•ˆé¢„æµ‹æ•°é‡: {}".format(len(pinn_field_pred)))
        
        # æ·»åŠ è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
        error_percentiles = [5, 25, 50, 75, 95]
        error_stats = np.percentile(np.abs(train_errors), error_percentiles)
        print("   ğŸ“ˆ è®­ç»ƒè¯¯å·®åˆ†å¸ƒ (ç»å¯¹å€¼):")
        for p, val in zip(error_percentiles, error_stats):
            print(f"      {p}%åˆ†ä½æ•°: {val:.4e}")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        error_threshold = np.mean(np.abs(train_errors)) + 3 * np.std(np.abs(train_errors))
        outlier_count = np.sum(np.abs(train_errors) > error_threshold)
        outlier_percentage = outlier_count / len(train_errors) * 100
        print(f"   âš ï¸ å¼‚å¸¸è¯¯å·®ç‚¹: {outlier_count}ä¸ª ({outlier_percentage:.1f}%)")
        
        # ç©ºé—´è¯¯å·®åˆ†æï¼ˆå¦‚æœè®­ç»ƒç‚¹è¾ƒå¤šï¼‰
        if len(train_points) >= 10:
            # è®¡ç®—è¯¯å·®çš„ç©ºé—´ç›¸å…³æ€§
            spatial_distances = np.linalg.norm(train_points[:, None] - train_points[None, :], axis=2)
            error_correlations = []
            
            # é€‰æ‹©å‡ ä¸ªè·ç¦»èŒƒå›´æ¥åˆ†æè¯¯å·®ç›¸å…³æ€§
            distance_ranges = [0.5, 1.0, 2.0, 5.0]
            for dist_range in distance_ranges:
                close_pairs = (spatial_distances > 0) & (spatial_distances < dist_range)
                if np.sum(close_pairs) > 0:
                    error_pairs = train_errors[:, None] * train_errors[None, :]
                    mean_error_corr = np.mean(error_pairs[close_pairs])
                    error_correlations.append((dist_range, mean_error_corr))
            
            if error_correlations:
                print("   ğŸ—ºï¸ ç©ºé—´è¯¯å·®ç›¸å…³æ€§:")
                for dist, corr in error_correlations:
                    print(f"      è·ç¦»<{dist:.1f}m: ç›¸å…³æ€§={corr:.4e}")
        
        # å­˜å‚¨è¯¯å·®ç»Ÿè®¡ç»“æœ
        results['pinn_train_errors'] = train_errors
        results['pinn_train_metrics'] = train_metrics
        results['pinn_predictions'] = pinn_field_pred
        # ==================== PINNè¯¯å·®ç»Ÿè®¡ç»“æŸ ====================
        
        # æ­¥éª¤3: æ®‹å·®Kriging
        print("âš¡ æ­¥éª¤3: æ®‹å·®Krigingæ’å€¼...")
        print(f"   ğŸ” è®¡ç®—PINNè®­ç»ƒç‚¹é¢„æµ‹ä¸çœŸå®å€¼çš„æ®‹å·®...")
        print(f"   ğŸŒ å¯¹æ®‹å·®è¿›è¡ŒKrigingç©ºé—´æ’å€¼...")
        print(f"   ğŸ“Š è®­ç»ƒç‚¹æ•°é‡: {len(train_points)}")
        print(f"   ğŸ“ é¢„æµ‹ç‚¹æ•°é‡: {len(prediction_points)}")
        
        residual_pred, residual_std = self.mode1_tools['residual_kriging'].residual_kriging(
            train_points, train_values, pinn_train_pred, prediction_points,
            return_uncertainty=True, **kwargs.get('kriging_params', {})
        )
        
        print(f"   âœ… æ®‹å·®Krigingæ’å€¼å®Œæˆ")
        print(f"   ğŸ“ˆ æ®‹å·®é¢„æµ‹èŒƒå›´: [{np.min(residual_pred):.4e}, {np.max(residual_pred):.4e}]")
        if residual_std is not None:
            print(f"   ğŸ“Š æ®‹å·®ä¸ç¡®å®šåº¦èŒƒå›´: [{np.min(residual_std):.4e}, {np.max(residual_std):.4e}]")
        results['residual_predictions'] = residual_pred
        results['residual_std'] = residual_std
        
        # æ­¥éª¤4: åŠ æƒèåˆ
        print("ğŸ”— æ­¥éª¤4: åŠ æƒèåˆ...")
        if residual_std is not None and not np.all(residual_std == 0):
            fused_pred, confidence_bounds = self.mode1_tools['fusion'].fuse_residual(
                pinn_field_pred, residual_pred, fusion_weight, residual_std
            )
            results['confidence_bounds'] = confidence_bounds
        else:
            fused_pred = self.mode1_tools['fusion'].fuse_residual(
                pinn_field_pred, residual_pred, fusion_weight
            )
            results['confidence_bounds'] = None
            
        results['final_predictions'] = fused_pred
        results['fusion_weight'] = fusion_weight
        
        print("âœ… æ–¹æ¡ˆ1æµç¨‹å®Œæˆ!")
        return results
    
    def run_mode2_pipeline(self,
                          train_points: np.ndarray, 
                          train_values: np.ndarray,
                          prediction_points: np.ndarray,
                          roi_strategy: Optional[str] = None,
                          augment_factor: Optional[float] = None,
                          dose_data: Optional[Dict] = None,
                          sampling_strategy: str = 'adaptive',
                          sample_balancing: bool = True,  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦è¿›è¡Œæ ·æœ¬å¹³è¡¡
                          **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–¹æ¡ˆ2çš„å·¥ä½œæµ: Kriging ROIæ ·æœ¬æ‰©å…… -> PINNé‡è®­ç»ƒ
        Executes Mode 2 workflow: Kriging ROI sample augmentation -> PINN re-training
        """
        start_time = time.time()
        results = {'timing': {}}

        # è·å–é…ç½®å‚æ•°
        roi_strategy = roi_strategy or self.config.roi_detection_strategy
        augment_factor = augment_factor or self.config.sample_augment_factor
        
        print("\n" + "-"*20 + " æ–¹æ¡ˆ2: Krigingæ•°æ®å¢å¼º -> PINNé‡è®­ç»ƒ " + "-"*20)

        # ==================== æ­¥éª¤1: åˆå§‹PINNè®­ç»ƒå’Œé¢„æµ‹ (ä½œä¸ºåŸºçº¿) ====================
        print("âš¡ æ­¥éª¤1: åˆå§‹PINNè®­ç»ƒ (ç”¨äºåŸºçº¿å¯¹æ¯”å’ŒROIæ£€æµ‹)...")
        # åˆ›å»ºPINNé€‚é…å™¨å®ä¾‹
        pinn_adapter_mode2 = PINNAdapter(physical_params=self.physical_params, config=self.config)
        
        pinn_adapter_mode2.fit_from_memory(train_points, train_values, dose_data, **kwargs)
        
        # ä½¿ç”¨åˆå§‹PINNè¿›è¡Œé¢„æµ‹ï¼Œä½œä¸ºæ€§èƒ½å¯¹æ¯”çš„åŸºçº¿
        initial_pinn_predictions = pinn_adapter_mode2.predict(prediction_points)
        # è·å–è®­ç»ƒç‚¹ä¸Šçš„é¢„æµ‹ï¼Œç”¨äºROIæ£€æµ‹
        train_pinn_predictions = pinn_adapter_mode2.predict(train_points)
        results['pinn_predictions'] = initial_pinn_predictions
        print(f"   âœ… åˆå§‹PINNè®­ç»ƒå’ŒåŸºçº¿é¢„æµ‹å®Œæˆã€‚")
        results['timing']['initial_pinn'] = time.time() - start_time

        # ==================== æ­¥éª¤2: ROIæ£€æµ‹ ====================
        current_time = time.time()
        print(f"âš¡ æ­¥éª¤2: æ„Ÿå…´è¶£åŒºåŸŸ(ROI)æ£€æµ‹ (ç­–ç•¥: {roi_strategy})...")
        roi_detector = self.mode2_tools['roi_detector']
        
        # å¦‚æœä½¿ç”¨æ¢¯åº¦æ„ŸçŸ¥çš„ROIæ£€æµ‹ï¼Œä¼ å…¥PINNé¢„æµ‹ç»“æœ
        roi_detection_params = {}
        if roi_strategy == 'gradient_aware':
            roi_detection_params['pinn_predictions'] = train_pinn_predictions
            print(f"   ğŸ” ä½¿ç”¨æ¢¯åº¦æ„ŸçŸ¥çš„ROIæ£€æµ‹ï¼Œåˆ†æPINNè¯¯å·®åœºæ¢¯åº¦...")
        
        roi_bounds = roi_detector.detect_roi(
            train_points, train_values, roi_strategy=roi_strategy, **roi_detection_params
        )
        print(f"   âœ… ROIæ£€æµ‹å®Œæˆã€‚")
        results['timing']['roi_detection'] = time.time() - current_time

        # ==================== æ­¥éª¤3: Krigingæ•°æ®å¢å¼º ====================
        current_time = time.time()
        print(f"âš¡ æ­¥éª¤3: Krigingæ•°æ®å¢å¼º (æ‰©å……å› å­: {augment_factor}, é‡‡æ ·ç­–ç•¥: {sampling_strategy})...")
        augmentor = self.mode2_tools['sample_augmentor']
        augmented_points, augmented_values = augmentor.augment_by_kriging(
            train_points, train_values, roi_bounds, 
            augment_factor=augment_factor,
            sampling_strategy=sampling_strategy
        )
        print(f"   âœ… æˆåŠŸç”Ÿæˆ {len(augmented_points) - len(train_points)} ä¸ªæ–°æ ·æœ¬ç‚¹ã€‚")
        print(f"   ğŸ“Š å¢å¼ºåæ€»è®­ç»ƒç‚¹æ•°: {len(augmented_points)}")
        results['timing']['augmentation'] = time.time() - current_time
        
        # ==================== æ­¥éª¤4: æ ·æœ¬å¹³è¡¡ (æ–°å¢) ====================
        current_time = time.time()
        if sample_balancing:
            print("âš¡ æ­¥éª¤4: æ ·æœ¬å¹³è¡¡å¤„ç†...")
            
            # è·å–åŸæ ·æœ¬å’Œæ–°æ ·æœ¬åœ¨ROIä¸­çš„æ¯”ä¾‹
            n_original = len(train_points)
            n_augmented = len(augmented_points) - n_original
            
            # è®¡ç®—æ ·æœ¬æƒé‡
            sample_weights = np.ones(len(augmented_points))
            
            # ç¡®å®šåŸå§‹æ ·æœ¬å’Œæ–°ç”Ÿæˆæ ·æœ¬çš„æƒé‡
            original_weight = 1.0
            augmented_weight = 0.5  # æ–°ç”Ÿæˆçš„æ ·æœ¬æƒé‡
            
            # åŸæ ·æœ¬ä½¿ç”¨æ ‡å‡†æƒé‡
            sample_weights[:n_original] = original_weight
            
            # æ–°ç”Ÿæˆçš„æ ·æœ¬ä½¿ç”¨è¾ƒä½çš„æƒé‡
            sample_weights[n_original:] = augmented_weight
            
            # å¯¹æƒé‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶å’Œä¸ºæ ·æœ¬æ€»æ•°
            sample_weights = sample_weights * len(sample_weights) / np.sum(sample_weights)
            
            print(f"   ğŸ“Š æ ·æœ¬æƒé‡ç»Ÿè®¡: åŸå§‹æ ·æœ¬={original_weight}, æ–°æ ·æœ¬={augmented_weight}")
            print(f"   âœ… æ ·æœ¬å¹³è¡¡å¤„ç†å®Œæˆã€‚")
            
            results['sample_weights'] = sample_weights
        else:
            print("âš¡ æ­¥éª¤4: è·³è¿‡æ ·æœ¬å¹³è¡¡ï¼Œä½¿ç”¨å‡åŒ€æƒé‡...")
            results['sample_weights'] = np.ones(len(augmented_points))
        
        results['timing']['sample_balancing'] = time.time() - current_time
        
        # ==================== æ­¥éª¤5: PINNæ¨¡å‹é‡è®­ç»ƒ ====================
        current_time = time.time()
        print("âš¡ æ­¥éª¤5: ä½¿ç”¨å¢å¼ºæ•°æ®è¿›è¡ŒPINNæ¨¡å‹é‡è®­ç»ƒ...")
        
        # å¦‚æœå¯ç”¨äº†æ ·æœ¬å¹³è¡¡ï¼Œæ·»åŠ æ ·æœ¬æƒé‡å‚æ•°
        if sample_balancing:
            # å…‹éš†ä¸€ä¸ªä¿®æ”¹åçš„kwargså­—å…¸
            train_kwargs = kwargs.copy()
            train_kwargs['sample_weights'] = results['sample_weights']
            print("   ğŸ”§ å°†æ ·æœ¬æƒé‡ä¼ é€’ç»™PINNè®­ç»ƒ...")
        else:
            train_kwargs = kwargs
        
        # ä½¿ç”¨é€‚é…å™¨è¿›è¡Œé‡è®­ç»ƒ
        pinn_adapter_mode2.fit_from_memory(
            train_points=augmented_points, 
            train_values=augmented_values, 
            dose_data=dose_data, 
            **train_kwargs
        )
        print(f"   âœ… PINNæ¨¡å‹é‡è®­ç»ƒå®Œæˆã€‚")
        results['timing']['pinn_retrain'] = time.time() - current_time

        # ==================== æ­¥éª¤6: æœ€ç»ˆé¢„æµ‹ ====================
        current_time = time.time()
        print("âš¡ æ­¥éª¤6: ä½¿ç”¨é‡è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæœ€ç»ˆé¢„æµ‹...")
        final_predictions = pinn_adapter_mode2.predict(prediction_points)
        results['final_predictions'] = final_predictions
        print(f"   âœ… æœ€ç»ˆé¢„æµ‹å®Œæˆã€‚")
        results['timing']['final_prediction'] = time.time() - current_time
        
        results['timing']['total'] = time.time() - start_time
        return results

def print_compose_banner():
    """æ‰“å°é¡¹ç›®æ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         GPU Block-Kriging Ã— PINN è€¦åˆé‡å»ºå·¥å…·æ¨¡å—            â•‘  
    â•‘        GPU-Accelerated Block Kriging Ã— PINN Coupling        â•‘
    â•‘                                                              â•‘
    â•‘  ğŸš€ æ–¹æ¡ˆ1: PINN â†’ æ®‹å·®Kriging â†’ åŠ æƒèåˆ                     â•‘
    â•‘  ğŸ¯ æ–¹æ¡ˆ2: Kriging ROIæ ·æœ¬æ‰©å…… â†’ PINNé‡è®­ç»ƒ                  â•‘  
    â•‘                                                              â•‘
    â•‘  ğŸ’¡ æ”¯æŒGPUåŠ é€Ÿ | ğŸ”¬ ç‰©ç†çº¦æŸ | ğŸ“Š ä¸ç¡®å®šåº¦é‡åŒ–              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

if __name__ == "__main__":
    print_compose_banner()
    validate_compose_environment() 