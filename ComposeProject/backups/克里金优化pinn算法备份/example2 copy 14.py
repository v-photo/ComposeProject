import numpy as np
import sys
from pathlib import Path
import deepxde as dde
import pandas as pd
import os
from sklearn.model_selection import train_test_split

# --- è§£å†³WSLç¯å¢ƒä¸‹çš„matplotlibæ˜¾ç¤ºé—®é¢˜ ---
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨æ— GUIçš„åç«¯ï¼Œé¿å…Qté”™è¯¯
import matplotlib.pyplot as plt

# --- è·¯å¾„è®¾ç½® ---
try:
    current_dir = Path(__file__).parent.resolve()
    project_root = current_dir.parent
except NameError:
    project_root = Path('.').resolve()

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'PINN'))
sys.path.insert(0, str(project_root / 'Kriging'))

# --- æ¨¡å—å¯¼å…¥ ---
try:
    from PINN.data_processing import DataLoader
    from PINN.dataAnalysis import get_data
    from myKriging import training as kriging_training, testing as kriging_testing
    print("âœ… å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥æˆåŠŸã€‚")
except ImportError as e:
    print(f"âŒ å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# =================================================================================
#  å…è´£å£°æ˜ï¼šä»¥ä¸‹ç±»å’Œå‡½æ•°å‡ä¸ºå ä½ç¬¦ (Placeholder)
#  æ‚¨éœ€è¦æ ¹æ®æ‚¨ç°æœ‰çš„ PINN å’Œ Kriging åº“æ¥å¡«å……å®ƒä»¬çš„å…·ä½“å®ç°ã€‚
#  è¿™é‡Œçš„éª¨æ¶æ˜¯ä¸ºäº†æ¸…æ™°åœ°å±•ç¤º"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"è¿™ä¸€æŠ€æœ¯è·¯çº¿ã€‚
# =================================================================================

# --- [å…¨å±€] å®éªŒæ§åˆ¶å¼€å…³ ---
ENABLE_KRIGING = True     # ğŸ”§ æ§åˆ¶æ˜¯å¦å¯ç”¨å…‹é‡Œé‡‘å¼•å¯¼çš„é‡é‡‡æ ·
ENABLE_DATA_INJECTION = False  # ğŸ”§ æ§åˆ¶æ˜¯å¦å¯ç”¨æ•°æ®æ³¨å…¥ç­–ç•¥
ENABLE_RAPID_IMPROVEMENT_EARLY_STOP = False  # ğŸ”§ æ§åˆ¶æ˜¯å¦å¯ç”¨å¿«é€Ÿæ”¹å–„æ—©åœ

# --- [å…¨å±€] æ¢ç´¢ç‡é…ç½® ---
# ğŸ“Š æ¢ç´¢ç‡é€’å‡ç­–ç•¥é…ç½®
# è®¡ç®—å…¬å¼: exploration_ratio = max(FINAL, INITIAL - (cycle-1) * DECAY_RATE)

# # ğŸ¯ å½“å‰é…ç½® (é€‚ä¸­ç­–ç•¥)
# INITIAL_EXPLORATION_RATIO = 0.20    # åˆå§‹æ¢ç´¢ç‡ (ç¬¬1å‘¨æœŸ): 20%
# FINAL_EXPLORATION_RATIO = 0.05      # æœ€ç»ˆæ¢ç´¢ç‡ (æ”¶æ•›å€¼): 5%
# EXPLORATION_DECAY_RATE = 0.02       # æ¯å‘¨æœŸé€’å‡ç‡: 2%
# ğŸ‘† è¯¥é…ç½®ä¸‹ï¼šç¬¬1å‘¨æœŸ20% â†’ ç¬¬8å‘¨æœŸ5% â†’ ä¹‹åä¿æŒ5%

# ğŸ’¡ å…¶ä»–å¸¸ç”¨é…ç½®ç¤ºä¾‹ (å–æ¶ˆæ³¨é‡Šä½¿ç”¨):
# 
# ğŸš€ æ¿€è¿›ç­–ç•¥ (å¿«é€Ÿä»æ¢ç´¢è½¬å‘åˆ©ç”¨)
# INITIAL_EXPLORATION_RATIO = 0.25    # 25%
# FINAL_EXPLORATION_RATIO = 0.02      # 2%
# EXPLORATION_DECAY_RATE = 0.05       # 5%
# # # æ•ˆæœï¼šç¬¬1å‘¨æœŸ25% â†’ ç¬¬5å‘¨æœŸ5% â†’ ç¬¬6å‘¨æœŸ2%
#
# ğŸŒ ä¿å®ˆç­–ç•¥ (é•¿æœŸä¿æŒæ¢ç´¢)
INITIAL_EXPLORATION_RATIO = 0.50    # 50%
FINAL_EXPLORATION_RATIO = 0.18      # 18%
EXPLORATION_DECAY_RATE = 0.3       # 4%
# # æ•ˆæœï¼šç¬¬1å‘¨æœŸ15% â†’ ç¬¬8å‘¨æœŸ8% â†’ ä¹‹åä¿æŒ8%
#
# # ğŸ¯ ç²¾å‡†ç­–ç•¥ (é«˜åˆ©ç”¨ç‡)
# INITIAL_EXPLORATION_RATIO = 0.30    # 30%
# FINAL_EXPLORATION_RATIO = 0.03      # 3%
# EXPLORATION_DECAY_RATE = 0.03       # 3%
# # æ•ˆæœï¼šç¬¬1å‘¨æœŸ30% â†’ ç¬¬10å‘¨æœŸ3% â†’ ä¹‹åä¿æŒ3%

# --- [æ–°å¢] æŸå¤±æƒé‡æ¯”å€¼é¢„è®¾é…ç½® ---
# ğŸ”§ LOSS_RATIO é¢„è®¾é€‰é¡¹ï¼Œæ–¹ä¾¿å¿«é€Ÿåˆ‡æ¢æµ‹è¯•ä¸åŒæ¯”å€¼å¯¹å…‹é‡Œé‡‘æ•ˆæœçš„å½±å“
# 
# ğŸ“Š å…‹é‡Œé‡‘ä¼˜åŒ–å‹å¥½çš„æ¨èæ¯”å€¼:
LOSS_RATIO_PHYSICS_DOMINANT_SUPER = 0.1   # ğŸ”¬ è¶…çº§ç‰©ç†ä¸»å¯¼: ç‰©ç†æƒé‡ > æ•°æ®æƒé‡ (10:1)
LOSS_RATIO_PHYSICS_DOMINANT = 0.5   # ğŸ”¬ ç‰©ç†ä¸»å¯¼: ç‰©ç†æƒé‡ > æ•°æ®æƒé‡ (2:1)
LOSS_RATIO_PHYSICS_STRONG = 0.8     # âš—ï¸  ç‰©ç†ä¼˜å…ˆ: ç‰©ç†çº¦æŸç¨å¼ºäºæ•°æ®æ‹Ÿåˆ (1.25:1)  
LOSS_RATIO_EQUAL_BALANCE = 1.0      # âš–ï¸  å®Œå…¨å¹³è¡¡: ç‰©ç†æƒé‡ = æ•°æ®æƒé‡ (1:1)
LOSS_RATIO_CONSERVATIVE = 3.0       # ğŸ›¡ï¸  ä¿å®ˆç­–ç•¥: å¼ºè°ƒç‰©ç†çº¦æŸï¼Œæ®‹å·®åˆ†å¸ƒå¹³æ»‘
LOSS_RATIO_BALANCED = 5.0           # âš–ï¸  å¹³è¡¡ç­–ç•¥: å…¼é¡¾æ•°æ®æ‹Ÿåˆå’Œç‰©ç†çº¦æŸ (æ¨è)
LOSS_RATIO_AGGRESSIVE = 8.0         # ğŸ¯ æ¿€è¿›ç­–ç•¥: åé‡æ•°æ®æ‹Ÿåˆï¼Œé€‚åˆé«˜è´¨é‡æ•°æ®
LOSS_RATIO_CURRENT_DEFAULT = 10.0   # ğŸ“Œ å½“å‰é»˜è®¤: å¯èƒ½å¯¹å…‹é‡Œé‡‘æ•ˆæœä¸ä½³
LOSS_RATIO_TOO_HIGH = 15.0          # âš ï¸  è¿‡é«˜æ¯”å€¼: è¿‡åº¦æ•°æ®æ‹Ÿåˆï¼Œä¸åˆ©äºå…‹é‡Œé‡‘æ’å€¼

# --- [æ–°å¢] åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥é…ç½® ---
# ğŸ¯ å½“å¯ç”¨å…‹é‡Œé‡‘é‡é‡‡æ ·æ—¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µåŠ¨æ€ç­–ç•¥ï¼š
# å‰ä¸€åŠè®­ç»ƒï¼šä¸“æ³¨æ•°æ®å­¦ä¹ ï¼ˆé«˜æ•°æ®æƒé‡ï¼Œæ— å…‹é‡Œé‡‘é‡é‡‡æ ·ï¼‰
# åä¸€åŠè®­ç»ƒï¼šå¯ç”¨å…‹é‡Œé‡‘é‡é‡‡æ ·ï¼Œé€æ¸é™ä½æ•°æ®æƒé‡
DYNAMIC_LOSS_RATIO_START = 10.0     # ğŸš€ åŠ¨æ€ç­–ç•¥èµ·å§‹æ¯”å€¼ï¼šå‰3/4æ®µè®­ç»ƒçš„æ•°æ®æƒé‡/ç‰©ç†æƒé‡æ¯”å€¼
DYNAMIC_LOSS_RATIO_END = 0.1       # ğŸ¯ åŠ¨æ€ç­–ç•¥ç»“æŸæ¯”å€¼ï¼šå1/4æ®µè®­ç»ƒç»“æŸæ—¶çš„æ¯”å€¼
FIXED_LOSS_RATIO = 10              # ğŸ“Œ å›ºå®šç­–ç•¥æ¯”å€¼ï¼šå½“ä¸å¯ç”¨å…‹é‡Œé‡‘æ—¶ä½¿ç”¨çš„å›ºå®šæ¯”å€¼

# ğŸ’¡ ä½¿ç”¨å»ºè®®:
# ğŸ”¬ ç‰©ç†ä¸»å¯¼ç­–ç•¥ (loss_ratio < 1.0):
# - ä¼˜åŠ¿: å¼ºç‰©ç†ä¸€è‡´æ€§ï¼Œæ®‹å·®åˆ†å¸ƒæ›´å¹³æ»‘è¿ç»­ï¼Œæœ‰åˆ©äºå…‹é‡Œé‡‘ç©ºé—´æ’å€¼
# - é€‚ç”¨: ç‰©ç†æ–¹ç¨‹é«˜åº¦å¯ä¿¡ã€æ•°æ®å­˜åœ¨å™ªå£°ã€éœ€è¦å¤–æ¨é¢„æµ‹çš„åœºæ™¯
# - é£é™©: å¯èƒ½åœ¨è§‚æµ‹æ•°æ®ç‚¹é™„è¿‘æ‹Ÿåˆä¸å¤Ÿç²¾ç¡®
#
# âš–ï¸ å¹³è¡¡ç­–ç•¥ (loss_ratio = 1.0 ~ 5.0):  
# - ä¼˜åŠ¿: å…¼é¡¾æ•°æ®æ‹Ÿåˆå’Œç‰©ç†çº¦æŸï¼Œæ®‹å·®åˆ†å¸ƒç›¸å¯¹å‡åŒ€
# - é€‚ç”¨: å¤§å¤šæ•°PINNåº”ç”¨ï¼Œç‰¹åˆ«æ˜¯æœ‰é€‚é‡è§‚æµ‹æ•°æ®çš„æƒ…å†µ
# - æ¨è: å…‹é‡Œé‡‘ä¼˜åŒ–çš„æœ€ä½³èµ·å§‹é€‰æ‹©
#
# ğŸ¯ æ•°æ®ä¸»å¯¼ç­–ç•¥ (loss_ratio > 5.0):
# - ä¼˜åŠ¿: ç²¾ç¡®æ‹Ÿåˆè§‚æµ‹æ•°æ®ï¼Œé€‚åˆé«˜è´¨é‡å¯†é›†æ•°æ®
# - é€‚ç”¨: æ•°æ®è´¨é‡å¾ˆé«˜ã€ç‰©ç†æ–¹ç¨‹å¯èƒ½æœ‰è¿‘ä¼¼è¯¯å·®çš„åœºæ™¯  
# - é£é™©: æ®‹å·®åœ¨æ•°æ®ç‚¹é™„è¿‘è¢«è¿‡åº¦å‹åˆ¶ï¼Œä¸åˆ©äºå…‹é‡Œé‡‘æ’å€¼
#
# ğŸ§ª å®éªŒå»ºè®®: ä¾æ¬¡æµ‹è¯• [0.5, 1.0, 3.0, 5.0, 8.0, 10.0] æ¥æ‰¾åˆ°æœ€ä½³æ¯”å€¼

# å››ç§å®éªŒæ¨¡å¼:
# ENABLE_KRIGING=False, ENABLE_DATA_INJECTION=False: ä»…å‘¨æœŸæ€§é‡å¯ (æ— è‡ªé€‚åº”ç­–ç•¥)
# ENABLE_KRIGING=False, ENABLE_DATA_INJECTION=True:  ä»…æ•°æ®æ³¨å…¥ç­–ç•¥
# ENABLE_KRIGING=True,  ENABLE_DATA_INJECTION=False: ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·ç­–ç•¥  
# ENABLE_KRIGING=True,  ENABLE_DATA_INJECTION=True:  å®Œæ•´è‡ªé€‚åº”PINN

class DummyDataLoader:
    """
    ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»å¤–éƒ¨æ–‡ä»¶åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®(æ›¿ä»£åŸæœ‰çš„DummyDataLoader)ã€‚
    """
    def __init__(self, data_path: str, space_dims: np.ndarray, num_samples: int):
        self.data_path = data_path
        self.space_dims = space_dims
        self.num_samples = num_samples
        print(f"INFO: (DataLoader) Initialized with data_path='{self.data_path}'")

    def get_training_data(self, split_ratios: list = None, test_set_size: int = None):
        """
        åŠ è½½ã€å¤„ç†å¹¶é‡‡æ ·ç¨€ç–è®­ç»ƒç‚¹ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ¯”ä¾‹åˆ—è¡¨è¿›è¡Œåˆ†å‰²ã€‚
        
        Args:
            split_ratios (list, optional): ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œå…¶å’Œåº”å°äº1ã€‚
                ä¾‹å¦‚ [0.7, 0.1, 0.1] ä»£è¡¨ï¼š
                - 70% ä½œä¸ºä¸»è®­ç»ƒé›†
                - 10% ä½œä¸ºç¬¬ä¸€ä¸ªå‚¨å¤‡é›†
                - 10% ä½œä¸ºç¬¬äºŒä¸ªå‚¨å¤‡é›†
                - å‰©ä½™çš„ 10% å°†ä½œä¸ºæµ‹è¯•é›†ã€‚
                å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ 80/20 è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ã€‚
            test_set_size (int, optional): å¦‚æœæŒ‡å®šï¼Œå°†ç”Ÿæˆç‹¬ç«‹çš„æµ‹è¯•é›†è€Œéä»è®­ç»ƒæ•°æ®åˆ†å‰²ã€‚
        """
        # ... (å‰é¢åŠ è½½å’Œé‡‡æ ·æ•°æ®çš„éƒ¨åˆ†ä¿æŒä¸å˜) ...
        print(f"INFO: (DataLoader) Loading raw data from {self.data_path}...")
        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            
        raw_data = get_data(self.data_path)
        
        print("INFO: (DataLoader) Normalizing dose data...")
        dose_data = DataLoader.load_dose_from_dict(
            data_dict=raw_data,
            space_dims=self.space_dims
        )
        
        print(f"INFO: (DataLoader) Sampling {self.num_samples} training points...")
        train_points, train_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=self.num_samples,
            sampling_strategy='positive_only',
        )
        print(f"INFO: (DataLoader) âœ… Successfully sampled {len(train_points)} points.")

        # å°†åæ ‡å’Œå€¼åˆå¹¶æˆ [x, y, z, value] æ ¼å¼
        all_sampled_data = np.hstack([train_points, train_values.reshape(-1, 1)])
        
        # [æ–°å¢] ç”Ÿæˆç‹¬ç«‹æµ‹è¯•é›†ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if test_set_size is not None:
            print(f"INFO: (DataLoader) Generating independent test set of size {test_set_size}...")
            test_set = self._generate_independent_test_set(dose_data, test_set_size)
        else:
            test_set = None  # å°†åœ¨ä¸‹é¢çš„åˆ†å‰²é€»è¾‘ä¸­å¤„ç†
        
        # [æ–°é€»è¾‘] ä½¿ç”¨å¯é…ç½®çš„åˆ†å‰²ç­–ç•¥
        if split_ratios is None:
            # é»˜è®¤è¡Œä¸ºï¼š80/20 åˆ†å‰²
            if test_set is None:
                main_train_set, test_set = train_test_split(all_sampled_data, test_size=0.2, random_state=42)
            else:
                main_train_set = all_sampled_data  # å…¨éƒ¨ç”¨ä½œè®­ç»ƒæ•°æ®
            reserve_pools = []
        else:
            if test_set is None and sum(split_ratios) >= 1.0:
                raise ValueError("split_ratios çš„æ€»å’Œå¿…é¡»å°äº 1.0ï¼Œä»¥ä¾¿ä¸ºæµ‹è¯•é›†ç•™å‡ºç©ºé—´ã€‚")

            remaining_data = all_sampled_data
            data_pools = []
            
            # å¾ªç¯åˆ‡åˆ†å‡ºä¸»è®­ç»ƒé›†å’Œæ‰€æœ‰å‚¨å¤‡é›†
            current_total_fraction = 1.0
            for ratio in split_ratios:
                # è®¡ç®—å½“å‰æ¯”ä¾‹ç›¸å¯¹äºå‰©ä½™æ•°æ®é‡çš„æ¯”ä¾‹
                split_fraction = ratio / current_total_fraction
                
                # [ä¿®å¤] æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
                test_size_fraction = 1.0 - split_fraction
                if test_size_fraction < 1e-10:  # å¦‚æœéå¸¸æ¥è¿‘0ï¼Œåˆ™è®¾ä¸º0
                    test_size_fraction = 0.0
                elif test_size_fraction > 1.0:  # å¦‚æœè¶…è¿‡1ï¼Œåˆ™è®¾ä¸º1
                    test_size_fraction = 1.0
                
                # å¦‚æœtest_sizeä¸º0ï¼Œè¯´æ˜å‰©ä½™æ•°æ®å°±æ˜¯å½“å‰pool
                if test_size_fraction == 0.0:
                    pool = remaining_data
                    remaining_data = np.array([]).reshape(0, remaining_data.shape[1]) if len(remaining_data) > 0 else np.array([])
                else:
                    pool, remaining_data = train_test_split(remaining_data, test_size=test_size_fraction, random_state=42)
                
                data_pools.append(pool)
                current_total_fraction -= ratio

            main_train_set = data_pools[0]
            reserve_pools = data_pools[1:]
            
            # å¦‚æœæ²¡æœ‰ç‹¬ç«‹æµ‹è¯•é›†ï¼Œåˆ™ä½¿ç”¨å‰©ä½™æ•°æ®
            if test_set is None:
                test_set = remaining_data
        
        print(f"INFO: (DataLoader) âœ… Split data into: Main training ({len(main_train_set)}), Test ({len(test_set)}), Reserve Pools ({len(reserve_pools)} pools).")
        if reserve_pools:
            for i, pool in enumerate(reserve_pools):
                print(f"    - Reserve Pool {i+1}: {len(pool)} points")

        return main_train_set, reserve_pools, test_set, dose_data

    def _generate_independent_test_set(self, dose_data: dict, test_set_size: int):
        """
        ç”Ÿæˆå®Œå…¨ç‹¬ç«‹äºè®­ç»ƒæ•°æ®çš„æµ‹è¯•é›†ï¼Œåœ¨æ•´ä¸ªç‰©ç†åŸŸå†…å‡åŒ€é‡‡æ ·ã€‚
        
        Args:
            dose_data (dict): åŒ…å«ç‰©ç†åŸŸè¾¹ç•Œçš„æ•°æ®å­—å…¸
            test_set_size (int): æµ‹è¯•é›†å¤§å°
            
        Returns:
            np.ndarray: æµ‹è¯•é›†æ•°æ® [x, y, z, value]
        """
        # ä½¿ç”¨ DataLoader.sample_training_points åœ¨æ•´ä¸ªåŸŸå†…é‡‡æ ·æµ‹è¯•ç‚¹
        test_points, test_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=test_set_size,
            sampling_strategy='uniform',  # ä½¿ç”¨å‡åŒ€é‡‡æ ·
        )
        
        # åˆå¹¶ä¸º [x, y, z, value] æ ¼å¼
        test_set = np.hstack([test_points, test_values.reshape(-1, 1)])
        print(f"INFO: (DataLoader) âœ… Generated independent test set with {len(test_set)} points.")
        return test_set

class GPUKriging:
    """
    [çœŸå®å®ç°] GPUåŠ é€Ÿçš„å…‹é‡Œé‡‘æ¨¡å‹çš„é€‚é…å™¨ã€‚
    è¯¥å®ç°å€Ÿé‰´äº† ComposeTools.py ä¸­çš„ KrigingAdapterï¼Œå¹¶è°ƒç”¨äº† myKriging åº“ã€‚
    """
    def __init__(self, variogram_model='exponential', **kwargs):
        """
        åˆå§‹åŒ–Krigingé€‚é…å™¨ã€‚
        
        Args:
            variogram_model (str): å…‹é‡Œé‡‘æ‰€éœ€çš„å˜å¼‚å‡½æ•°æ¨¡å‹ã€‚
            **kwargs: å…¶ä»–å¯ä»¥ä¼ é€’ç»™ myKriging åº“çš„å‚æ•° (å¦‚ nlags, block_size)ã€‚
        """
        self.model = None
        self._is_fitted = False
        self.variogram_model = variogram_model
        self.kriging_params = kwargs
        print(f"INFO: (GPUKriging) Initialized with variogram model: {self.variogram_model}")

    def fit(self, points: np.ndarray, values: np.ndarray):
        """
        ä½¿ç”¨ç¨€ç–çš„ç‚¹åæ ‡å’Œå¯¹åº”çš„æ®‹å·®å€¼æ¥è®­ç»ƒå…‹é‡Œé‡‘ä»£ç†æ¨¡å‹ã€‚
        """
        print(f"INFO: (GPUKriging) Fitting model with {len(points)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df = pd.DataFrame({
            'x': points[:, 0],
            'y': points[:, 1],
            'z': points[:, 2],
            'target': values
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_training å‡½æ•°
        self.model = kriging_training(
            df=df,
            variogram_model=self.variogram_model,
            nlags=self.kriging_params.get('nlags', 8),
            enable_plotting=False, # è®­ç»ƒä»£ç†æ¨¡å‹æ—¶é€šå¸¸ä¸ç»˜å›¾
            weight=False,
            uk=False,
            cpu_on=False # ç¡®ä¿ä½¿ç”¨GPU
        )
        
        self._is_fitted = True
        print("INFO: (GPUKriging) âœ… Model fitted.")

    def predict(self, points_to_predict: np.ndarray) -> np.ndarray:
        """
        å¯¹æ–°çš„ç‚¹è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼Œåˆ©ç”¨GPUåŠ é€Ÿã€‚
        """
        if not self._is_fitted:
            raise RuntimeError("Kriging model must be fitted before prediction.")
        
        print(f"INFO: (GPUKriging) Predicting values for {len(points_to_predict)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df_pred = pd.DataFrame({
            'x': points_to_predict[:, 0],
            'y': points_to_predict[:, 1],
            'z': points_to_predict[:, 2],
            'target': np.zeros(points_to_predict.shape[0]) # è™šæ‹Ÿç›®æ ‡å€¼
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_testing å‡½æ•°ï¼Œç¡®ä¿ä½¿ç”¨GPUåŠ é€Ÿé…ç½®
        predictions, _ = kriging_testing(
            df=df_pred,
            model=self.model,
            block_size=self.kriging_params.get('block_size', 10000),
            cpu_on=False, # ç¡®ä¿ä½¿ç”¨GPU
            style="gpu_b", # ä½¿ç”¨GPUæ‰¹å¤„ç†é£æ ¼
            multi_process=False,
            print_time=False,
            torch_ac=False, # ä½¿ç”¨PyTorchåŠ é€Ÿ
            compute_precision=False # é¢„æµ‹æ®‹å·®æ—¶ä¸éœ€è¦ç²¾åº¦è®¡ç®—
        )
        
        print(f"INFO: (GPUKriging) âœ… Prediction complete.")
        return predictions.flatten() # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„

class PINNModel:
    """
    [çœŸå®å®ç°] ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ã€‚
    è¯¥å®ç°è¢«è®¾è®¡ä¸ºå¯ä»å¤–éƒ¨æ§åˆ¶çš„æ¨¡å¼ï¼Œä»¥æ”¯æŒè‡ªé€‚åº”è®­ç»ƒæµç¨‹ã€‚
    """
    def __init__(self, dose_data: dict, training_data: np.ndarray, test_data: np.ndarray, num_collocation_points: int, network_layers=[3, 64, 64, 64, 1], lr=1e-3, loss_ratio=10.0):
        """
        åˆå§‹åŒ–PINNæ¨¡å‹ï¼Œä½†ä¸PINNTrainerä¸åŒï¼Œè¿™é‡Œåªåšå‡†å¤‡å·¥ä½œï¼Œä¸å¼€å§‹è®­ç»ƒã€‚
        
        Args:
            dose_data (dict): ä»DataLoaderåŠ è½½çš„æ•°æ®å­—å…¸ã€‚
            training_data (np.ndarray): ç¨€ç–è®­ç»ƒæ•°æ® [x,y,z,value]ã€‚
            test_data (np.ndarray): ç¨€ç–æµ‹è¯•æ•°æ® [x,y,z,value]ã€‚
            num_collocation_points (int): æ±‚è§£åŸŸç‚¹çš„æ•°é‡ã€‚
            network_layers (list): ç¥ç»ç½‘ç»œç»“æ„ã€‚
            lr (float): å­¦ä¹ ç‡ã€‚
            loss_ratio (float): æ•°æ®æŸå¤±æƒé‡ä¸ç‰©ç†æŸå¤±æƒé‡çš„æ¯”å€¼ (æ•°æ®æŸå¤±æƒé‡ / ç‰©ç†æŸå¤±æƒé‡)ï¼Œé»˜è®¤10.0ã€‚
        """
        print("INFO: (PINNModel) Initializing a DeepXDE-based model for external control...")
        
        self.test_data_linear = test_data # å­˜å‚¨çº¿æ€§å°ºåº¦çš„æµ‹è¯•æ•°æ®
        
        # 1. å®šä¹‰å‡ ä½•
        world_min = dose_data['world_min']
        world_max = dose_data['world_max']
        self.geometry = dde.geometry.Cuboid(world_min, world_max)

        # 2. å®šä¹‰å¯è®­ç»ƒå‚æ•°
        k_initial_guess = 1.0 
        self.log_k_pinn = dde.Variable(np.log(k_initial_guess))
        
        # 3. å®šä¹‰PDEä¸ºç±»çš„æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å…¶ä»–åœ°æ–¹å¤ç”¨
        self.pde = self._build_pde_func()
        
        # 4. å®šä¹‰è®­ç»ƒæ•°æ®ç‚¹
        observe_x = training_data[:, :3]
        observe_y = np.log(np.maximum(training_data[:, 3:], 1e-30))
        data_points = dde.icbc.PointSetBC(observe_x, observe_y, component=0)
        
        # 5. ç»„åˆæˆdde.data.PDEå¯¹è±¡
        self.data = dde.data.PDE(
            self.geometry,
            self.pde,
            [data_points],
            num_domain=num_collocation_points,
            anchors=observe_x,
            # æˆ‘ä»¬è‡ªå®šä¹‰çš„æŒ‡æ ‡ä¼šä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„self.test_data_linear
        )
        
        # 6. åˆ›å»ºç½‘ç»œå’Œæ¨¡å‹
        self.net = dde.nn.FNN(network_layers, "tanh", "Glorot normal")
        self.model = dde.Model(self.data, self.net)
        
        # 7. è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°è¢«ç§»å‡ºä¸ºç±»æ–¹æ³• mean_relative_error_metric

        self.lr = lr # ä¿å­˜å­¦ä¹ ç‡ä»¥å¤‡é‡ç¼–è¯‘æ—¶ä½¿ç”¨
        self.loss_ratio = loss_ratio # [æ–°å¢] ä¿å­˜æŸå¤±æ¯”å€¼ä»¥å¤‡é‡ç¼–è¯‘æ—¶ä½¿ç”¨
        
        # [æ–°å¢] MREå†å²è®°å½•åˆ—è¡¨
        self.mre_history = []
        self.epoch_history = []
        
        # 8. ç¼–è¯‘æ¨¡å‹ï¼ŒåŠ å…¥è‡ªå®šä¹‰æŒ‡æ ‡
        self.compile_model()
        print(f"INFO: (PINNModel) âœ… Model compiled with loss_ratio={loss_ratio:.1f} (data/physics weight ratio).")
        print(f"      â””â”€ Loss weights: [physics={1.0:.1f}, data={loss_ratio:.1f}]")
        
    def compile_model(self):
        """å°†æ¨¡å‹ç¼–è¯‘å°è£…æˆä¸€ä¸ªæ–¹æ³•ï¼Œæ–¹ä¾¿é‡ç”¨ã€‚"""
        # [ä¿®æ­£] åœ¨è¿™é‡Œè®¾ç½®æŒ‡æ ‡å‡½æ•°çš„æ˜¾ç¤ºåç§°
        # self.mean_relative_error_metric.__name__ = "MRE_test_set" # [ä¿®æ­£] ç§»é™¤æ­¤è¡Œï¼Œä¸èƒ½ä¸ºç±»æ–¹æ³•è®¾ç½®__name__

        # [æ–°å¢] åŸºäºloss_ratioåŠ¨æ€è®¡ç®—loss_weights
        # loss_ratio = æ•°æ®æŸå¤±æƒé‡ / ç‰©ç†æŸå¤±æƒé‡
        physics_weight = 1.0
        data_weight = self.loss_ratio
        loss_weights = [physics_weight, data_weight]

        self.model.compile(
            "adam", 
            lr=self.lr, 
            loss_weights=loss_weights, 
            external_trainable_variables=[self.log_k_pinn],
            metrics=[self.mean_relative_error_metric] # [ä¿®æ­£] ä¼ é€’å‡½æ•°å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        )
    
    def update_loss_ratio(self, new_loss_ratio: float):
        """
        [æ–°å¢] åŠ¨æ€æ›´æ–°æŸå¤±æƒé‡æ¯”å€¼å¹¶é‡æ–°ç¼–è¯‘æ¨¡å‹ã€‚
        
        Args:
            new_loss_ratio (float): æ–°çš„æ•°æ®æŸå¤±æƒé‡/ç‰©ç†æŸå¤±æƒé‡æ¯”å€¼
        """
        if abs(self.loss_ratio - new_loss_ratio) > 1e-6:  # é¿å…ä¸å¿…è¦çš„é‡ç¼–è¯‘
            old_ratio = self.loss_ratio
            self.loss_ratio = new_loss_ratio
            self.compile_model()
            print(f"INFO: (PINNModel) æŸå¤±æƒé‡æ¯”å€¼å·²æ›´æ–°: {old_ratio:.2f} â†’ {new_loss_ratio:.2f}")
            print(f"      â””â”€ æ–°çš„æŸå¤±æƒé‡: [ç‰©ç†={1.0:.1f}, æ•°æ®={new_loss_ratio:.1f}]")
            
            # [ä¿®æ­£] é‡æ–°ç¼–è¯‘åéœ€è¦é‡å»ºè®­ç»ƒçŠ¶æ€
            self.model.train(iterations=0, display_every=100000)
        else:
            print(f"INFO: (PINNModel) æŸå¤±æƒé‡æ¯”å€¼æœªå˜åŒ–ï¼Œè·³è¿‡é‡ç¼–è¯‘ (å½“å‰: {self.loss_ratio:.2f})")

    def mean_relative_error_metric(self, y_true_ignored, y_pred_ignored):
        """
        ä¸€ä¸ª"hack"çš„æŒ‡æ ‡å‡½æ•°ã€‚å®ƒå¿½ç•¥ddeä¼ å…¥çš„å‚æ•°ï¼Œ
        è½¬è€Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„ã€åŸºäºçœŸå®ç‰©ç†å€¼çš„æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚
        """
        # ä½¿ç”¨æ¨¡å‹å¯¹æˆ‘ä»¬è‡ªå·±çš„æµ‹è¯•ç‚¹è¿›è¡Œé¢„æµ‹
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        
        # å°†é¢„æµ‹å€¼å’ŒçœŸå®å€¼éƒ½è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))
        
        # [æ–°å¢] è®°å½•MREå†å²
        current_epoch = self.model.train_state.step if self.model.train_state.step else 0
        self.mre_history.append(mre)
        self.epoch_history.append(current_epoch)
        
        return mre

    def inject_new_data(self, new_data_array: np.ndarray):
        """
        [æ–°èƒ½åŠ›] å‘æ¨¡å‹ä¸­æ³¨å…¥æ–°çš„è®­ç»ƒæ•°æ®ç‚¹ã€‚
        """
        print(f"\nINFO: (PINNModel)  injecting {len(new_data_array)} new data points...")
        
        # 1. è·å–ç°æœ‰æ•°æ®
        current_bc = self.data.bcs[0]
        current_points = current_bc.points
        current_values_log = current_bc.values.cpu()

        # 2. å‡†å¤‡æ–°æ•°æ®
        new_points = new_data_array[:, :3]
        new_values_log = np.log(np.maximum(new_data_array[:, 3:], 1e-30)).reshape(-1, 1)

        # 3. åˆå¹¶æ–°æ—§æ•°æ®
        combined_points = np.vstack([current_points, new_points])
        combined_values_log = np.vstack([current_values_log, new_values_log])
        
        print(f"    Total training points increased to {len(combined_points)}.")

        # 4. åˆ›å»ºæ–°çš„ PointSetBC å’Œ PDE æ•°æ®å¯¹è±¡
        new_bc = dde.icbc.PointSetBC(combined_points, combined_values_log, component=0)
        
        # æ›´æ–°é”šç‚¹ä»¥åŒ…å«æ‰€æœ‰è®­ç»ƒæ•°æ®
        new_anchors = combined_points
        
        new_data_obj = dde.data.PDE(
            self.geometry,
            self.pde,
            [new_bc],
            num_domain=self.data.num_domain,
            anchors=new_anchors
        )
        
        # 5. æ›´æ–°æ¨¡å‹çš„æ•°æ®å¹¶é‡æ–°ç¼–è¯‘
        self.data = new_data_obj
        self.model.data = self.data
        self.compile_model()
        print("INFO: (PINNModel) âœ… Model re-compiled with new data. Initializing new train state...")
        # [ä¿®æ­£] è°ƒç”¨ train(0) æ¥å¼ºåˆ¶ä½¿ç”¨æ–°æ•°æ®å¯¹è±¡é‡å»ºè®­ç»ƒçŠ¶æ€
        self.model.train(iterations=0, display_every=100000) # display_everyè®¾ä¸ºå¤§æ•°ä»¥é¿å…ä¸å¿…è¦çš„è¾“å‡º
        print("INFO: (PINNModel) âœ… New train state initialized.")
        
        # [ä¿®æ­£] æ³¨å…¥æ•°æ®åï¼Œè®°å½•ä¸€æ¬¡å½“å‰MREä»¥ä¿æŒå†å²è¿ç»­æ€§
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        current_mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))
        current_epoch = self.model.train_state.step if self.model.train_state.step else 0
        self.mre_history.append(current_mre)
        self.epoch_history.append(current_epoch)
        print(f"INFO: (PINNModel) MRE after data injection: {current_mre:.6f} at epoch {current_epoch}")
        
    def _build_pde_func(self):
        """å°†PDEå®šä¹‰å°è£…åœ¨ä¸€ä¸ªå·¥å‚å‡½æ•°ä¸­ï¼Œä»¥æ•è·self.log_k_pinnã€‚"""
        def pde_func(x, u):
            grad_u_sq = dde.grad.jacobian(u, x, i=0, j=0)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=1)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=2)**2
            laplacian_u = dde.grad.hessian(u, x, i=0, j=0) + \
                          dde.grad.hessian(u, x, i=1, j=1) + \
                          dde.grad.hessian(u, x, i=2, j=2)
            k_squared = dde.backend.exp(2 * self.log_k_pinn)
            return grad_u_sq + laplacian_u - k_squared
        return pde_func

    def run_training_cycle(self, max_epochs: int, detect_every: int, collocation_points: np.ndarray, 
                         detection_threshold: float = 0.1):
        """
        [é‡æ„] æ‰§è¡Œä¸€ä¸ªå¸¦æœ‰åŠ¨æ€åœæ­¢æ¡ä»¶çš„è®­ç»ƒå‘¨æœŸã€‚
        
        Args:
            max_epochs (int): å½“å‰å‘¨æœŸçš„æœ€å¤§è®­ç»ƒè½®æ•°ã€‚
            detect_every (int): æ¯éš”å¤šå°‘è½®è¿›è¡Œä¸€æ¬¡æ€§èƒ½æ£€æµ‹ã€‚
            collocation_points (np.ndarray): ç”¨äºæœ¬å‘¨æœŸçš„é…ç½®ç‚¹ã€‚
            detection_threshold (float): è§¦å‘æ—©åœçš„ç›¸å¯¹æ”¹è¿›é˜ˆå€¼ã€‚
        
        Returns:
            dict: ä¸€ä¸ªåŒ…å«è®­ç»ƒç»“æœä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚{'stagnation_detected': bool}
        """
        # 1. ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä¹‰æ£€æŸ¥ç‚¹æ–‡ä»¶åçš„å‰ç¼€ï¼Œå¹¶ç¡®ä¿ç›®å½•å­˜åœ¨
        script_dir = Path(__file__).parent.resolve()
        checkpoint_path_prefix = str(script_dir / "models" / "best_model_in_cycle")
        os.makedirs(Path(checkpoint_path_prefix).parent, exist_ok=True)

        # 2. æ›´æ–°æ±‚è§£åŸŸç‚¹
        num_bc_points = self.data.bcs[0].points.shape[0]
        if self.model.train_state.X_train is None:
            # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ï¼Œä»¥ä¾¿å¯ä»¥ä¿®æ”¹X_train
            self.model.train(iterations=0)
        start_index = num_bc_points
        end_index = len(self.model.train_state.X_train) - len(self.data.anchors)
        self.model.train_state.X_train[start_index:end_index] = collocation_points
        
        # [æ–°] åˆå§‹åŒ–æœ¬å‘¨æœŸçš„è¿”å›çŠ¶æ€
        stagnation_detected_this_run = False
        data_injected_this_cycle = False
        
        # 3. åˆ›å»ºå›è°ƒï¼Œå¹¶ç”¨å½“å‰æ¨¡å‹çš„æ€§èƒ½åˆå§‹åŒ–å®ƒ
        stopper = EarlyCycleStopper(
            detection_threshold=detection_threshold,
            display_every=5,
            checkpoint_path_prefix=checkpoint_path_prefix
        )
        # [ä¿®æ­£] åœ¨é‡ç½®æ—¶ï¼Œä¼ å…¥å½“å‰æ¨¡å‹çš„MREå’Œåˆå§‹æ£€æŸ¥ç‚¹ä½œä¸ºåŸºçº¿
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        initial_mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))

        # ä¸ºåˆå§‹çŠ¶æ€åˆ›å»ºç¬¬ä¸€ä¸ªåŸºå‡†æ£€æŸ¥ç‚¹
        epochs_before_cycle = self.model.train_state.step or 0
        self.model.save(checkpoint_path_prefix, verbose=0)
        initial_model_path = f"{checkpoint_path_prefix}-{epochs_before_cycle}.pt"
        
        stopper.reset_cycle(initial_mre=initial_mre, initial_model_path=initial_model_path)
        
        print(f"INFO: (PINNModel) Starting dynamic training cycle (max: {max_epochs} epochs, detect every: {detect_every})...")
        print(f"    Initial MRE for this cycle is {initial_mre:.4f}")
        
        remaining_epochs = max_epochs
        while remaining_epochs > 0:
            epochs_to_run = min(detect_every, remaining_epochs)
            
            self.model.train(
                iterations=epochs_to_run, 
                display_every=5,
                callbacks=[stopper]
            )

            # --- [æ–°ç­–ç•¥] æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰ç»“æŸæœ¬è½®è‡ªé€‚åº”å‘¨æœŸ ---
            should_exit_cycle = False

            # æ¡ä»¶1: åœæ» (Stagnation) - æ¨¡å‹æ€§èƒ½åœ¨æœ¬è½®è®­ç»ƒåå˜å·®
            if stopper.best_model_path and os.path.exists(stopper.best_model_path):
                latest_mre = self.model.train_state.metrics_test[-1]
                if latest_mre > stopper.best_mre:
                    print(f"    âš ï¸ Stagnation detected: MRE increased to {latest_mre:.4f} (best is {stopper.best_mre:.4f}).")
                    stagnation_detected_this_run = True
                    
                    print(f"    â†³ Forcing new adaptive resampling cycle...")
                    self.model.restore(stopper.best_model_path, verbose=0) 
                    should_exit_cycle = True

            # æ¡ä»¶2: å¿«é€Ÿæå‡ (Rapid Improvement) - ğŸ”§ å¯é€‰æ‹©æ˜¯å¦å¯ç”¨
            if ENABLE_RAPID_IMPROVEMENT_EARLY_STOP and stopper.should_stop:
                print(f"\nINFO: (PINNModel) ğŸ“ˆ Rapid improvement! Capitalizing on gains and forcing new resampling.")
                should_exit_cycle = True
            
            if should_exit_cycle:
                break
                
            remaining_epochs -= epochs_to_run
        else:
             print(f"\nINFO: (PINNModel) Max epochs reached for this cycle.")

        # 4. [é‡è¦] æ— è®ºå¦‚ä½•ï¼Œéƒ½ä»æœ€ç»ˆçš„æœ€ä½³æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å‹ï¼Œå¹¶æ¸…ç†æ–‡ä»¶
        if stopper.best_model_path and os.path.exists(stopper.best_model_path):
            print(f"INFO: (PINNModel) Restoring model to best state from '{stopper.best_model_path}'...")
            self.model.restore(stopper.best_model_path, verbose=1)
            os.remove(stopper.best_model_path) # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        else:
            print("WARNING: (PINNModel) Best checkpoint file not found. Model may not be in its best state.")
            
        return {'stagnation_detected': stagnation_detected_this_run}

    def predict(self, points: np.ndarray) -> np.ndarray:
        """
        [æ–°] ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¿”å›çº¿æ€§å°ºåº¦çš„ç‰©ç†å€¼ã€‚
        
        Args:
            points (np.ndarray): å¾…é¢„æµ‹ç‚¹çš„åæ ‡ï¼Œå½¢çŠ¶ä¸º (N, 3)ã€‚
            
        Returns:
            np.ndarray: é¢„æµ‹çš„ç‰©ç†å€¼ï¼Œå½¢çŠ¶ä¸º (N,)ã€‚
        """
        print(f"INFO: (PINNModel) Predicting on {len(points)} points...")
        # model.predict è¿”å›çš„æ˜¯å¯¹æ•°å°ºåº¦çš„å€¼
        pred_y_log = self.model.predict(points)
        # è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
        pred_y_linear = np.exp(pred_y_log)
        return pred_y_linear.flatten()

    def compute_pde_residual(self, points: np.ndarray) -> np.ndarray:
        """
        [çœŸå®å®ç°] è®¡ç®—ç»™å®šç‚¹ä¸Šçš„ç‰©ç†æ–¹ç¨‹æ®‹å·®ã€‚
        åˆ©ç”¨ deepxde çš„ model.predict(operator=...) åŠŸèƒ½ã€‚
        """
        print(f"INFO: (PINNModel) Computing PDE residuals for {len(points)} points...")
        
        # deepxde.Model.predict å¯ä»¥æ¥å—ä¸€ä¸ª operator å‚æ•°
        # æˆ‘ä»¬å°† self.pde (åœ¨__init__ä¸­å®šä¹‰çš„å‡½æ•°) ä½œä¸ºç®—å­ä¼ å…¥
        residuals = self.model.predict(points, operator=self.pde)
        
        # è¿”å›æ®‹å·®çš„ç»å¯¹å€¼ï¼Œå¹¶å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        return np.abs(residuals).flatten()

class EarlyCycleStopper(dde.callbacks.Callback):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰å›è°ƒï¼Œç”¨äºåœ¨è®­ç»ƒå‘¨æœŸå†…å®ç°åŸºäºæ€§èƒ½çš„"æ—©åœ"ã€‚
    åŒæ—¶è‡ªå·±è´Ÿè´£ä¿å­˜å‘¨æœŸå†…çš„æœ€ä½³æ¨¡å‹ã€‚
    """
    def __init__(self, detection_threshold: float, display_every: int, checkpoint_path_prefix: str):
        super().__init__()
        self.threshold = detection_threshold
        self.display_every = display_every
        self.checkpoint_path_prefix = checkpoint_path_prefix
        self.best_mre = np.inf
        self.should_stop = False
        self.best_model_path = "" # å°†å­˜å‚¨æœ€ä½³æ¨¡å‹çš„å®Œæ•´çœŸå®è·¯å¾„

    def reset_cycle(self, initial_mre: float = np.inf, initial_model_path: str = ""):
        """
        æ‰‹åŠ¨é‡ç½®æ•´ä¸ªå‘¨æœŸçš„çŠ¶æ€ï¼Œä¸ºæ–°çš„è‡ªé€‚åº”å‘¨æœŸåšå‡†å¤‡ã€‚
        å¯ä»¥æ¥æ”¶ä¸€ä¸ªåˆå§‹MREå’Œæ¨¡å‹è·¯å¾„ä½œä¸ºæœ¬å‘¨æœŸçš„æ€§èƒ½åŸºçº¿ã€‚
        """
        # æ¸…ç†ä¸Šä¸€è½®å¯èƒ½é—ç•™çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        if self.best_model_path and os.path.exists(self.best_model_path):
            os.remove(self.best_model_path)
        
        self.best_mre = initial_mre
        self.best_model_path = initial_model_path
        self.should_stop = False

    def on_epoch_end(self):
        """åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¢«è°ƒç”¨, å¹¶ä¸”åœ¨è¿™é‡Œæ£€æŸ¥æ€§èƒ½"""
        if self.model.train_state.step > 0 and self.model.train_state.step % self.display_every == 0:
            if not self.model.train_state.metrics_test:
                 return

            latest_mre = self.model.train_state.metrics_test[-1]
            
            if self.best_mre != np.inf:
                improvement = self.best_mre - latest_mre
                required_improvement_amount = self.best_mre * self.threshold
                
                if improvement > required_improvement_amount:
                    print(f"    ğŸ’¡ Early Stop: MRE dropped from {self.best_mre:.4f} to {latest_mre:.4f} (>{self.threshold:.0%}).")
                    self.should_stop = True
            
            # åˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯å¦æ˜¯æ–°çš„æœ€ä¼˜æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¿å­˜å®ƒ
            if latest_mre < self.best_mre:
                print(f"    â­ New best model found (MRE: {latest_mre:.4f}). Checkpointing...")
                self.best_mre = latest_mre

                # æ¸…ç†ä¸Šä¸€ä¸ªæœ€ä½³æ¨¡å‹
                if self.best_model_path and os.path.exists(self.best_model_path):
                    os.remove(self.best_model_path)
                
                # æ„å»ºæ–°çš„æœ€ä½³æ¨¡å‹è·¯å¾„å¹¶ä¿å­˜
                current_step = self.model.train_state.step
                self.best_model_path = f"{self.checkpoint_path_prefix}-{current_step}.pt"
                self.model.save(self.checkpoint_path_prefix, verbose=0)

class AdaptiveSampler:
    """
    [å»ºè®®æ‚¨å®ç°] è‡ªé€‚åº”é‡‡æ ·å™¨ã€‚
    """
    def __init__(self, domain_bounds, total_candidates=100000):
        self.bounds = domain_bounds
        # é¢„å…ˆåœ¨æ•´ä¸ªåŸŸå†…ç”Ÿæˆå¤§é‡çš„å€™é€‰ç‚¹ï¼Œåç»­ä»ä¸­ç­›é€‰
        self.candidate_points = np.random.rand(total_candidates, 3) * \
            (domain_bounds[1] - domain_bounds[0]) + domain_bounds[0]
        print(f"INFO: (AdaptiveSampler) Initialized with {total_candidates} candidate points.")

    def generate_new_collocation_points(
        self,
        kriging_model: GPUKriging,
        num_points_to_sample: int,
        cycle_number: int = 1
    ) -> tuple[np.ndarray, float]:
        """
        ä½¿ç”¨Krigingæ¨¡å‹å¼•å¯¼ç”Ÿæˆæ–°çš„é…ç½®ç‚¹ã€‚
        Args:
            kriging_model: è®­ç»ƒå¥½çš„æ®‹å·®ä»£ç†æ¨¡å‹ã€‚
            num_points_to_sample: éœ€è¦ç”Ÿæˆçš„æ€»ç‚¹æ•°ã€‚
            cycle_number: å½“å‰æ˜¯ç¬¬å‡ ä¸ªè‡ªé€‚åº”å‘¨æœŸï¼Œç”¨äºåŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥ã€‚
        Returns:
            tuple: (æ–°çš„é…ç½®ç‚¹é›†, ä½¿ç”¨çš„æ¢ç´¢ç‡)
        """
        # [æ–°é€»è¾‘] åŸºäºå‘¨æœŸæ•°å’Œå…¨å±€é…ç½®è®¡ç®—æ¢ç´¢ç‡
        exploration_ratio = max(
            FINAL_EXPLORATION_RATIO,
            INITIAL_EXPLORATION_RATIO - (cycle_number - 1) * EXPLORATION_DECAY_RATE
        )
        
        print(f"INFO: (AdaptiveSampler) å‘¨æœŸæ€§å…‹é‡Œé‡‘é‡é‡‡æ · (ç¬¬{cycle_number}æ¬¡)")
        print(f"      æ¢ç´¢ç‡: {exploration_ratio:.1%} (åˆå§‹:{INITIAL_EXPLORATION_RATIO:.1%} â†’ æœ€ç»ˆ:{FINAL_EXPLORATION_RATIO:.1%})")

        # 1. ä½¿ç”¨Krigingä»£ç†æ¨¡å‹é¢„æµ‹æ‰€æœ‰å€™é€‰ç‚¹çš„æ®‹å·®
        predicted_residuals = kriging_model.predict(self.candidate_points)
        print(f"    - Krigingé¢„æµ‹æ®‹å·®ç»Ÿè®¡ (åœ¨ {len(self.candidate_points)} ä¸ªå€™é€‰ç‚¹ä¸Š):")
        print(f"      - Max={np.max(predicted_residuals):.4e}, "
              f"Min={np.min(predicted_residuals):.4e}, "
              f"Mean={np.mean(predicted_residuals):.4e}, "
              f"Std={np.std(predicted_residuals):.4e}")

        # 2. "Hard-Case Mining": æ‰¾åˆ°é¢„æµ‹æ®‹å·®æœ€å¤§çš„ç‚¹çš„ç´¢å¼•
        num_exploitation_points = int(num_points_to_sample * (1 - exploration_ratio))
        hard_case_indices = np.argsort(predicted_residuals)[-num_exploitation_points:]
        exploitation_points = self.candidate_points[hard_case_indices]

        # 3. "Exploration": åŠ å…¥ä¸€éƒ¨åˆ†éšæœºç‚¹ä»¥é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜
        num_exploration_points = num_points_to_sample - num_exploitation_points
        random_indices = np.random.choice(len(self.candidate_points), num_exploration_points, replace=False)
        exploration_points = self.candidate_points[random_indices]

        print(f"INFO: (AdaptiveSampler) Generated {num_exploitation_points} exploitation points and {num_exploration_points} exploration points.")
        
        return np.vstack([exploitation_points, exploration_points]), exploration_ratio

def main():
    """
    ä¸»å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ª"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"æµç¨‹ã€‚
    """
    # --- 1. åˆå§‹åŒ– ---
    # !! æ³¨æ„: DOMAIN_BOUNDS ç°åœ¨ä»…ç”¨äºå¯è§†åŒ–æˆ–é‡‡æ ·å™¨ï¼Œå®é™…ç‰©ç†è¾¹ç•Œç”±åŠ è½½çš„æ•°æ®å†³å®š !!
    DOMAIN_BOUNDS = np.array([[0., 0., 0.], [1., 1., 1.]]) 
    TOTAL_EPOCHS = 8000
    ADAPTIVE_CYCLE_EPOCHS = 2000  # æ¯å¤šå°‘ä¸ªepochæ‰§è¡Œä¸€æ¬¡è‡ªé€‚åº”è°ƒæ•´
    DETECT_EPOCHS = 500 # æ¯100è½®æ£€æµ‹ä¸€æ¬¡æ€§èƒ½ [ä¿®æ­£æ³¨é‡Š]
    DATA_SPLIT_RATIOS = [0.5] + [0.1]*5

    # --- [æ–°å¢] åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥ ---
    # ğŸ¯ æ ¹æ®æ˜¯å¦å¯ç”¨å…‹é‡Œé‡‘é‡é‡‡æ ·é€‰æ‹©ä¸åŒçš„æŸå¤±æƒé‡ç­–ç•¥
    if ENABLE_KRIGING:
        # å¯ç”¨å…‹é‡Œé‡‘æ—¶ï¼Œä½¿ç”¨ä¸¤é˜¶æ®µåŠ¨æ€ç­–ç•¥
        INITIAL_LOSS_RATIO = DYNAMIC_LOSS_RATIO_START  # å‰åŠæ®µï¼šé«˜æ•°æ®æƒé‡ï¼Œä¸“æ³¨å­¦ä¹ æ•°æ®
        FINAL_LOSS_RATIO = DYNAMIC_LOSS_RATIO_END      # ååŠæ®µï¼šé™ä½æ•°æ®æƒé‡ï¼Œåˆ©äºå…‹é‡Œé‡‘
        USE_DYNAMIC_STRATEGY = True
        strategy_desc = f"åŠ¨æ€ç­–ç•¥: {INITIAL_LOSS_RATIO:.1f} â†’ {FINAL_LOSS_RATIO:.1f}"
    else:
        # ä¸å¯ç”¨å…‹é‡Œé‡‘æ—¶ï¼Œä½¿ç”¨å›ºå®šæŸå¤±æƒé‡
        INITIAL_LOSS_RATIO = FIXED_LOSS_RATIO
        FINAL_LOSS_RATIO = FIXED_LOSS_RATIO  
        USE_DYNAMIC_STRATEGY = False
        strategy_desc = f"å›ºå®šç­–ç•¥: {FIXED_LOSS_RATIO:.1f}"

    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œï¼šè‡ªé€‚åº”PINNè®­ç»ƒå®éªŒ")
    print("="*60)
    print(f"ğŸ“‹ å®éªŒé…ç½®:")
    print(f"   - æŸå¤±æƒé‡ç­–ç•¥: {strategy_desc}")
    if USE_DYNAMIC_STRATEGY:
        print(f"     â”œâ”€ å‰3/4æ®µè®­ç»ƒ: æ•°æ®æƒé‡/ç‰©ç†æƒé‡ = {INITIAL_LOSS_RATIO:.1f} (ä¸“æ³¨æ•°æ®å­¦ä¹ )")
        print(f"     â”œâ”€ å1/4æ®µè®­ç»ƒ: {INITIAL_LOSS_RATIO:.1f} â†’ {FINAL_LOSS_RATIO:.1f} (æŒ‡æ•°è¡°å‡ï¼Œç‰©ç†ä¼˜åŒ–)")
        print(f"     â”œâ”€ è¡°å‡ç­–ç•¥: å¿«é€Ÿé™ä½æ•°æ®æƒé‡ï¼Œè®©ç‰©ç†çº¦æŸå……åˆ†å‘æŒ¥ä½œç”¨")
        print(f"     â””â”€ å…‹é‡Œé‡‘ç­–ç•¥: ä»…å1/4æ®µå¯ç”¨ï¼Œåˆ©ç”¨ç‰©ç†ä¸»å¯¼ä¸‹çš„å¹³æ»‘æ®‹å·®åˆ†å¸ƒ")
    else:
        print(f"     â””â”€ å›ºå®šæŸå¤±æƒé‡: [ç‰©ç†={1.0:.1f}, æ•°æ®={INITIAL_LOSS_RATIO:.1f}]")
    print(f"   - å…‹é‡Œé‡‘å¼•å¯¼é‡‡æ ·: {'âœ… å¯ç”¨' if ENABLE_KRIGING else 'âŒ ç¦ç”¨'}")
    if ENABLE_KRIGING:
        print(f"     â””â”€ æ¢ç´¢ç‡ç­–ç•¥: {INITIAL_EXPLORATION_RATIO:.1%} â†’ {FINAL_EXPLORATION_RATIO:.1%} (æ¯å‘¨æœŸ-{EXPLORATION_DECAY_RATE:.1%})")
    print(f"   - æ•°æ®æ³¨å…¥ç­–ç•¥: {'âœ… å¯ç”¨' if ENABLE_DATA_INJECTION else 'âŒ ç¦ç”¨'}")
    print(f"   - å¿«é€Ÿæ”¹å–„æ—©åœ: {'âœ… å¯ç”¨' if ENABLE_RAPID_IMPROVEMENT_EARLY_STOP else 'âŒ ç¦ç”¨'}")
    print(f"   - æ€»è®­ç»ƒè½®æ•°: {TOTAL_EPOCHS}")
    print(f"   - å¹²é¢„å‘¨æœŸ: æ¯ {ADAPTIVE_CYCLE_EPOCHS} è½®")
    
    # ç¡®å®šå®éªŒç±»å‹
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        exp_type = "å®Œæ•´è‡ªé€‚åº”PINN (æ•°æ®æ³¨å…¥ + å…‹é‡Œé‡‘é‡é‡‡æ ·)"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        exp_type = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·ç­–ç•¥"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        exp_type = "ä»…æ•°æ®æ³¨å…¥ç­–ç•¥"
    else:
        exp_type = "ä»…å‘¨æœŸæ€§é‡å¯ (æ— è‡ªé€‚åº”ç­–ç•¥)"
    
    print(f"   - å®éªŒç±»å‹: {exp_type}")
    print("="*60 + "\n")
    
    # --- æ•°æ®åŠ è½½å‚æ•° ---
    DATA_PATH = "PINN/DATA.xlsx"
    SPACE_DIMS = np.array([20.0, 10.0, 10.0])
    NUM_SAMPLES = 100
    
    # --- æ¨¡å‹è®­ç»ƒå‚æ•° ---
    NUM_COLLOCATION_POINTS = 4096
    NUM_RESIDUAL_SCOUT_POINTS = 5000 # ç”¨äºä¾¦å¯Ÿçš„ç‚¹æ•°ï¼Œè¿œå°‘äºè®­ç»ƒé…ç½®ç‚¹æ•°

    # 1. æ•°æ®åŠ è½½
    data_loader = DummyDataLoader(
        data_path=DATA_PATH,
        space_dims=SPACE_DIMS,
        num_samples=NUM_SAMPLES
    )
    main_train_set, reserve_data_pools, test_data, dose_data = data_loader.get_training_data(
        split_ratios=DATA_SPLIT_RATIOS,
        test_set_size=300  # [æ–°å¢] ç‹¬ç«‹æµ‹è¯•é›†å¤§å°
    )

    # 2. æ¨¡å‹å’Œé‡‡æ ·å™¨åˆå§‹åŒ–
    world_min = dose_data['world_min']
    world_max = dose_data['world_max']
    
    # ä½¿ç”¨åˆå§‹æŸå¤±æƒé‡æ¯”å€¼åˆå§‹åŒ–PINNæ¨¡å‹
    pinn = PINNModel(
        dose_data=dose_data, 
        training_data=main_train_set, # [æ–°] åªç”¨ä¸»è®­ç»ƒé›†åˆå§‹åŒ–
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS,
        loss_ratio=INITIAL_LOSS_RATIO  # [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€ç­–ç•¥çš„åˆå§‹æ¯”å€¼
    )
    kriging = GPUKriging()
    sampler = AdaptiveSampler(domain_bounds=np.vstack([world_min, world_max]))
    
    # åˆå§‹é…ç½®ç‚¹ï¼šåœ¨çœŸå®ç‰©ç†ç©ºé—´å†…é‡‡æ ·
    current_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                  (world_max - world_min) + world_min)

    # --- 3. [ä¿®æ­£] è®­ç»ƒå¾ªç¯ ---
    # ä½¿ç”¨ while å¾ªç¯æ¥ç¡®ä¿æ€»è®­ç»ƒè½®æ•°è¾¾æ ‡
    total_epochs_trained = 0
    cycle_counter = 0  # ğŸ”§ æ–°å¢ï¼šå‘¨æœŸè®¡æ•°å™¨ï¼Œç”¨äºå¯é çš„å…‹é‡Œé‡‘è§¦å‘
    
    # [æ–°å¢] é‡è¦äº‹ä»¶è®°å½•åˆ—è¡¨ï¼Œç”¨äºå›¾è¡¨æ ‡æ³¨
    important_events = []  # æ ¼å¼: [(epoch, event_type, description), ...]
    
    # [æ–°å¢] åŠ¨æ€ç­–ç•¥ç›¸å…³å˜é‡
    training_phase_transition_point = TOTAL_EPOCHS * 3 // 4  # è®­ç»ƒé˜¶æ®µè½¬æ¢ç‚¹ï¼ˆ3/4å¤„ï¼‰
    kriging_enabled_this_cycle = False  # å½“å‰å‘¨æœŸæ˜¯å¦å¯ç”¨å…‹é‡Œé‡‘
    current_loss_ratio = INITIAL_LOSS_RATIO  # å½“å‰ä½¿ç”¨çš„æŸå¤±æƒé‡æ¯”å€¼
    
    print(f"\nğŸ¯ åŠ¨æ€è®­ç»ƒç­–ç•¥:")
    if USE_DYNAMIC_STRATEGY:
        print(f"   ğŸ“š ç¬¬ä¸€é˜¶æ®µ (0-{training_phase_transition_point}è½®): æ•°æ®å­¦ä¹ æœŸ")
        print(f"      â”œâ”€ æŸå¤±æƒé‡æ¯”å€¼: {INITIAL_LOSS_RATIO:.1f} (æ•°æ®ä¸»å¯¼ï¼Œå……åˆ†å­¦ä¹ è§‚æµ‹æ•°æ®)")
        print(f"      â””â”€ å…‹é‡Œé‡‘é‡é‡‡æ ·: âŒ ç¦ç”¨ (é«˜æ•°æ®æƒé‡å¯¼è‡´æ®‹å·®åˆ†å¸ƒä¸å‡ï¼Œä¸åˆ©äºå…‹é‡Œé‡‘)")
        print(f"   ğŸ”¬ ç¬¬äºŒé˜¶æ®µ ({training_phase_transition_point}-{TOTAL_EPOCHS}è½®): ç‰©ç†çº¦æŸä¼˜åŒ–æœŸ")
        print(f"      â”œâ”€ æŸå¤±æƒé‡æ¯”å€¼: {INITIAL_LOSS_RATIO:.1f} â†’ {FINAL_LOSS_RATIO:.1f} (æŒ‡æ•°è¡°å‡)")
        print(f"      â”œâ”€ è¡°å‡ç‰¹ç‚¹: å¿«é€Ÿé™ä½æ•°æ®æƒé‡ï¼Œå¼ºåŒ–ç‰©ç†çº¦æŸ")
        print(f"      â””â”€ å…‹é‡Œé‡‘é‡é‡‡æ ·: âœ… å¯ç”¨ (ç‰©ç†ä¸»å¯¼äº§ç”Ÿå¹³æ»‘æ®‹å·®åˆ†å¸ƒï¼Œåˆ©äºå…‹é‡Œé‡‘)")
    else:
        print(f"   ğŸ“Š å›ºå®šç­–ç•¥: æŸå¤±æƒé‡æ¯”å€¼ä¿æŒ {INITIAL_LOSS_RATIO:.1f}")
    print()

    while total_epochs_trained < TOTAL_EPOCHS:
        remaining_total_epochs = TOTAL_EPOCHS - total_epochs_trained
        
        # æœ¬æ¬¡è‡ªé€‚åº”å‘¨æœŸçš„æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œä¸èƒ½è¶…è¿‡æ€»å‰©ä½™è½®æ•°
        cycle_max_epochs = min(ADAPTIVE_CYCLE_EPOCHS, remaining_total_epochs)
        
        print(f"\n--- ä¸»å¾ªç¯å‘¨æœŸ: ç›®æ ‡è®­ç»ƒ {total_epochs_trained} -> {total_epochs_trained + cycle_max_epochs} ---")
        
        # [æ–°å¢] åŠ¨æ€ç­–ç•¥: æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æŸå¤±æƒé‡æˆ–å…‹é‡Œé‡‘çŠ¶æ€
        if USE_DYNAMIC_STRATEGY:
            # è®¡ç®—å½“å‰è®­ç»ƒè¿›åº¦æ¯”ä¾‹
            current_progress = total_epochs_trained / TOTAL_EPOCHS
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¿›å…¥ç¬¬äºŒé˜¶æ®µï¼ˆå…‹é‡Œé‡‘å¯ç”¨é˜¶æ®µï¼‰
            is_in_kriging_phase = total_epochs_trained >= training_phase_transition_point
            
            if is_in_kriging_phase:
                # ç¬¬äºŒé˜¶æ®µï¼šå¯ç”¨å…‹é‡Œé‡‘ï¼ˆç‰©ç†ä¸»å¯¼äº§ç”Ÿå¹³æ»‘æ®‹å·®åˆ†å¸ƒï¼Œåˆ©äºå…‹é‡Œé‡‘ï¼‰
                kriging_enabled_this_cycle = True
            else:
                # ç¬¬ä¸€é˜¶æ®µï¼šç¦ç”¨å…‹é‡Œé‡‘ï¼ˆæ•°æ®æƒé‡é«˜å¯¼è‡´æ®‹å·®åˆ†å¸ƒä¸å‡åŒ€ï¼Œä¸åˆ©äºå…‹é‡Œé‡‘ï¼‰
                kriging_enabled_this_cycle = False
            
            if is_in_kriging_phase:
                # ç¬¬äºŒé˜¶æ®µï¼šæŒ‡æ•°è¡°å‡æŸå¤±æƒé‡æ¯”å€¼ï¼ˆä»æ•°æ®ä¸»å¯¼è½¬å‘ç‰©ç†ä¸»å¯¼ï¼‰
                progress_in_second_phase = (total_epochs_trained - training_phase_transition_point) / (TOTAL_EPOCHS - training_phase_transition_point)
                
                # ä½¿ç”¨æŒ‡æ•°è¡°å‡ç­–ç•¥ï¼Œå¿«é€Ÿé™ä½æ•°æ®æƒé‡ï¼Œè®©ç‰©ç†çº¦æŸå‘æŒ¥ä½œç”¨
                # å…¬å¼ï¼šratio = start * ((end/start)^(progress^2))
                # progress^2 è®©å‰æœŸå¿«é€Ÿä¸‹é™ï¼Œç¡®ä¿ç‰©ç†æƒé‡ä¸»å¯¼æœŸæœ‰è¶³å¤Ÿè®­ç»ƒæ—¶é—´
                decay_factor = (FINAL_LOSS_RATIO / INITIAL_LOSS_RATIO) ** (progress_in_second_phase ** 2)
                new_loss_ratio = INITIAL_LOSS_RATIO * decay_factor
                
                # ç¡®ä¿ä¸ä½äºæœ€ç»ˆç›®æ ‡å€¼
                new_loss_ratio = max(new_loss_ratio, FINAL_LOSS_RATIO)
                
                # é˜¶æ®µè½¬æ¢æç¤ºï¼ˆä»…åœ¨åˆšè¿›å…¥ç¬¬äºŒé˜¶æ®µæ—¶æ˜¾ç¤ºï¼‰
                if total_epochs_trained == training_phase_transition_point or (total_epochs_trained < training_phase_transition_point + ADAPTIVE_CYCLE_EPOCHS and cycle_counter <= 1):
                    print(f"\nğŸ”„ è¿›å…¥ç¬¬äºŒé˜¶æ®µ: ç‰©ç†çº¦æŸä¼˜åŒ–æœŸ")
                    print(f"   ğŸ“‰ å¼€å§‹é™ä½æ•°æ®æƒé‡ï¼Œå¼ºåŒ–ç‰©ç†çº¦æŸ")
                    print(f"   âœ… å…‹é‡Œé‡‘é‡é‡‡æ ·å·²å¯ç”¨ï¼ˆåˆ©ç”¨ç‰©ç†ä¸»å¯¼ä¸‹çš„å¹³æ»‘æ®‹å·®åˆ†å¸ƒï¼‰")
                    important_events.append((
                        total_epochs_trained, 
                        'phase_transition', 
                        'è¿›å…¥ç‰©ç†çº¦æŸæœŸï¼Œå¯ç”¨å…‹é‡Œé‡‘'
                    ))
            else:
                # ç¬¬ä¸€é˜¶æ®µï¼šå›ºå®šæ•°æ®ä¸»å¯¼æƒé‡
                new_loss_ratio = INITIAL_LOSS_RATIO
            
            # æ›´æ–°æŸå¤±æƒé‡ï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
            if abs(current_loss_ratio - new_loss_ratio) > 1e-6:
                pinn.update_loss_ratio(new_loss_ratio)
                current_loss_ratio = new_loss_ratio
                
                # è®°å½•æŸå¤±æƒé‡å˜åŒ–äº‹ä»¶
                important_events.append((
                    total_epochs_trained, 
                    'loss_ratio_update', 
                    f'æŸå¤±æƒé‡æ¯”å€¼æ›´æ–°è‡³ {new_loss_ratio:.2f}'
                ))
            
            print(f"ğŸ“Š å½“å‰è®­ç»ƒçŠ¶æ€:")
            print(f"   - è®­ç»ƒè¿›åº¦: {current_progress:.1%} ({total_epochs_trained}/{TOTAL_EPOCHS})")
            print(f"   - è®­ç»ƒé˜¶æ®µ: {'ç¬¬äºŒé˜¶æ®µ(ç‰©ç†çº¦æŸä¼˜åŒ–)' if is_in_kriging_phase else 'ç¬¬ä¸€é˜¶æ®µ(æ•°æ®å­¦ä¹ )'}")
            print(f"   - æŸå¤±æƒé‡æ¯”å€¼: {current_loss_ratio:.2f}")
            if is_in_kriging_phase:
                phase_progress = (total_epochs_trained - training_phase_transition_point) / (TOTAL_EPOCHS - training_phase_transition_point)
                print(f"   - ç¬¬äºŒé˜¶æ®µè¿›åº¦: {phase_progress:.1%}")
                if current_loss_ratio < 1.0:
                    print(f"   - ğŸ’¡ å½“å‰ç‰©ç†æƒé‡ä¸»å¯¼ (æ¯”å€¼ < 1.0)ï¼Œæœ‰åˆ©äºå…‹é‡Œé‡‘ä¼˜åŒ–")
                elif current_loss_ratio < 3.0:
                    print(f"   - ğŸ“Š å½“å‰æ¥è¿‘å¹³è¡¡çŠ¶æ€ï¼Œæ­£åœ¨å‘ç‰©ç†ä¸»å¯¼è½¬å˜")
                else:
                    print(f"   - ğŸ“ˆ å½“å‰æ•°æ®æƒé‡ä»å ä¸»å¯¼ï¼Œæ­£åœ¨é€æ­¥é™ä½")
            print(f"   - å…‹é‡Œé‡‘é‡é‡‡æ ·: {'âœ… å¯ç”¨' if kriging_enabled_this_cycle else 'âŒ ç¦ç”¨'}")
        else:
            # å›ºå®šç­–ç•¥ï¼šå…‹é‡Œé‡‘çŠ¶æ€ç”±å…¨å±€å¼€å…³å†³å®š
            kriging_enabled_this_cycle = ENABLE_KRIGING

        # 2a. ä½¿ç”¨å½“å‰çš„é…ç½®ç‚¹ï¼Œå¯¹PINNè¿›è¡Œä¸€è½®å¸¸è§„è®­ç»ƒ
        print(f"PHASE 2a: å¸¸è§„PINNè®­ç»ƒ (æœ¬å‘¨æœŸä¸Šé™: {cycle_max_epochs} epochs)...")
        
        # è®°å½•è¿›å…¥æ­¤å‘¨æœŸå‰çš„è®­ç»ƒæ­¥æ•°
        epochs_before_cycle = pinn.model.train_state.step or 0
        
        # è°ƒç”¨æ”¹é€ åçš„æ–¹æ³•ï¼Œå¹¶æ¥æ”¶å…¶è¿”å›ç»“æœ
        cycle_result = pinn.run_training_cycle(
            max_epochs=cycle_max_epochs,
            detect_every=DETECT_EPOCHS,
            collocation_points=current_collocation_points,
            detection_threshold=0.1
        )
        
        # è®¡ç®—æœ¬å‘¨æœŸå®é™…è®­ç»ƒäº†å¤šå°‘è½®
        epochs_this_cycle = (pinn.model.train_state.step or 0) - epochs_before_cycle
        total_epochs_trained += epochs_this_cycle
        cycle_counter += 1  # ğŸ”§ å¢åŠ å‘¨æœŸè®¡æ•°

        print(f"\nINFO: æœ¬å‘¨æœŸå®é™…è®­ç»ƒ {epochs_this_cycle} è½®. æ€»è®­ç»ƒè¿›åº¦: {total_epochs_trained}/{TOTAL_EPOCHS}")
        print(f"ğŸ”¢ å‘¨æœŸè®¡æ•°: ç¬¬ {cycle_counter} ä¸ªå‘¨æœŸå®Œæˆ")
        
        # ğŸ” æ–°å¢ï¼šæ€§èƒ½åˆ†æ - è®°å½•å½“å‰å‘¨æœŸçš„æœ€ç»ˆMRE
        current_mre = pinn.model.train_state.metrics_test[-1] if pinn.model.train_state.metrics_test else float('inf')
        print(f"ğŸ“Š å‘¨æœŸæ€§èƒ½: ç¬¬{cycle_counter}å‘¨æœŸç»“æŸæ—¶MRE = {current_mre:.6f} (è®­ç»ƒ{epochs_this_cycle}è½®)")
        
        # ğŸ” å¦‚æœæ˜¯ç¬¬2+å‘¨æœŸï¼Œè®¡ç®—æ”¹å–„ç‡
        if cycle_counter > 1 and hasattr(main, 'previous_cycle_mre'):
            improvement = main.previous_cycle_mre - current_mre
            improvement_rate = improvement / main.previous_cycle_mre if main.previous_cycle_mre > 0 else 0
            print(f"    â””â”€ ç›¸æ¯”ä¸Šå‘¨æœŸæ”¹å–„: {improvement:.6f} ({improvement_rate:.2%})")
            
            # è¯„ä¼°æ”¶æ•›é€Ÿåº¦
            if improvement_rate > 0.1:
                print(f"    ğŸš€ å¿«é€Ÿæ”¶æ•›! æ”¹å–„ç‡ > 10%")
            elif improvement_rate > 0.05:
                print(f"    ğŸ“ˆ è‰¯å¥½æ”¶æ•›! æ”¹å–„ç‡ > 5%")
            elif improvement_rate > 0:
                print(f"    ğŸ“Š ç¼“æ…¢æ”¹å–„")
            else:
                print(f"    âš ï¸  æ€§èƒ½ä¸‹é™æˆ–åœæ»")
        
        # ä¿å­˜å½“å‰MREä¾›ä¸‹ä¸€å‘¨æœŸæ¯”è¾ƒ
        if not hasattr(main, 'previous_cycle_mre'):
            main.previous_cycle_mre = current_mre
        else:
            main.previous_cycle_mre = current_mre

        # å¦‚æœå·²ç»è®­ç»ƒå¤Ÿäº†ï¼Œå°±æå‰ç»“æŸä¸»å¾ªç¯
        if total_epochs_trained >= TOTAL_EPOCHS:
            print("\nINFO: æ€»è®­ç»ƒè½®æ•°å·²è¾¾åˆ°ç›®æ ‡ï¼Œç»“æŸè‡ªé€‚åº”è®­ç»ƒã€‚")
            break

        # 2. ğŸ”§ æ”¹ç”¨å‘¨æœŸè®¡æ•°å™¨è¿›è¡Œå¯é çš„å‘¨æœŸæ€§å¹²é¢„è§¦å‘
        should_trigger_intervention = cycle_counter > 0  # æ¯ä¸ªå‘¨æœŸéƒ½æ£€æŸ¥æ˜¯å¦éœ€è¦å¹²é¢„
        print(f"ğŸ” å¹²é¢„è§¦å‘æ£€æŸ¥: å‘¨æœŸ {cycle_counter} å®Œæˆï¼Œåº”è¯¥è§¦å‘å¹²é¢„ â†’ {should_trigger_intervention}")
        
        if should_trigger_intervention:
            print("\n" + "!"*60)
            
            # æ ¹æ®å¯ç”¨çš„ç­–ç•¥ç¡®å®šå¹²é¢„ç±»å‹æè¿°
            intervention_types = []
            if ENABLE_DATA_INJECTION:
                intervention_types.append("æ•°æ®æ³¨å…¥")
            if ENABLE_KRIGING:
                intervention_types.append("å…‹é‡Œé‡‘é‡é‡‡æ ·")
            
            if intervention_types:
                intervention_desc = " + ".join(intervention_types)
                print(f"!! è®­ç»ƒè¾¾åˆ° {total_epochs_trained} è½®ï¼Œè§¦å‘å‘¨æœŸæ€§å¹²é¢„: {intervention_desc} !!")
            else:
                print(f"!! è®­ç»ƒè¾¾åˆ° {total_epochs_trained} è½®ï¼Œè§¦å‘å‘¨æœŸæ€§é‡å¯ (æ— è‡ªé€‚åº”ç­–ç•¥) !!")
            print("!"*60)

            # --- å¹²é¢„æªæ–½ 1: æ³¨å…¥æ–°æ•°æ® (å¦‚æœå¯ç”¨ä¸”è¿˜æœ‰æ•°æ®) ---
            if ENABLE_DATA_INJECTION:
                if reserve_data_pools:
                    print("\nPHASE A: æ³¨å…¥æ–°çš„å‚¨å¤‡è®­ç»ƒæ•°æ®...")
                    data_injection_epoch = pinn.model.train_state.step or 0
                    data_to_inject = reserve_data_pools.pop(0)
                    pinn.inject_new_data(data_to_inject)
                    print("PHASE A: âœ… æ–°æ•°æ®æ³¨å…¥å®Œæˆã€‚")
                    
                    # [æ–°å¢] è®°å½•æ•°æ®æ³¨å…¥äº‹ä»¶  
                    important_events.append((
                        data_injection_epoch, 
                        'data_injection', 
                        f'å‘¨æœŸæ€§æ•°æ®æ³¨å…¥ (+{len(data_to_inject)}ç‚¹, ç¬¬{cycle_counter}æ¬¡)'
                    ))
                else:
                    print("\nWARNING: æ•°æ®æ³¨å…¥å·²å¯ç”¨ï¼Œä½†å·²æ— æ›´å¤šå‚¨å¤‡æ•°æ®å¯æ³¨å…¥ã€‚")
            else:
                print("\nPHASE A: æ•°æ®æ³¨å…¥å·²ç¦ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µã€‚")

            # --- å¹²é¢„æªæ–½ 2: å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ · (åŠ¨æ€ç­–ç•¥æ§åˆ¶) ---
            if kriging_enabled_this_cycle:  # [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€ç­–ç•¥æ§åˆ¶çš„å…‹é‡Œé‡‘å¯ç”¨çŠ¶æ€
                print("\nPHASE B: å¼€å§‹å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ ·...")
                
                # æ®‹å·®"ä¾¦å¯Ÿ"
                scout_points = (np.random.rand(NUM_RESIDUAL_SCOUT_POINTS, 3) *
                                (world_max - world_min) + world_min)
                true_residuals = pinn.compute_pde_residual(scout_points)
                print(f"    - çœŸå®PDEæ®‹å·®ç»Ÿè®¡ (åœ¨ {len(scout_points)} ä¸ªä¾¦å¯Ÿç‚¹ä¸Š):")
                print(f"      - Max={np.max(true_residuals):.4e}, "
                      f"Min={np.min(true_residuals):.4e}, "
                      f"Mean={np.mean(true_residuals):.4e}, "
                      f"Std={np.std(true_residuals):.4e}")
                
                # ğŸ” æ®‹å·®è´¨é‡åˆ†æ
                high_residual_ratio = np.mean(true_residuals > np.mean(true_residuals) * 2)
                print(f"      - é«˜æ®‹å·®ç‚¹æ¯”ä¾‹: {high_residual_ratio:.1%} (æ®‹å·®>2å€å‡å€¼)")
                
                # ğŸ” æ–°å¢ï¼šæ®‹å·®åˆ†å¸ƒè´¨é‡è¯Šæ–­ (ç”¨äºè¯„ä¼°loss_ratioæ•ˆæœ)
                residual_std = np.std(true_residuals)
                residual_cv = residual_std / (np.mean(true_residuals) + 1e-10)  # å˜å¼‚ç³»æ•°
                print(f"      - æ®‹å·®å˜å¼‚ç³»æ•°: {residual_cv:.3f} (æ ‡å‡†å·®/å‡å€¼ï¼Œè¶Šå°è¶Šåˆ©äºå…‹é‡Œé‡‘)")
                print(f"      - å½“å‰loss_ratio: {current_loss_ratio:.1f} çš„æ®‹å·®åˆ†å¸ƒç‰¹å¾åˆ†æå®Œæˆ")
                
                # ğŸ’¡ æ®‹å·®è´¨é‡è¯„ä»·
                if residual_cv < 0.5:
                    print(f"      - âœ… æ®‹å·®åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼Œæœ‰åˆ©äºå…‹é‡Œé‡‘æ’å€¼")
                elif residual_cv < 1.0:
                    print(f"      - âš ï¸  æ®‹å·®åˆ†å¸ƒä¸­ç­‰å˜å¼‚ï¼Œå…‹é‡Œé‡‘æ•ˆæœå¯èƒ½ä¸€èˆ¬")
                else:
                    print(f"      - âŒ æ®‹å·®åˆ†å¸ƒå˜å¼‚è¿‡å¤§ï¼Œå»ºè®®è¿›ä¸€æ­¥é™ä½loss_ratioæ”¹å–„ç‰©ç†çº¦æŸ")
                
                # å…‹é‡Œé‡‘ä»£ç†å»ºæ¨¡
                print("    ğŸ”§ å¼€å§‹è®­ç»ƒå…‹é‡Œé‡‘ä»£ç†æ¨¡å‹...")
                kriging.fit(scout_points, true_residuals)

                # è‡ªé€‚åº”é‡‡æ ·
                kriging_epoch = pinn.model.train_state.step or 0
                num_collocation_to_generate = pinn.data.num_domain
                print(f"INFO: Dynamically calculated {num_collocation_to_generate} collocation points to generate.")

                # ğŸ”§ ä½¿ç”¨å‘¨æœŸè®¡æ•°å™¨ä½œä¸ºå‘¨æœŸç¼–å·
                current_collocation_points, used_exploration_ratio = sampler.generate_new_collocation_points(
                    kriging_model=kriging,
                    num_points_to_sample=num_collocation_to_generate,
                    cycle_number=cycle_counter
                )
                print("PHASE B: âœ… æ–°çš„è‡ªé€‚åº”é…ç½®ç‚¹å·²ç”Ÿæˆã€‚")
                
                # ğŸ” æ–°å¢ï¼šè¯„ä¼°æ–°é…ç½®ç‚¹çš„é¢„æœŸæ®‹å·®è´¨é‡
                predicted_residuals_new = kriging.predict(current_collocation_points)
                old_residuals_sample = pinn.compute_pde_residual(current_collocation_points[:100])  # é‡‡æ ·100ä¸ªç‚¹è¯„ä¼°
                print(f"    ğŸ“Š æ–°é…ç½®ç‚¹è´¨é‡è¯„ä¼°:")
                print(f"      - å…‹é‡Œé‡‘é¢„æµ‹æ®‹å·®: Mean={np.mean(predicted_residuals_new):.4e}, Max={np.max(predicted_residuals_new):.4e}")
                print(f"      - å®é™…æ®‹å·®(é‡‡æ ·): Mean={np.mean(old_residuals_sample):.4e}, Max={np.max(old_residuals_sample):.4e}")
                residual_prediction_accuracy = np.corrcoef(
                    predicted_residuals_new[:100], old_residuals_sample
                )[0,1] if len(old_residuals_sample) == 100 else 0
                print(f"      - å…‹é‡Œé‡‘é¢„æµ‹å‡†ç¡®åº¦: {residual_prediction_accuracy:.3f} (ç›¸å…³ç³»æ•°)")
                
                # [æ–°å¢] è®°å½•å‘¨æœŸæ€§å…‹é‡Œé‡‘åº”ç”¨äº‹ä»¶
                important_events.append((
                    kriging_epoch, 
                    'kriging_resampling', 
                    f'å‘¨æœŸæ€§å…‹é‡Œé‡‘é‡é‡‡æ · (ç¬¬{cycle_counter}æ¬¡, æ¢ç´¢ç‡:{used_exploration_ratio:.1%})'
                ))
            else:
                # [ä¿®æ”¹] æ ¹æ®åŠ¨æ€ç­–ç•¥çŠ¶æ€ç»™å‡ºä¸åŒçš„æç¤ºä¿¡æ¯
                if USE_DYNAMIC_STRATEGY and total_epochs_trained < training_phase_transition_point:
                    print("\nPHASE B: å…‹é‡Œé‡‘é‡é‡‡æ ·åœ¨ç¬¬ä¸€é˜¶æ®µç¦ç”¨ï¼Œä¸“æ³¨æ•°æ®å­¦ä¹ ã€‚")
                    print("INFO: é…ç½®ç‚¹ä¿æŒä¸å˜ï¼Œå½“å‰å¤„äºæ•°æ®å­¦ä¹ æœŸã€‚")
                elif not ENABLE_KRIGING:
                    print("\nPHASE B: å…‹é‡Œé‡‘é‡é‡‡æ ·å·²åœ¨å…¨å±€é…ç½®ä¸­ç¦ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µã€‚")
                    print("INFO: é…ç½®ç‚¹ä¿æŒä¸å˜ï¼Œä»…ä¾é æ•°æ®æ³¨å…¥ç­–ç•¥ã€‚")
                else:
                    print("\nPHASE B: å…‹é‡Œé‡‘é‡é‡‡æ ·æœ¬å‘¨æœŸæœªå¯ç”¨ã€‚")
                    print("INFO: é…ç½®ç‚¹ä¿æŒä¸å˜ã€‚")

            # --- å‘¨æœŸæ€§å¹²é¢„å®Œæˆ ---
            # ç¡®å®šå¹²é¢„ç±»å‹æè¿°ï¼ˆè€ƒè™‘åŠ¨æ€ç­–ç•¥ï¼‰
            intervention_parts = []
            if ENABLE_DATA_INJECTION:
                intervention_parts.append("æ•°æ®æ³¨å…¥")
            if kriging_enabled_this_cycle:
                intervention_parts.append("å…‹é‡Œé‡‘é‡é‡‡æ ·")
            
            if intervention_parts:
                intervention_desc = " + ".join(intervention_parts)
                if USE_DYNAMIC_STRATEGY:
                    phase_desc = "ç¬¬äºŒé˜¶æ®µ" if total_epochs_trained >= training_phase_transition_point else "ç¬¬ä¸€é˜¶æ®µ"
                    intervention_desc += f" ({phase_desc})"
            else:
                intervention_desc = "å‘¨æœŸæ€§é‡å¯"
                if USE_DYNAMIC_STRATEGY:
                    intervention_desc += " (ç¬¬ä¸€é˜¶æ®µ)"
            
            print(f"\nINFO: ç¬¬ {cycle_counter} æ¬¡å‘¨æœŸæ€§å¹²é¢„å®Œæˆ ({intervention_desc})ã€‚")
            print("-" * 60)
        
        else:
            # ğŸ”§ è¿™ä¸ªåˆ†æ”¯ç°åœ¨åº”è¯¥ä¸ä¼šè¢«æ‰§è¡Œï¼Œå› ä¸ºæ¯ä¸ªå‘¨æœŸéƒ½ä¼šè§¦å‘å¹²é¢„æ£€æŸ¥
            print(f"\nâš ï¸  æœªé¢„æœŸçš„æƒ…å†µï¼šå‘¨æœŸ {cycle_counter} ä¸åº”è¯¥è·³è¿‡å¹²é¢„æ£€æŸ¥ï¼")
            pass

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

    # --- 4. è®­ç»ƒåŸå§‹PINNä½œä¸ºå¯¹æ¯”æ¨¡å‹ ---
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒåŸå§‹PINNä½œä¸ºå¯¹æ¯”åŸºçº¿")
    print("="*60 + "\n")

    # [ä¿®æ­£] ä¸ºåŸºçº¿æ¨¡å‹å‡†å¤‡ä¸è‡ªé€‚åº”PINNç›¸åŒçš„è®­ç»ƒæ•°æ®
    # è·å–è‡ªé€‚åº”PINNå®é™…ä½¿ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”
    adaptive_training_data = pinn.data.bcs[0].points  # è‡ªé€‚åº”PINNçš„å®é™…è®­ç»ƒç‚¹åæ ‡
    adaptive_training_values = pinn.data.bcs[0].values.cpu().numpy()  # å¯¹åº”çš„å€¼ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    
    # å°†å¯¹æ•°å°ºåº¦çš„å€¼è½¬æ¢å›çº¿æ€§å°ºåº¦ï¼Œç„¶ååˆå¹¶ä¸º [x,y,z,value] æ ¼å¼
    adaptive_training_linear_values = np.exp(adaptive_training_values)
    full_training_data = np.hstack([adaptive_training_data, adaptive_training_linear_values])
    
    print(f"INFO: åŸºçº¿PINNè®¾ç½®:")
    print(f"   - è®­ç»ƒç‚¹æ•°: {len(full_training_data)} (ä¸è‡ªé€‚åº”PINNæœ€ç»ˆä½¿ç”¨çš„è®­ç»ƒç‚¹ç›¸åŒ)")
    print(f"   - æŸå¤±æƒé‡ç­–ç•¥: å›ºå®šç­–ç•¥ (loss_ratio = {FIXED_LOSS_RATIO:.1f})")
    print(f"   - é…ç½®ç‚¹é‡‡æ ·: å›ºå®šéšæœºé‡‡æ ·")
    print(f"   - å¯¹æ¯”ç›®çš„: éªŒè¯åŠ¨æ€ç­–ç•¥ç›¸å¯¹äºå›ºå®šç­–ç•¥çš„ä¼˜åŠ¿")

    pinn_baseline = PINNModel(
        dose_data=dose_data, 
        training_data=full_training_data, # [æ–°] ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS,
        loss_ratio=FIXED_LOSS_RATIO  # [ä¿®æ­£] åŸºçº¿PINNä½¿ç”¨å›ºå®šç­–ç•¥ç¡®ä¿å…¬å¹³å¯¹æ¯”
    )
    # ä¸ºåŸå§‹PINNç”Ÿæˆä¸€æ¬¡æ€§çš„ã€å›ºå®šçš„é…ç½®ç‚¹
    baseline_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                   (world_max - world_min) + world_min)

    # åŸå§‹PINNä½¿ç”¨å›ºå®šçš„è®­ç»ƒå‘¨æœŸ
    print("INFO: (Baseline PINN) Setting collocation points...")
    # [ä¿®æ­£] å°†ç”Ÿæˆçš„é…ç½®ç‚¹æ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
    num_bc_points_base = pinn_baseline.data.bcs[0].points.shape[0]
    if pinn_baseline.model.train_state.X_train is None:
        pinn_baseline.model.train(iterations=0)
    start_index_base = num_bc_points_base
    end_index_base = len(pinn_baseline.model.train_state.X_train) - len(pinn_baseline.data.anchors)
    pinn_baseline.model.train_state.X_train[start_index_base:end_index_base] = baseline_collocation_points

    print("INFO: (Baseline PINN) Starting training...")
    pinn_baseline.model.train(iterations=TOTAL_EPOCHS, display_every=5)
    
    print("\n" + "="*60)
    print("ğŸ‰ åŸå§‹PINNè®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

    # --- 5. æœ€ç»ˆç»“æœè¯„ä¼°ä¸å¯¹æ¯” ---
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»“æœè¯„ä¼°ä¸å¯¹æ¯”")
    print("="*60 + "\n")

    test_points = test_data[:, :3]
    true_values = test_data[:, 3]

    print("æ­£åœ¨è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½...")
    adaptive_preds = pinn.predict(test_points)
    baseline_preds = pinn_baseline.predict(test_points)

    def calculate_mre(y_true, y_pred):
        return np.mean(np.abs(y_true - y_pred) / (y_true + 1e-10))

    mre_adaptive = calculate_mre(true_values, adaptive_preds)
    mre_baseline = calculate_mre(true_values, baseline_preds)
    
    # [æ–°å¢] è·å–ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒç‚¹æ•°
    adaptive_train_points = pinn.data.bcs[0].points.shape[0]
    baseline_train_points = pinn_baseline.data.bcs[0].points.shape[0]

    # åŠ¨æ€æ¨¡å‹åç§°
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        adaptive_model_name = "å®Œæ•´è‡ªé€‚åº”PINN"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        adaptive_model_name = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·PINN"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        adaptive_model_name = "ä»…æ•°æ®æ³¨å…¥PINN"
    else:
        adaptive_model_name = "å‘¨æœŸæ€§é‡å¯PINN"
    
    print(f"\n{'æ¨¡å‹':<32} | {'å¹³å‡ç›¸å¯¹è¯¯å·® (MRE)':<20} | {'è®­ç»ƒç‚¹æ•°':<12}")
    print("-" * 74)
    print(f"{adaptive_model_name:<32} | {mre_adaptive:<20.6%} | {adaptive_train_points:<12d}")
    print(f"{'åŸå§‹PINN (å›ºå®šé‡‡æ ·)':<32} | {mre_baseline:<20.6%} | {baseline_train_points:<12d}")
    print("-" * 74)
    
    # [æ–°å¢] è®­ç»ƒæ•ˆç‡å¯¹æ¯”
    print(f"\nğŸ“Š è®­ç»ƒæ•ˆç‡åˆ†æ:")
    print(f"   è‡ªé€‚åº”PINNä½¿ç”¨äº† {adaptive_train_points} ä¸ªè®­ç»ƒç‚¹ï¼Œè¾¾åˆ° MRE = {mre_adaptive:.6%}")
    print(f"   åŸºçº¿PINNä½¿ç”¨äº† {baseline_train_points} ä¸ªè®­ç»ƒç‚¹ï¼Œè¾¾åˆ° MRE = {mre_baseline:.6%}")
    if adaptive_train_points != baseline_train_points:
        efficiency_ratio = baseline_train_points / adaptive_train_points
        print(f"   è®­ç»ƒç‚¹æ•ˆç‡æ¯”: {efficiency_ratio:.2f}x (è‡ªé€‚åº”PINN vs åŸºçº¿PINN)")
    print(f"   ç‹¬ç«‹æµ‹è¯•é›†å¤§å°: {len(test_data)} ç‚¹")
    
    # ğŸ” æ–°å¢ï¼šæ”¶æ•›æ•ˆç‡åˆ†æ
    print(f"\nâš¡ æ”¶æ•›æ•ˆç‡å¯¹æ¯”:")
    if mre_adaptive < mre_baseline:
        improvement = (mre_baseline - mre_adaptive) / mre_baseline
        print(f"   ğŸ¯ è‡ªé€‚åº”PINNè¡¨ç°æ›´ä¼˜: ç›¸å¯¹æ”¹å–„ {improvement:.2%}")
        print(f"   ğŸ’¡ å…‹é‡Œé‡‘å¼•å¯¼ç­–ç•¥æœ‰æ•ˆ!")
    elif mre_adaptive > mre_baseline:
        degradation = (mre_adaptive - mre_baseline) / mre_baseline  
        print(f"   âš ï¸  è‡ªé€‚åº”PINNè¡¨ç°ç•¥å·®: ç›¸å¯¹ä¸‹é™ {degradation:.2%}")
        print(f"   ğŸ”§ å»ºè®®è°ƒæ•´æ¢ç´¢ç‡ç­–ç•¥æˆ–å¢åŠ è®­ç»ƒè½®æ•°")
    else:
        print(f"   ğŸ“Š ä¸¤ç§æ–¹æ³•æ€§èƒ½ç›¸å½“")
    
    # è®¡ç®—æ”¶æ•›é€Ÿåº¦æŒ‡æ ‡
    adaptive_epochs_to_convergence = len(pinn.mre_history)
    baseline_epochs_to_convergence = len(pinn_baseline.mre_history)
    
    print(f"\nğŸƒâ€â™‚ï¸ æ”¶æ•›é€Ÿåº¦åˆ†æ:")
    print(f"   è‡ªé€‚åº”PINN: {adaptive_epochs_to_convergence} æ¬¡è¯„ä¼°åˆ°è¾¾ MRE={mre_adaptive:.6%}")
    print(f"   åŸºçº¿PINN: {baseline_epochs_to_convergence} æ¬¡è¯„ä¼°åˆ°è¾¾ MRE={mre_baseline:.6%}")
    
    if adaptive_epochs_to_convergence < baseline_epochs_to_convergence:
        speed_improvement = (baseline_epochs_to_convergence - adaptive_epochs_to_convergence) / baseline_epochs_to_convergence
        print(f"   ğŸš€ è‡ªé€‚åº”PINNæ”¶æ•›æ›´å¿«: å‡å°‘ {speed_improvement:.1%} çš„è¯„ä¼°æ¬¡æ•°")
    else:
        print(f"   ğŸ“Š æ”¶æ•›é€Ÿåº¦ç›¸å½“æˆ–éœ€è¦æ›´å¤šè¯„ä¼°")

    # è¾“å‡ºé‡è¦äº‹ä»¶æ‘˜è¦
    if important_events:
        print(f"\nğŸ“‹ è®­ç»ƒè¿‡ç¨‹é‡è¦äº‹ä»¶æ‘˜è¦:")
        print("-" * 50)
        for epoch, event_type, description in important_events:
            event_name = "ğŸ”„ å…‹é‡Œé‡‘é‡é‡‡æ ·" if event_type == 'kriging_resampling' else "ğŸ“Š æ•°æ®æ³¨å…¥"
            print(f"  Epoch {epoch:4d}: {event_name} - {description}")
        print("-" * 50)

    # --- 6. ç»˜åˆ¶å¯¹æ¯”å›¾ ---
    print("\n" + "="*60)
    print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹MREå¯¹æ¯”å›¾")
    print("="*60 + "\n")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # æ‰“å°å°†è¦æ ‡æ³¨çš„äº‹ä»¶
    if important_events:
        print(f"INFO: å‡†å¤‡åœ¨å›¾è¡¨ä¸­æ ‡æ³¨ {len(important_events)} ä¸ªé‡è¦æ—¶é—´ç‚¹:")
        for epoch, event_type, description in important_events:
            print(f"  - Epoch {epoch}: {description}")
    
    # ç»˜åˆ¶è‡ªé€‚åº”PINNçš„MREå†å²
    if pinn.epoch_history and pinn.mre_history:
        # ä½¿ç”¨ä¸å‰é¢ä¸€è‡´çš„æ¨¡å‹åç§°
        if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
            adaptive_label = "å®Œæ•´è‡ªé€‚åº”PINN"
        elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
            adaptive_label = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·PINN"
        elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
            adaptive_label = "ä»…æ•°æ®æ³¨å…¥PINN"
        else:
            adaptive_label = "å‘¨æœŸæ€§é‡å¯PINN"
            
        ax.plot(pinn.epoch_history, pinn.mre_history, 
                label=adaptive_label, linewidth=2, alpha=0.8, color='blue')
    
    # ç»˜åˆ¶åŸºçº¿PINNçš„MREå†å²
    if pinn_baseline.epoch_history and pinn_baseline.mre_history:
        ax.plot(pinn_baseline.epoch_history, pinn_baseline.mre_history, 
                label='åŸå§‹PINN (å›ºå®šé‡‡æ ·)', linewidth=2, alpha=0.8, color='red')
    
    # æ·»åŠ é‡è¦äº‹ä»¶æ ‡æ³¨
    if important_events:
        print(f"INFO: æ ‡æ³¨ {len(important_events)} ä¸ªé‡è¦æ—¶é—´ç‚¹...")
        
        # å®šä¹‰äº‹ä»¶ç±»å‹çš„é¢œè‰²å’Œæ ·å¼
        event_styles = {
            'data_injection': {'color': 'green', 'linestyle': '--', 'alpha': 0.7, 'label': 'æ•°æ®æ³¨å…¥'},
            'kriging_resampling': {'color': 'orange', 'linestyle': '-.', 'alpha': 0.7, 'label': 'å…‹é‡Œé‡‘é‡é‡‡æ ·'},
            'phase_transition': {'color': 'purple', 'linestyle': ':', 'alpha': 0.7, 'label': 'é˜¶æ®µè½¬æ¢'},
            'loss_ratio_update': {'color': 'red', 'linestyle': '-', 'alpha': 0.5, 'label': 'æƒé‡æ›´æ–°'}
        }
        
        # æ™ºèƒ½æ ‡æ³¨ä½ç½®ç®—æ³•ï¼šé¿å…é‡å 
        def get_smart_annotation_positions(events, y_min, y_max):
            """æ™ºèƒ½è®¡ç®—æ ‡æ³¨ä½ç½®ï¼Œé¿å…é‡å """
            if not events:
                return []
            
            positions = []
            sorted_events = sorted(events, key=lambda x: x[0])  # æŒ‰epochæ’åº
            
            # åŠ¨æ€ç¡®å®šå±‚çº§æ•°é‡å’Œä½ç½®
            num_events = len(sorted_events)
            max_levels = min(6, max(3, num_events))  # è‡³å°‘3å±‚ï¼Œæœ€å¤š6å±‚
            
            # å®šä¹‰å¯ç”¨çš„yä½ç½®å±‚çº§ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼Œå¯¹æ•°ç©ºé—´å‡åŒ€åˆ†å¸ƒï¼‰
            y_levels = []
            if y_max > y_min and y_max / y_min > 10:  # å¯¹æ•°å°ºåº¦
                log_min, log_max = np.log10(y_min), np.log10(y_max)
                log_positions = np.linspace(log_max * 0.95, log_max * 0.5, max_levels)
                y_levels = [10 ** pos for pos in log_positions]
            else:  # çº¿æ€§å°ºåº¦
                y_levels = np.linspace(y_max * 0.95, y_max * 0.5, max_levels)
            
            # è®¡ç®—æœ€å°è·ç¦»ï¼ˆåŸºäºepochèŒƒå›´çš„è‡ªé€‚åº”ï¼‰
            if len(sorted_events) > 1:
                epoch_range = sorted_events[-1][0] - sorted_events[0][0]
                min_distance = max(200, epoch_range * 0.05)  # è‡³å°‘200epochæˆ–æ€»èŒƒå›´çš„5%
            else:
                min_distance = 200
            
            for i, (epoch, event_type, description) in enumerate(sorted_events):
                best_level = 0
                min_conflicts = float('inf')
                
                # å°è¯•æ¯ä¸ªå±‚çº§ï¼Œé€‰æ‹©å†²çªæœ€å°‘çš„
                for level_idx in range(len(y_levels)):
                    conflicts = 0
                    for prev_epoch, prev_level in positions:
                        if (abs(epoch - prev_epoch) < min_distance and 
                            level_idx == prev_level):
                            conflicts += 1
                    
                    if conflicts < min_conflicts:
                        min_conflicts = conflicts
                        best_level = level_idx
                        if conflicts == 0:  # æ‰¾åˆ°æ— å†²çªå±‚çº§ï¼Œç«‹å³ä½¿ç”¨
                            break
                
                positions.append((epoch, best_level))
            
            return [(epoch, y_levels[level]) for epoch, level in positions]
        
        # è·å–å½“å‰yè½´èŒƒå›´
        y_min, y_max = ax.get_ylim()
        
        # è®¡ç®—æ™ºèƒ½æ ‡æ³¨ä½ç½®
        annotation_positions = get_smart_annotation_positions(important_events, y_min, y_max)
        
        # ç»˜åˆ¶æ ‡æ³¨
        legend_handles = {}
        for i, ((epoch, event_type, description), (_, y_pos)) in enumerate(zip(important_events, annotation_positions)):
            style = event_styles.get(event_type, {'color': 'gray', 'linestyle': '-', 'alpha': 0.5, 'label': 'å…¶ä»–'})
            
            # ç»˜åˆ¶å‚ç›´çº¿ï¼ˆä»åº•éƒ¨åˆ°é¡¶éƒ¨ï¼‰
            line = ax.axvline(x=epoch, color=style['color'], linestyle=style['linestyle'], 
                             alpha=style['alpha'], linewidth=1.5)
            
            # æ”¶é›†å›¾ä¾‹å…ƒç´ ï¼ˆé¿å…é‡å¤ï¼‰
            if event_type not in legend_handles:
                legend_handles[event_type] = line
            
            # ç¼©çŸ­æè¿°æ–‡æœ¬ä»¥é¿å…è¿‡é•¿
            short_description = description.split('(')[0].strip()  # åªå–æ‹¬å·å‰çš„éƒ¨åˆ†
            if len(short_description) > 15:
                short_description = short_description[:12] + "..."
            
            # è®¡ç®—è¿æ¥çº¿çš„èµ·å§‹ç‚¹ï¼ˆå‚ç›´çº¿é¡¶éƒ¨é™„è¿‘ï¼‰
            line_start_y = y_max * 0.98
            
            # ç»˜åˆ¶è¿æ¥çº¿ï¼ˆä»å‚ç›´çº¿é¡¶éƒ¨åˆ°æ ‡æ³¨æ¡†ï¼‰
            if abs(y_pos - line_start_y) > y_max * 0.02:  # åªæœ‰å½“æ ‡æ³¨ä¸åœ¨é¡¶éƒ¨æ—¶æ‰ç”»è¿æ¥çº¿
                ax.annotate('', xy=(epoch, y_pos), xytext=(epoch, line_start_y),
                           arrowprops=dict(arrowstyle='-', color=style['color'], 
                                         alpha=0.4, lw=1, linestyle='dotted'))
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨ï¼ˆä½¿ç”¨æ™ºèƒ½ä½ç½®ï¼‰
            annotation = ax.annotate(
                f'{short_description}\n(E{epoch})',
                xy=(epoch, y_pos),
                xytext=(8, 0),
                textcoords='offset points',
                bbox=dict(boxstyle='round,pad=0.3', facecolor=style['color'], 
                         alpha=0.15, edgecolor=style['color'], linewidth=0.5),
                fontsize=8,
                ha='left',
                va='center',
                weight='normal'
            )
            
            # æ·»åŠ ä¸€ä¸ªå°åœ†ç‚¹æ ‡è®°åœ¨æ ‡æ³¨ä½ç½®
            ax.plot(epoch, y_pos, 'o', color=style['color'], markersize=4, 
                   alpha=0.8, markeredgecolor='white', markeredgewidth=0.5)
        
        # æ·»åŠ å›¾ä¾‹è¯´æ˜ï¼ˆåªæ˜¾ç¤ºå®é™…å‡ºç°çš„äº‹ä»¶ç±»å‹ï¼‰
        if legend_handles:
            from matplotlib.lines import Line2D
            legend_elements = []
            for event_type, line in legend_handles.items():
                style = event_styles.get(event_type, {'color': 'gray', 'linestyle': '-', 'label': 'å…¶ä»–'})
                legend_elements.append(
                    Line2D([0], [0], color=style['color'], linestyle=style['linestyle'], 
                          label=style['label'])
                )
            
            # åˆ›å»ºç¬¬äºŒä¸ªå›¾ä¾‹
            second_legend = ax.legend(handles=legend_elements, loc='upper right', 
                                     fontsize=9, title='é‡è¦äº‹ä»¶', title_fontsize=10,
                                     framealpha=0.9)
            ax.add_artist(second_legend)  # ä¿æŒåŸæœ‰å›¾ä¾‹
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('è®­ç»ƒè½®æ•° (Epochs)', fontsize=12)
    ax.set_ylabel('å¹³å‡ç›¸å¯¹è¯¯å·® (MRE)', fontsize=12)
    
    # åŠ¨æ€å›¾è¡¨æ ‡é¢˜
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        title_suffix = "å®Œæ•´è‡ªé€‚åº”"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        title_suffix = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        title_suffix = "ä»…æ•°æ®æ³¨å…¥"
    else:
        title_suffix = "å‘¨æœŸæ€§é‡å¯"
    
    ax.set_title(f'{title_suffix}PINN vs åŸºçº¿PINN: è®­ç»ƒè¿‡ç¨‹MREå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(loc='center right', fontsize=11)  # è°ƒæ•´åŸå›¾ä¾‹ä½ç½®
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡æ›´å¥½åœ°æ˜¾ç¤ºè¯¯å·®å˜åŒ–
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path(__file__).parent / "results"
    output_dir.mkdir(exist_ok=True)
    
    # åŠ¨æ€æ–‡ä»¶å
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        file_suffix = "full_adaptive"
        config_desc = "å®Œæ•´è‡ªé€‚åº”PINN (æ•°æ®æ³¨å…¥+å…‹é‡Œé‡‘)"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        file_suffix = "kriging_only"
        config_desc = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·ç­–ç•¥"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        file_suffix = "data_injection_only"
        config_desc = "ä»…æ•°æ®æ³¨å…¥ç­–ç•¥"
    else:
        file_suffix = "periodic_restart"
        config_desc = "ä»…å‘¨æœŸæ€§é‡å¯ç­–ç•¥"
    
    png_filename = f"mre_comparison_{file_suffix}.png"
    pdf_filename = f"mre_comparison_{file_suffix}.pdf"
    
    plt.savefig(output_dir / png_filename, dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / pdf_filename, bbox_inches='tight')
    
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   - PNGæ ¼å¼: {output_dir / png_filename}")
    print(f"   - PDFæ ¼å¼: {output_dir / pdf_filename}")
    print(f"   - å®éªŒé…ç½®: {config_desc}")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()

if __name__ == "__main__":
    main() 