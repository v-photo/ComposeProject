import numpy as np
import sys
from pathlib import Path
import deepxde as dde
import pandas as pd
import os
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# --- è·¯å¾„è®¾ç½® ---
try:
    current_dir = Path(__file__).parent.resolve()
    project_root = current_dir.parent
except NameError:
    project_root = Path('.').resolve()

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'PINN'))
sys.path.insert(0, str(project_root / 'Kriging'))

# --- æ¨¡å—å¯¼å…¥ ---
try:
    from PINN.data_processing import DataLoader
    from PINN.dataAnalysis import get_data
    from myKriging import training as kriging_training, testing as kriging_testing
    print("âœ… å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥æˆåŠŸã€‚")
except ImportError as e:
    print(f"âŒ å¤–éƒ¨æ•°æ®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# =================================================================================
#  å…è´£å£°æ˜ï¼šä»¥ä¸‹ç±»å’Œå‡½æ•°å‡ä¸ºå ä½ç¬¦ (Placeholder)
#  æ‚¨éœ€è¦æ ¹æ®æ‚¨ç°æœ‰çš„ PINN å’Œ Kriging åº“æ¥å¡«å……å®ƒä»¬çš„å…·ä½“å®ç°ã€‚
#  è¿™é‡Œçš„éª¨æ¶æ˜¯ä¸ºäº†æ¸…æ™°åœ°å±•ç¤º"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"è¿™ä¸€æŠ€æœ¯è·¯çº¿ã€‚
# =================================================================================

# --- [å…¨å±€] å®éªŒæ§åˆ¶å¼€å…³ ---
ENABLE_KRIGING = True     # ğŸ”§ æ§åˆ¶æ˜¯å¦å¯ç”¨å…‹é‡Œé‡‘å¼•å¯¼çš„é‡é‡‡æ ·
ENABLE_DATA_INJECTION = False  # ğŸ”§ æ§åˆ¶æ˜¯å¦å¯ç”¨æ•°æ®æ³¨å…¥ç­–ç•¥
ENABLE_RAPID_IMPROVEMENT_EARLY_STOP = True  # ğŸ”§ æ§åˆ¶æ˜¯å¦å¯ç”¨å¿«é€Ÿæ”¹å–„æ—©åœ

# --- [å…¨å±€] æ¢ç´¢ç‡é…ç½® ---
# ğŸ“Š æ¢ç´¢ç‡é€’å‡ç­–ç•¥é…ç½®
# è®¡ç®—å…¬å¼: exploration_ratio = max(FINAL, INITIAL - (cycle-1) * DECAY_RATE)

# # ğŸ¯ å½“å‰é…ç½® (é€‚ä¸­ç­–ç•¥)
# INITIAL_EXPLORATION_RATIO = 0.20    # åˆå§‹æ¢ç´¢ç‡ (ç¬¬1å‘¨æœŸ): 20%
# FINAL_EXPLORATION_RATIO = 0.05      # æœ€ç»ˆæ¢ç´¢ç‡ (æ”¶æ•›å€¼): 5%
# EXPLORATION_DECAY_RATE = 0.02       # æ¯å‘¨æœŸé€’å‡ç‡: 2%
# ğŸ‘† è¯¥é…ç½®ä¸‹ï¼šç¬¬1å‘¨æœŸ20% â†’ ç¬¬8å‘¨æœŸ5% â†’ ä¹‹åä¿æŒ5%

# ğŸ’¡ å…¶ä»–å¸¸ç”¨é…ç½®ç¤ºä¾‹ (å–æ¶ˆæ³¨é‡Šä½¿ç”¨):
# 
# ğŸš€ æ¿€è¿›ç­–ç•¥ (å¿«é€Ÿä»æ¢ç´¢è½¬å‘åˆ©ç”¨)
# INITIAL_EXPLORATION_RATIO = 0.25    # 25%
# FINAL_EXPLORATION_RATIO = 0.02      # 2%
# EXPLORATION_DECAY_RATE = 0.05       # 5%
# # # æ•ˆæœï¼šç¬¬1å‘¨æœŸ25% â†’ ç¬¬5å‘¨æœŸ5% â†’ ç¬¬6å‘¨æœŸ2%
#
# ğŸŒ ä¿å®ˆç­–ç•¥ (é•¿æœŸä¿æŒæ¢ç´¢)
INITIAL_EXPLORATION_RATIO = 0.50    # 50%
FINAL_EXPLORATION_RATIO = 0.018      # 18%
EXPLORATION_DECAY_RATE = 0.04       # 4%
# # æ•ˆæœï¼šç¬¬1å‘¨æœŸ15% â†’ ç¬¬8å‘¨æœŸ8% â†’ ä¹‹åä¿æŒ8%
#
# # ğŸ¯ ç²¾å‡†ç­–ç•¥ (é«˜åˆ©ç”¨ç‡)
# INITIAL_EXPLORATION_RATIO = 0.30    # 30%
# FINAL_EXPLORATION_RATIO = 0.03      # 3%
# EXPLORATION_DECAY_RATE = 0.03       # 3%
# # æ•ˆæœï¼šç¬¬1å‘¨æœŸ30% â†’ ç¬¬10å‘¨æœŸ3% â†’ ä¹‹åä¿æŒ3%

# å››ç§å®éªŒæ¨¡å¼:
# ENABLE_KRIGING=False, ENABLE_DATA_INJECTION=False: ä»…å‘¨æœŸæ€§é‡å¯ (æ— è‡ªé€‚åº”ç­–ç•¥)
# ENABLE_KRIGING=False, ENABLE_DATA_INJECTION=True:  ä»…æ•°æ®æ³¨å…¥ç­–ç•¥
# ENABLE_KRIGING=True,  ENABLE_DATA_INJECTION=False: ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·ç­–ç•¥  
# ENABLE_KRIGING=True,  ENABLE_DATA_INJECTION=True:  å®Œæ•´è‡ªé€‚åº”PINN

class DummyDataLoader:
    """
    ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»å¤–éƒ¨æ–‡ä»¶åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®(æ›¿ä»£åŸæœ‰çš„DummyDataLoader)ã€‚
    """
    def __init__(self, data_path: str, space_dims: np.ndarray, num_samples: int):
        self.data_path = data_path
        self.space_dims = space_dims
        self.num_samples = num_samples
        print(f"INFO: (DataLoader) Initialized with data_path='{self.data_path}'")

    def get_training_data(self, split_ratios: list = None, test_set_size: int = None):
        """
        åŠ è½½ã€å¤„ç†å¹¶é‡‡æ ·ç¨€ç–è®­ç»ƒç‚¹ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ¯”ä¾‹åˆ—è¡¨è¿›è¡Œåˆ†å‰²ã€‚
        
        Args:
            split_ratios (list, optional): ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œå…¶å’Œåº”å°äº1ã€‚
                ä¾‹å¦‚ [0.7, 0.1, 0.1] ä»£è¡¨ï¼š
                - 70% ä½œä¸ºä¸»è®­ç»ƒé›†
                - 10% ä½œä¸ºç¬¬ä¸€ä¸ªå‚¨å¤‡é›†
                - 10% ä½œä¸ºç¬¬äºŒä¸ªå‚¨å¤‡é›†
                - å‰©ä½™çš„ 10% å°†ä½œä¸ºæµ‹è¯•é›†ã€‚
                å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ 80/20 è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ã€‚
            test_set_size (int, optional): å¦‚æœæŒ‡å®šï¼Œå°†ç”Ÿæˆç‹¬ç«‹çš„æµ‹è¯•é›†è€Œéä»è®­ç»ƒæ•°æ®åˆ†å‰²ã€‚
        """
        # ... (å‰é¢åŠ è½½å’Œé‡‡æ ·æ•°æ®çš„éƒ¨åˆ†ä¿æŒä¸å˜) ...
        print(f"INFO: (DataLoader) Loading raw data from {self.data_path}...")
        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            
        raw_data = get_data(self.data_path)
        
        print("INFO: (DataLoader) Normalizing dose data...")
        dose_data = DataLoader.load_dose_from_dict(
            data_dict=raw_data,
            space_dims=self.space_dims
        )
        
        print(f"INFO: (DataLoader) Sampling {self.num_samples} training points...")
        train_points, train_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=self.num_samples,
            sampling_strategy='positive_only',
        )
        print(f"INFO: (DataLoader) âœ… Successfully sampled {len(train_points)} points.")

        # å°†åæ ‡å’Œå€¼åˆå¹¶æˆ [x, y, z, value] æ ¼å¼
        all_sampled_data = np.hstack([train_points, train_values.reshape(-1, 1)])
        
        # [æ–°å¢] ç”Ÿæˆç‹¬ç«‹æµ‹è¯•é›†ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if test_set_size is not None:
            print(f"INFO: (DataLoader) Generating independent test set of size {test_set_size}...")
            test_set = self._generate_independent_test_set(dose_data, test_set_size)
        else:
            test_set = None  # å°†åœ¨ä¸‹é¢çš„åˆ†å‰²é€»è¾‘ä¸­å¤„ç†
        
        # [æ–°é€»è¾‘] ä½¿ç”¨å¯é…ç½®çš„åˆ†å‰²ç­–ç•¥
        if split_ratios is None:
            # é»˜è®¤è¡Œä¸ºï¼š80/20 åˆ†å‰²
            if test_set is None:
                main_train_set, test_set = train_test_split(all_sampled_data, test_size=0.2, random_state=42)
            else:
                main_train_set = all_sampled_data  # å…¨éƒ¨ç”¨ä½œè®­ç»ƒæ•°æ®
            reserve_pools = []
        else:
            if test_set is None and sum(split_ratios) >= 1.0:
                raise ValueError("split_ratios çš„æ€»å’Œå¿…é¡»å°äº 1.0ï¼Œä»¥ä¾¿ä¸ºæµ‹è¯•é›†ç•™å‡ºç©ºé—´ã€‚")

            remaining_data = all_sampled_data
            data_pools = []
            
            # å¾ªç¯åˆ‡åˆ†å‡ºä¸»è®­ç»ƒé›†å’Œæ‰€æœ‰å‚¨å¤‡é›†
            current_total_fraction = 1.0
            for ratio in split_ratios:
                # è®¡ç®—å½“å‰æ¯”ä¾‹ç›¸å¯¹äºå‰©ä½™æ•°æ®é‡çš„æ¯”ä¾‹
                split_fraction = ratio / current_total_fraction
                pool, remaining_data = train_test_split(remaining_data, test_size=(1.0 - split_fraction), random_state=42)
                data_pools.append(pool)
                current_total_fraction -= ratio

            main_train_set = data_pools[0]
            reserve_pools = data_pools[1:]
            
            # å¦‚æœæ²¡æœ‰ç‹¬ç«‹æµ‹è¯•é›†ï¼Œåˆ™ä½¿ç”¨å‰©ä½™æ•°æ®
            if test_set is None:
                test_set = remaining_data
        
        print(f"INFO: (DataLoader) âœ… Split data into: Main training ({len(main_train_set)}), Test ({len(test_set)}), Reserve Pools ({len(reserve_pools)} pools).")
        if reserve_pools:
            for i, pool in enumerate(reserve_pools):
                print(f"    - Reserve Pool {i+1}: {len(pool)} points")

        return main_train_set, reserve_pools, test_set, dose_data

    def _generate_independent_test_set(self, dose_data: dict, test_set_size: int):
        """
        ç”Ÿæˆå®Œå…¨ç‹¬ç«‹äºè®­ç»ƒæ•°æ®çš„æµ‹è¯•é›†ï¼Œåœ¨æ•´ä¸ªç‰©ç†åŸŸå†…å‡åŒ€é‡‡æ ·ã€‚
        
        Args:
            dose_data (dict): åŒ…å«ç‰©ç†åŸŸè¾¹ç•Œçš„æ•°æ®å­—å…¸
            test_set_size (int): æµ‹è¯•é›†å¤§å°
            
        Returns:
            np.ndarray: æµ‹è¯•é›†æ•°æ® [x, y, z, value]
        """
        # ä½¿ç”¨ DataLoader.sample_training_points åœ¨æ•´ä¸ªåŸŸå†…é‡‡æ ·æµ‹è¯•ç‚¹
        test_points, test_values, _ = DataLoader.sample_training_points(
            dose_data, 
            num_samples=test_set_size,
            sampling_strategy='uniform',  # ä½¿ç”¨å‡åŒ€é‡‡æ ·
        )
        
        # åˆå¹¶ä¸º [x, y, z, value] æ ¼å¼
        test_set = np.hstack([test_points, test_values.reshape(-1, 1)])
        print(f"INFO: (DataLoader) âœ… Generated independent test set with {len(test_set)} points.")
        return test_set

class GPUKriging:
    """
    [çœŸå®å®ç°] GPUåŠ é€Ÿçš„å…‹é‡Œé‡‘æ¨¡å‹çš„é€‚é…å™¨ã€‚
    è¯¥å®ç°å€Ÿé‰´äº† ComposeTools.py ä¸­çš„ KrigingAdapterï¼Œå¹¶è°ƒç”¨äº† myKriging åº“ã€‚
    """
    def __init__(self, variogram_model='exponential', **kwargs):
        """
        åˆå§‹åŒ–Krigingé€‚é…å™¨ã€‚
        
        Args:
            variogram_model (str): å…‹é‡Œé‡‘æ‰€éœ€çš„å˜å¼‚å‡½æ•°æ¨¡å‹ã€‚
            **kwargs: å…¶ä»–å¯ä»¥ä¼ é€’ç»™ myKriging åº“çš„å‚æ•° (å¦‚ nlags, block_size)ã€‚
        """
        self.model = None
        self._is_fitted = False
        self.variogram_model = variogram_model
        self.kriging_params = kwargs
        print(f"INFO: (GPUKriging) Initialized with variogram model: {self.variogram_model}")

    def fit(self, points: np.ndarray, values: np.ndarray):
        """
        ä½¿ç”¨ç¨€ç–çš„ç‚¹åæ ‡å’Œå¯¹åº”çš„æ®‹å·®å€¼æ¥è®­ç»ƒå…‹é‡Œé‡‘ä»£ç†æ¨¡å‹ã€‚
        """
        print(f"INFO: (GPUKriging) Fitting model with {len(points)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df = pd.DataFrame({
            'x': points[:, 0],
            'y': points[:, 1],
            'z': points[:, 2],
            'target': values
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_training å‡½æ•°
        self.model = kriging_training(
            df=df,
            variogram_model=self.variogram_model,
            nlags=self.kriging_params.get('nlags', 8),
            enable_plotting=False, # è®­ç»ƒä»£ç†æ¨¡å‹æ—¶é€šå¸¸ä¸ç»˜å›¾
            weight=False,
            uk=False,
            cpu_on=False # ç¡®ä¿ä½¿ç”¨GPU
        )
        
        self._is_fitted = True
        print("INFO: (GPUKriging) âœ… Model fitted.")

    def predict(self, points_to_predict: np.ndarray) -> np.ndarray:
        """
        å¯¹æ–°çš„ç‚¹è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼Œåˆ©ç”¨GPUåŠ é€Ÿã€‚
        """
        if not self._is_fitted:
            raise RuntimeError("Kriging model must be fitted before prediction.")
        
        print(f"INFO: (GPUKriging) Predicting values for {len(points_to_predict)} points...")
        # 1. å°† NumPy æ•°ç»„è½¬æ¢ä¸º myKriging æ‰€æœŸæœ›çš„ Pandas DataFrame
        df_pred = pd.DataFrame({
            'x': points_to_predict[:, 0],
            'y': points_to_predict[:, 1],
            'z': points_to_predict[:, 2],
            'target': np.zeros(points_to_predict.shape[0]) # è™šæ‹Ÿç›®æ ‡å€¼
        })

        # 2. è°ƒç”¨å¤–éƒ¨çš„ kriging_testing å‡½æ•°ï¼Œç¡®ä¿ä½¿ç”¨GPUåŠ é€Ÿé…ç½®
        predictions, _ = kriging_testing(
            df=df_pred,
            model=self.model,
            block_size=self.kriging_params.get('block_size', 10000),
            cpu_on=False, # ç¡®ä¿ä½¿ç”¨GPU
            style="gpu_b", # ä½¿ç”¨GPUæ‰¹å¤„ç†é£æ ¼
            multi_process=False,
            print_time=False,
            torch_ac=False, # ä½¿ç”¨PyTorchåŠ é€Ÿ
            compute_precision=False # é¢„æµ‹æ®‹å·®æ—¶ä¸éœ€è¦ç²¾åº¦è®¡ç®—
        )
        
        print(f"INFO: (GPUKriging) âœ… Prediction complete.")
        return predictions.flatten() # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„

class PINNModel:
    """
    [çœŸå®å®ç°] ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ã€‚
    è¯¥å®ç°è¢«è®¾è®¡ä¸ºå¯ä»å¤–éƒ¨æ§åˆ¶çš„æ¨¡å¼ï¼Œä»¥æ”¯æŒè‡ªé€‚åº”è®­ç»ƒæµç¨‹ã€‚
    """
    def __init__(self, dose_data: dict, training_data: np.ndarray, test_data: np.ndarray, num_collocation_points: int, network_layers=[3, 64, 64, 64, 1], lr=1e-3):
        """
        åˆå§‹åŒ–PINNæ¨¡å‹ï¼Œä½†ä¸PINNTrainerä¸åŒï¼Œè¿™é‡Œåªåšå‡†å¤‡å·¥ä½œï¼Œä¸å¼€å§‹è®­ç»ƒã€‚
        
        Args:
            dose_data (dict): ä»DataLoaderåŠ è½½çš„æ•°æ®å­—å…¸ã€‚
            training_data (np.ndarray): ç¨€ç–è®­ç»ƒæ•°æ® [x,y,z,value]ã€‚
            test_data (np.ndarray): ç¨€ç–æµ‹è¯•æ•°æ® [x,y,z,value]ã€‚
            num_collocation_points (int): æ±‚è§£åŸŸç‚¹çš„æ•°é‡ã€‚
            network_layers (list): ç¥ç»ç½‘ç»œç»“æ„ã€‚
            lr (float): å­¦ä¹ ç‡ã€‚
        """
        print("INFO: (PINNModel) Initializing a DeepXDE-based model for external control...")
        
        self.test_data_linear = test_data # å­˜å‚¨çº¿æ€§å°ºåº¦çš„æµ‹è¯•æ•°æ®
        
        # 1. å®šä¹‰å‡ ä½•
        world_min = dose_data['world_min']
        world_max = dose_data['world_max']
        self.geometry = dde.geometry.Cuboid(world_min, world_max)

        # 2. å®šä¹‰å¯è®­ç»ƒå‚æ•°
        k_initial_guess = 1.0 
        self.log_k_pinn = dde.Variable(np.log(k_initial_guess))
        
        # 3. å®šä¹‰PDEä¸ºç±»çš„æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å…¶ä»–åœ°æ–¹å¤ç”¨
        self.pde = self._build_pde_func()
        
        # 4. å®šä¹‰è®­ç»ƒæ•°æ®ç‚¹
        observe_x = training_data[:, :3]
        observe_y = np.log(np.maximum(training_data[:, 3:], 1e-30))
        data_points = dde.icbc.PointSetBC(observe_x, observe_y, component=0)
        
        # 5. ç»„åˆæˆdde.data.PDEå¯¹è±¡
        self.data = dde.data.PDE(
            self.geometry,
            self.pde,
            [data_points],
            num_domain=num_collocation_points,
            anchors=observe_x,
            # æˆ‘ä»¬è‡ªå®šä¹‰çš„æŒ‡æ ‡ä¼šä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„self.test_data_linear
        )
        
        # 6. åˆ›å»ºç½‘ç»œå’Œæ¨¡å‹
        self.net = dde.nn.FNN(network_layers, "tanh", "Glorot normal")
        self.model = dde.Model(self.data, self.net)
        
        # 7. è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°è¢«ç§»å‡ºä¸ºç±»æ–¹æ³• mean_relative_error_metric

        self.lr = lr # ä¿å­˜å­¦ä¹ ç‡ä»¥å¤‡é‡ç¼–è¯‘æ—¶ä½¿ç”¨
        
        # [æ–°å¢] MREå†å²è®°å½•åˆ—è¡¨
        self.mre_history = []
        self.epoch_history = []
        
        # 8. ç¼–è¯‘æ¨¡å‹ï¼ŒåŠ å…¥è‡ªå®šä¹‰æŒ‡æ ‡
        self.compile_model()
        print("INFO: (PINNModel) âœ… Model compiled and ready for training cycles.")
        
    def compile_model(self):
        """å°†æ¨¡å‹ç¼–è¯‘å°è£…æˆä¸€ä¸ªæ–¹æ³•ï¼Œæ–¹ä¾¿é‡ç”¨ã€‚"""
        # [ä¿®æ­£] åœ¨è¿™é‡Œè®¾ç½®æŒ‡æ ‡å‡½æ•°çš„æ˜¾ç¤ºåç§°
        # self.mean_relative_error_metric.__name__ = "MRE_test_set" # [ä¿®æ­£] ç§»é™¤æ­¤è¡Œï¼Œä¸èƒ½ä¸ºç±»æ–¹æ³•è®¾ç½®__name__

        self.model.compile(
            "adam", 
            lr=self.lr, 
            loss_weights=[1, 10], 
            external_trainable_variables=[self.log_k_pinn],
            metrics=[self.mean_relative_error_metric] # [ä¿®æ­£] ä¼ é€’å‡½æ•°å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        )

    def mean_relative_error_metric(self, y_true_ignored, y_pred_ignored):
        """
        ä¸€ä¸ª"hack"çš„æŒ‡æ ‡å‡½æ•°ã€‚å®ƒå¿½ç•¥ddeä¼ å…¥çš„å‚æ•°ï¼Œ
        è½¬è€Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±å­˜å‚¨çš„ã€åŸºäºçœŸå®ç‰©ç†å€¼çš„æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚
        """
        # ä½¿ç”¨æ¨¡å‹å¯¹æˆ‘ä»¬è‡ªå·±çš„æµ‹è¯•ç‚¹è¿›è¡Œé¢„æµ‹
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        
        # å°†é¢„æµ‹å€¼å’ŒçœŸå®å€¼éƒ½è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))
        
        # [æ–°å¢] è®°å½•MREå†å²
        current_epoch = self.model.train_state.step if self.model.train_state.step else 0
        self.mre_history.append(mre)
        self.epoch_history.append(current_epoch)
        
        return mre

    def inject_new_data(self, new_data_array: np.ndarray):
        """
        [æ–°èƒ½åŠ›] å‘æ¨¡å‹ä¸­æ³¨å…¥æ–°çš„è®­ç»ƒæ•°æ®ç‚¹ã€‚
        """
        print(f"\nINFO: (PINNModel)  injecting {len(new_data_array)} new data points...")
        
        # 1. è·å–ç°æœ‰æ•°æ®
        current_bc = self.data.bcs[0]
        current_points = current_bc.points
        current_values_log = current_bc.values.cpu()

        # 2. å‡†å¤‡æ–°æ•°æ®
        new_points = new_data_array[:, :3]
        new_values_log = np.log(np.maximum(new_data_array[:, 3:], 1e-30)).reshape(-1, 1)

        # 3. åˆå¹¶æ–°æ—§æ•°æ®
        combined_points = np.vstack([current_points, new_points])
        combined_values_log = np.vstack([current_values_log, new_values_log])
        
        print(f"    Total training points increased to {len(combined_points)}.")

        # 4. åˆ›å»ºæ–°çš„ PointSetBC å’Œ PDE æ•°æ®å¯¹è±¡
        new_bc = dde.icbc.PointSetBC(combined_points, combined_values_log, component=0)
        
        # æ›´æ–°é”šç‚¹ä»¥åŒ…å«æ‰€æœ‰è®­ç»ƒæ•°æ®
        new_anchors = combined_points
        
        new_data_obj = dde.data.PDE(
            self.geometry,
            self.pde,
            [new_bc],
            num_domain=self.data.num_domain,
            anchors=new_anchors
        )
        
        # 5. æ›´æ–°æ¨¡å‹çš„æ•°æ®å¹¶é‡æ–°ç¼–è¯‘
        self.data = new_data_obj
        self.model.data = self.data
        self.compile_model()
        print("INFO: (PINNModel) âœ… Model re-compiled with new data. Initializing new train state...")
        # [ä¿®æ­£] è°ƒç”¨ train(0) æ¥å¼ºåˆ¶ä½¿ç”¨æ–°æ•°æ®å¯¹è±¡é‡å»ºè®­ç»ƒçŠ¶æ€
        self.model.train(iterations=0, display_every=100000) # display_everyè®¾ä¸ºå¤§æ•°ä»¥é¿å…ä¸å¿…è¦çš„è¾“å‡º
        print("INFO: (PINNModel) âœ… New train state initialized.")
        
        # [ä¿®æ­£] æ³¨å…¥æ•°æ®åï¼Œè®°å½•ä¸€æ¬¡å½“å‰MREä»¥ä¿æŒå†å²è¿ç»­æ€§
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        current_mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))
        current_epoch = self.model.train_state.step if self.model.train_state.step else 0
        self.mre_history.append(current_mre)
        self.epoch_history.append(current_epoch)
        print(f"INFO: (PINNModel) MRE after data injection: {current_mre:.6f} at epoch {current_epoch}")
        
    def _build_pde_func(self):
        """å°†PDEå®šä¹‰å°è£…åœ¨ä¸€ä¸ªå·¥å‚å‡½æ•°ä¸­ï¼Œä»¥æ•è·self.log_k_pinnã€‚"""
        def pde_func(x, u):
            grad_u_sq = dde.grad.jacobian(u, x, i=0, j=0)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=1)**2 + \
                        dde.grad.jacobian(u, x, i=0, j=2)**2
            laplacian_u = dde.grad.hessian(u, x, i=0, j=0) + \
                          dde.grad.hessian(u, x, i=1, j=1) + \
                          dde.grad.hessian(u, x, i=2, j=2)
            k_squared = dde.backend.exp(2 * self.log_k_pinn)
            return grad_u_sq + laplacian_u - k_squared
        return pde_func

    def run_training_cycle(self, max_epochs: int, detect_every: int, collocation_points: np.ndarray, 
                         detection_threshold: float = 0.1):
        """
        [é‡æ„] æ‰§è¡Œä¸€ä¸ªå¸¦æœ‰åŠ¨æ€åœæ­¢æ¡ä»¶çš„è®­ç»ƒå‘¨æœŸã€‚
        
        Args:
            max_epochs (int): å½“å‰å‘¨æœŸçš„æœ€å¤§è®­ç»ƒè½®æ•°ã€‚
            detect_every (int): æ¯éš”å¤šå°‘è½®è¿›è¡Œä¸€æ¬¡æ€§èƒ½æ£€æµ‹ã€‚
            collocation_points (np.ndarray): ç”¨äºæœ¬å‘¨æœŸçš„é…ç½®ç‚¹ã€‚
            detection_threshold (float): è§¦å‘æ—©åœçš„ç›¸å¯¹æ”¹è¿›é˜ˆå€¼ã€‚
        
        Returns:
            dict: ä¸€ä¸ªåŒ…å«è®­ç»ƒç»“æœä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚{'stagnation_detected': bool}
        """
        # 1. ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä¹‰æ£€æŸ¥ç‚¹æ–‡ä»¶åçš„å‰ç¼€ï¼Œå¹¶ç¡®ä¿ç›®å½•å­˜åœ¨
        script_dir = Path(__file__).parent.resolve()
        checkpoint_path_prefix = str(script_dir / "models" / "best_model_in_cycle")
        os.makedirs(Path(checkpoint_path_prefix).parent, exist_ok=True)

        # 2. æ›´æ–°æ±‚è§£åŸŸç‚¹
        num_bc_points = self.data.bcs[0].points.shape[0]
        if self.model.train_state.X_train is None:
            # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ï¼Œä»¥ä¾¿å¯ä»¥ä¿®æ”¹X_train
            self.model.train(iterations=0)
        start_index = num_bc_points
        end_index = len(self.model.train_state.X_train) - len(self.data.anchors)
        self.model.train_state.X_train[start_index:end_index] = collocation_points
        
        # [æ–°] åˆå§‹åŒ–æœ¬å‘¨æœŸçš„è¿”å›çŠ¶æ€
        stagnation_detected_this_run = False
        data_injected_this_cycle = False
        
        # 3. åˆ›å»ºå›è°ƒï¼Œå¹¶ç”¨å½“å‰æ¨¡å‹çš„æ€§èƒ½åˆå§‹åŒ–å®ƒ
        stopper = EarlyCycleStopper(
            detection_threshold=detection_threshold,
            display_every=5,
            checkpoint_path_prefix=checkpoint_path_prefix
        )
        # [ä¿®æ­£] åœ¨é‡ç½®æ—¶ï¼Œä¼ å…¥å½“å‰æ¨¡å‹çš„MREå’Œåˆå§‹æ£€æŸ¥ç‚¹ä½œä¸ºåŸºçº¿
        test_x = self.test_data_linear[:, :3]
        pred_y_log = self.model.predict(test_x)
        pred_y_linear = np.exp(pred_y_log)
        true_y_linear = self.test_data_linear[:, 3:]
        initial_mre = np.mean(np.abs(true_y_linear - pred_y_linear) / (true_y_linear + 1e-10))

        # ä¸ºåˆå§‹çŠ¶æ€åˆ›å»ºç¬¬ä¸€ä¸ªåŸºå‡†æ£€æŸ¥ç‚¹
        epochs_before_cycle = self.model.train_state.step or 0
        self.model.save(checkpoint_path_prefix, verbose=0)
        initial_model_path = f"{checkpoint_path_prefix}-{epochs_before_cycle}.pt"
        
        stopper.reset_cycle(initial_mre=initial_mre, initial_model_path=initial_model_path)
        
        print(f"INFO: (PINNModel) Starting dynamic training cycle (max: {max_epochs} epochs, detect every: {detect_every})...")
        print(f"    Initial MRE for this cycle is {initial_mre:.4f}")
        
        remaining_epochs = max_epochs
        while remaining_epochs > 0:
            epochs_to_run = min(detect_every, remaining_epochs)
            
            self.model.train(
                iterations=epochs_to_run, 
                display_every=5,
                callbacks=[stopper]
            )

            # --- [æ–°ç­–ç•¥] æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰ç»“æŸæœ¬è½®è‡ªé€‚åº”å‘¨æœŸ ---
            should_exit_cycle = False

            # æ¡ä»¶1: åœæ» (Stagnation) - æ¨¡å‹æ€§èƒ½åœ¨æœ¬è½®è®­ç»ƒåå˜å·®
            if stopper.best_model_path and os.path.exists(stopper.best_model_path):
                latest_mre = self.model.train_state.metrics_test[-1]
                if latest_mre > stopper.best_mre:
                    print(f"    âš ï¸ Stagnation detected: MRE increased to {latest_mre:.4f} (best is {stopper.best_mre:.4f}).")
                    stagnation_detected_this_run = True
                    
                    print(f"    â†³ Forcing new adaptive resampling cycle...")
                    self.model.restore(stopper.best_model_path, verbose=0) 
                    should_exit_cycle = True

            # æ¡ä»¶2: å¿«é€Ÿæå‡ (Rapid Improvement) - ğŸ”§ å¯é€‰æ‹©æ˜¯å¦å¯ç”¨
            if ENABLE_RAPID_IMPROVEMENT_EARLY_STOP and stopper.should_stop:
                print(f"\nINFO: (PINNModel) ğŸ“ˆ Rapid improvement! Capitalizing on gains and forcing new resampling.")
                should_exit_cycle = True
            
            if should_exit_cycle:
                break
                
            remaining_epochs -= epochs_to_run
        else:
             print(f"\nINFO: (PINNModel) Max epochs reached for this cycle.")

        # 4. [é‡è¦] æ— è®ºå¦‚ä½•ï¼Œéƒ½ä»æœ€ç»ˆçš„æœ€ä½³æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å‹ï¼Œå¹¶æ¸…ç†æ–‡ä»¶
        if stopper.best_model_path and os.path.exists(stopper.best_model_path):
            print(f"INFO: (PINNModel) Restoring model to best state from '{stopper.best_model_path}'...")
            self.model.restore(stopper.best_model_path, verbose=1)
            os.remove(stopper.best_model_path) # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        else:
            print("WARNING: (PINNModel) Best checkpoint file not found. Model may not be in its best state.")
            
        return {'stagnation_detected': stagnation_detected_this_run}

    def predict(self, points: np.ndarray) -> np.ndarray:
        """
        [æ–°] ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¿”å›çº¿æ€§å°ºåº¦çš„ç‰©ç†å€¼ã€‚
        
        Args:
            points (np.ndarray): å¾…é¢„æµ‹ç‚¹çš„åæ ‡ï¼Œå½¢çŠ¶ä¸º (N, 3)ã€‚
            
        Returns:
            np.ndarray: é¢„æµ‹çš„ç‰©ç†å€¼ï¼Œå½¢çŠ¶ä¸º (N,)ã€‚
        """
        print(f"INFO: (PINNModel) Predicting on {len(points)} points...")
        # model.predict è¿”å›çš„æ˜¯å¯¹æ•°å°ºåº¦çš„å€¼
        pred_y_log = self.model.predict(points)
        # è½¬æ¢å›çº¿æ€§ç‰©ç†å°ºåº¦
        pred_y_linear = np.exp(pred_y_log)
        return pred_y_linear.flatten()

    def compute_pde_residual(self, points: np.ndarray) -> np.ndarray:
        """
        [çœŸå®å®ç°] è®¡ç®—ç»™å®šç‚¹ä¸Šçš„ç‰©ç†æ–¹ç¨‹æ®‹å·®ã€‚
        åˆ©ç”¨ deepxde çš„ model.predict(operator=...) åŠŸèƒ½ã€‚
        """
        print(f"INFO: (PINNModel) Computing PDE residuals for {len(points)} points...")
        
        # deepxde.Model.predict å¯ä»¥æ¥å—ä¸€ä¸ª operator å‚æ•°
        # æˆ‘ä»¬å°† self.pde (åœ¨__init__ä¸­å®šä¹‰çš„å‡½æ•°) ä½œä¸ºç®—å­ä¼ å…¥
        residuals = self.model.predict(points, operator=self.pde)
        
        # è¿”å›æ®‹å·®çš„ç»å¯¹å€¼ï¼Œå¹¶å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        return np.abs(residuals).flatten()

class EarlyCycleStopper(dde.callbacks.Callback):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰å›è°ƒï¼Œç”¨äºåœ¨è®­ç»ƒå‘¨æœŸå†…å®ç°åŸºäºæ€§èƒ½çš„"æ—©åœ"ã€‚
    åŒæ—¶è‡ªå·±è´Ÿè´£ä¿å­˜å‘¨æœŸå†…çš„æœ€ä½³æ¨¡å‹ã€‚
    """
    def __init__(self, detection_threshold: float, display_every: int, checkpoint_path_prefix: str):
        super().__init__()
        self.threshold = detection_threshold
        self.display_every = display_every
        self.checkpoint_path_prefix = checkpoint_path_prefix
        self.best_mre = np.inf
        self.should_stop = False
        self.best_model_path = "" # å°†å­˜å‚¨æœ€ä½³æ¨¡å‹çš„å®Œæ•´çœŸå®è·¯å¾„

    def reset_cycle(self, initial_mre: float = np.inf, initial_model_path: str = ""):
        """
        æ‰‹åŠ¨é‡ç½®æ•´ä¸ªå‘¨æœŸçš„çŠ¶æ€ï¼Œä¸ºæ–°çš„è‡ªé€‚åº”å‘¨æœŸåšå‡†å¤‡ã€‚
        å¯ä»¥æ¥æ”¶ä¸€ä¸ªåˆå§‹MREå’Œæ¨¡å‹è·¯å¾„ä½œä¸ºæœ¬å‘¨æœŸçš„æ€§èƒ½åŸºçº¿ã€‚
        """
        # æ¸…ç†ä¸Šä¸€è½®å¯èƒ½é—ç•™çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        if self.best_model_path and os.path.exists(self.best_model_path):
            os.remove(self.best_model_path)
        
        self.best_mre = initial_mre
        self.best_model_path = initial_model_path
        self.should_stop = False

    def on_epoch_end(self):
        """åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¢«è°ƒç”¨, å¹¶ä¸”åœ¨è¿™é‡Œæ£€æŸ¥æ€§èƒ½"""
        if self.model.train_state.step > 0 and self.model.train_state.step % self.display_every == 0:
            if not self.model.train_state.metrics_test:
                 return

            latest_mre = self.model.train_state.metrics_test[-1]
            
            if self.best_mre != np.inf:
                improvement = self.best_mre - latest_mre
                required_improvement_amount = self.best_mre * self.threshold
                
                if improvement > required_improvement_amount:
                    print(f"    ğŸ’¡ Early Stop: MRE dropped from {self.best_mre:.4f} to {latest_mre:.4f} (>{self.threshold:.0%}).")
                    self.should_stop = True
            
            # åˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯å¦æ˜¯æ–°çš„æœ€ä¼˜æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¿å­˜å®ƒ
            if latest_mre < self.best_mre:
                print(f"    â­ New best model found (MRE: {latest_mre:.4f}). Checkpointing...")
                self.best_mre = latest_mre

                # æ¸…ç†ä¸Šä¸€ä¸ªæœ€ä½³æ¨¡å‹
                if self.best_model_path and os.path.exists(self.best_model_path):
                    os.remove(self.best_model_path)
                
                # æ„å»ºæ–°çš„æœ€ä½³æ¨¡å‹è·¯å¾„å¹¶ä¿å­˜
                current_step = self.model.train_state.step
                self.best_model_path = f"{self.checkpoint_path_prefix}-{current_step}.pt"
                self.model.save(self.checkpoint_path_prefix, verbose=0)

class AdaptiveSampler:
    """
    [å»ºè®®æ‚¨å®ç°] è‡ªé€‚åº”é‡‡æ ·å™¨ã€‚
    """
    def __init__(self, domain_bounds, total_candidates=100000):
        self.bounds = domain_bounds
        # é¢„å…ˆåœ¨æ•´ä¸ªåŸŸå†…ç”Ÿæˆå¤§é‡çš„å€™é€‰ç‚¹ï¼Œåç»­ä»ä¸­ç­›é€‰
        self.candidate_points = np.random.rand(total_candidates, 3) * \
            (domain_bounds[1] - domain_bounds[0]) + domain_bounds[0]
        print(f"INFO: (AdaptiveSampler) Initialized with {total_candidates} candidate points.")

    def generate_new_collocation_points(
        self,
        kriging_model: GPUKriging,
        num_points_to_sample: int,
        cycle_number: int = 1
    ) -> tuple[np.ndarray, float]:
        """
        ä½¿ç”¨Krigingæ¨¡å‹å¼•å¯¼ç”Ÿæˆæ–°çš„é…ç½®ç‚¹ã€‚
        Args:
            kriging_model: è®­ç»ƒå¥½çš„æ®‹å·®ä»£ç†æ¨¡å‹ã€‚
            num_points_to_sample: éœ€è¦ç”Ÿæˆçš„æ€»ç‚¹æ•°ã€‚
            cycle_number: å½“å‰æ˜¯ç¬¬å‡ ä¸ªè‡ªé€‚åº”å‘¨æœŸï¼Œç”¨äºåŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥ã€‚
        Returns:
            tuple: (æ–°çš„é…ç½®ç‚¹é›†, ä½¿ç”¨çš„æ¢ç´¢ç‡)
        """
        # [æ–°é€»è¾‘] åŸºäºå‘¨æœŸæ•°å’Œå…¨å±€é…ç½®è®¡ç®—æ¢ç´¢ç‡
        exploration_ratio = max(
            FINAL_EXPLORATION_RATIO,
            INITIAL_EXPLORATION_RATIO - (cycle_number - 1) * EXPLORATION_DECAY_RATE
        )
        
        print(f"INFO: (AdaptiveSampler) å‘¨æœŸæ€§å…‹é‡Œé‡‘é‡é‡‡æ · (ç¬¬{cycle_number}æ¬¡)")
        print(f"      æ¢ç´¢ç‡: {exploration_ratio:.1%} (åˆå§‹:{INITIAL_EXPLORATION_RATIO:.1%} â†’ æœ€ç»ˆ:{FINAL_EXPLORATION_RATIO:.1%})")

        # 1. ä½¿ç”¨Krigingä»£ç†æ¨¡å‹é¢„æµ‹æ‰€æœ‰å€™é€‰ç‚¹çš„æ®‹å·®
        predicted_residuals = kriging_model.predict(self.candidate_points)
        print(f"    - Krigingé¢„æµ‹æ®‹å·®ç»Ÿè®¡ (åœ¨ {len(self.candidate_points)} ä¸ªå€™é€‰ç‚¹ä¸Š):")
        print(f"      - Max={np.max(predicted_residuals):.4e}, "
              f"Min={np.min(predicted_residuals):.4e}, "
              f"Mean={np.mean(predicted_residuals):.4e}, "
              f"Std={np.std(predicted_residuals):.4e}")

        # 2. "Hard-Case Mining": æ‰¾åˆ°é¢„æµ‹æ®‹å·®æœ€å¤§çš„ç‚¹çš„ç´¢å¼•
        num_exploitation_points = int(num_points_to_sample * (1 - exploration_ratio))
        hard_case_indices = np.argsort(predicted_residuals)[-num_exploitation_points:]
        exploitation_points = self.candidate_points[hard_case_indices]

        # 3. "Exploration": åŠ å…¥ä¸€éƒ¨åˆ†éšæœºç‚¹ä»¥é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜
        num_exploration_points = num_points_to_sample - num_exploitation_points
        random_indices = np.random.choice(len(self.candidate_points), num_exploration_points, replace=False)
        exploration_points = self.candidate_points[random_indices]

        print(f"INFO: (AdaptiveSampler) Generated {num_exploitation_points} exploitation points and {num_exploration_points} exploration points.")
        
        return np.vstack([exploitation_points, exploration_points]), exploration_ratio

def main():
    """
    ä¸»å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ª"å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”PINNè®­ç»ƒ"æµç¨‹ã€‚
    """
    # --- 1. åˆå§‹åŒ– ---
    # !! æ³¨æ„: DOMAIN_BOUNDS ç°åœ¨ä»…ç”¨äºå¯è§†åŒ–æˆ–é‡‡æ ·å™¨ï¼Œå®é™…ç‰©ç†è¾¹ç•Œç”±åŠ è½½çš„æ•°æ®å†³å®š !!
    DOMAIN_BOUNDS = np.array([[0., 0., 0.], [1., 1., 1.]]) 
    TOTAL_EPOCHS = 1000
    ADAPTIVE_CYCLE_EPOCHS = 200  # æ¯å¤šå°‘ä¸ªepochæ‰§è¡Œä¸€æ¬¡è‡ªé€‚åº”è°ƒæ•´
    DETECT_EPOCHS = 100 # æ¯100è½®æ£€æµ‹ä¸€æ¬¡æ€§èƒ½ [ä¿®æ­£æ³¨é‡Š]
    DATA_SPLIT_RATIOS = [0.7] + [0.05]*6

    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œï¼šè‡ªé€‚åº”PINNè®­ç»ƒå®éªŒ")
    print("="*60)
    print(f"ğŸ“‹ å®éªŒé…ç½®:")
    print(f"   - å…‹é‡Œé‡‘å¼•å¯¼é‡‡æ ·: {'âœ… å¯ç”¨' if ENABLE_KRIGING else 'âŒ ç¦ç”¨'}")
    if ENABLE_KRIGING:
        print(f"     â””â”€ æ¢ç´¢ç‡ç­–ç•¥: {INITIAL_EXPLORATION_RATIO:.1%} â†’ {FINAL_EXPLORATION_RATIO:.1%} (æ¯å‘¨æœŸ-{EXPLORATION_DECAY_RATE:.1%})")
    print(f"   - æ•°æ®æ³¨å…¥ç­–ç•¥: {'âœ… å¯ç”¨' if ENABLE_DATA_INJECTION else 'âŒ ç¦ç”¨'}")
    print(f"   - å¿«é€Ÿæ”¹å–„æ—©åœ: {'âœ… å¯ç”¨' if ENABLE_RAPID_IMPROVEMENT_EARLY_STOP else 'âŒ ç¦ç”¨'}")
    print(f"   - æ€»è®­ç»ƒè½®æ•°: {TOTAL_EPOCHS}")
    print(f"   - å¹²é¢„å‘¨æœŸ: æ¯ {ADAPTIVE_CYCLE_EPOCHS} è½®")
    
    # ç¡®å®šå®éªŒç±»å‹
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        exp_type = "å®Œæ•´è‡ªé€‚åº”PINN (æ•°æ®æ³¨å…¥ + å…‹é‡Œé‡‘é‡é‡‡æ ·)"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        exp_type = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·ç­–ç•¥"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        exp_type = "ä»…æ•°æ®æ³¨å…¥ç­–ç•¥"
    else:
        exp_type = "ä»…å‘¨æœŸæ€§é‡å¯ (æ— è‡ªé€‚åº”ç­–ç•¥)"
    
    print(f"   - å®éªŒç±»å‹: {exp_type}")
    print("="*60 + "\n")
    
    # --- æ•°æ®åŠ è½½å‚æ•° ---
    DATA_PATH = "PINN/DATA.xlsx"
    SPACE_DIMS = np.array([20.0, 10.0, 10.0])
    NUM_SAMPLES = 50
    
    # --- æ¨¡å‹è®­ç»ƒå‚æ•° ---
    NUM_COLLOCATION_POINTS = 4096
    NUM_RESIDUAL_SCOUT_POINTS = 5000 # ç”¨äºä¾¦å¯Ÿçš„ç‚¹æ•°ï¼Œè¿œå°‘äºè®­ç»ƒé…ç½®ç‚¹æ•°

    # 1. æ•°æ®åŠ è½½
    data_loader = DummyDataLoader(
        data_path=DATA_PATH,
        space_dims=SPACE_DIMS,
        num_samples=NUM_SAMPLES
    )
    main_train_set, reserve_data_pools, test_data, dose_data = data_loader.get_training_data(
        split_ratios=DATA_SPLIT_RATIOS,
        test_set_size=300  # [æ–°å¢] ç‹¬ç«‹æµ‹è¯•é›†å¤§å°
    )

    # 2. æ¨¡å‹å’Œé‡‡æ ·å™¨åˆå§‹åŒ–
    world_min = dose_data['world_min']
    world_max = dose_data['world_max']
    
    pinn = PINNModel(
        dose_data=dose_data, 
        training_data=main_train_set, # [æ–°] åªç”¨ä¸»è®­ç»ƒé›†åˆå§‹åŒ–
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS
    )
    kriging = GPUKriging()
    sampler = AdaptiveSampler(domain_bounds=np.vstack([world_min, world_max]))
    
    # åˆå§‹é…ç½®ç‚¹ï¼šåœ¨çœŸå®ç‰©ç†ç©ºé—´å†…é‡‡æ ·
    current_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                  (world_max - world_min) + world_min)

    # --- 3. [ä¿®æ­£] è®­ç»ƒå¾ªç¯ ---
    # ä½¿ç”¨ while å¾ªç¯æ¥ç¡®ä¿æ€»è®­ç»ƒè½®æ•°è¾¾æ ‡
    total_epochs_trained = 0
    cycle_counter = 0  # ğŸ”§ æ–°å¢ï¼šå‘¨æœŸè®¡æ•°å™¨ï¼Œç”¨äºå¯é çš„å…‹é‡Œé‡‘è§¦å‘
    
    # [æ–°å¢] é‡è¦äº‹ä»¶è®°å½•åˆ—è¡¨ï¼Œç”¨äºå›¾è¡¨æ ‡æ³¨
    important_events = []  # æ ¼å¼: [(epoch, event_type, description), ...]

    while total_epochs_trained < TOTAL_EPOCHS:
        remaining_total_epochs = TOTAL_EPOCHS - total_epochs_trained
        
        # æœ¬æ¬¡è‡ªé€‚åº”å‘¨æœŸçš„æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œä¸èƒ½è¶…è¿‡æ€»å‰©ä½™è½®æ•°
        cycle_max_epochs = min(ADAPTIVE_CYCLE_EPOCHS, remaining_total_epochs)
        
        print(f"\n--- ä¸»å¾ªç¯å‘¨æœŸ: ç›®æ ‡è®­ç»ƒ {total_epochs_trained} -> {total_epochs_trained + cycle_max_epochs} ---")

        # 2a. ä½¿ç”¨å½“å‰çš„é…ç½®ç‚¹ï¼Œå¯¹PINNè¿›è¡Œä¸€è½®å¸¸è§„è®­ç»ƒ
        print(f"PHASE 2a: å¸¸è§„PINNè®­ç»ƒ (æœ¬å‘¨æœŸä¸Šé™: {cycle_max_epochs} epochs)...")
        
        # è®°å½•è¿›å…¥æ­¤å‘¨æœŸå‰çš„è®­ç»ƒæ­¥æ•°
        epochs_before_cycle = pinn.model.train_state.step or 0
        
        # è°ƒç”¨æ”¹é€ åçš„æ–¹æ³•ï¼Œå¹¶æ¥æ”¶å…¶è¿”å›ç»“æœ
        cycle_result = pinn.run_training_cycle(
            max_epochs=cycle_max_epochs,
            detect_every=DETECT_EPOCHS,
            collocation_points=current_collocation_points,
            detection_threshold=0.1
        )
        
        # è®¡ç®—æœ¬å‘¨æœŸå®é™…è®­ç»ƒäº†å¤šå°‘è½®
        epochs_this_cycle = (pinn.model.train_state.step or 0) - epochs_before_cycle
        total_epochs_trained += epochs_this_cycle
        cycle_counter += 1  # ğŸ”§ å¢åŠ å‘¨æœŸè®¡æ•°

        print(f"\nINFO: æœ¬å‘¨æœŸå®é™…è®­ç»ƒ {epochs_this_cycle} è½®. æ€»è®­ç»ƒè¿›åº¦: {total_epochs_trained}/{TOTAL_EPOCHS}")
        print(f"ğŸ”¢ å‘¨æœŸè®¡æ•°: ç¬¬ {cycle_counter} ä¸ªå‘¨æœŸå®Œæˆ")
        
        # ğŸ” æ–°å¢ï¼šæ€§èƒ½åˆ†æ - è®°å½•å½“å‰å‘¨æœŸçš„æœ€ç»ˆMRE
        current_mre = pinn.model.train_state.metrics_test[-1] if pinn.model.train_state.metrics_test else float('inf')
        print(f"ğŸ“Š å‘¨æœŸæ€§èƒ½: ç¬¬{cycle_counter}å‘¨æœŸç»“æŸæ—¶MRE = {current_mre:.6f} (è®­ç»ƒ{epochs_this_cycle}è½®)")
        
        # ğŸ” å¦‚æœæ˜¯ç¬¬2+å‘¨æœŸï¼Œè®¡ç®—æ”¹å–„ç‡
        if cycle_counter > 1 and hasattr(main, 'previous_cycle_mre'):
            improvement = main.previous_cycle_mre - current_mre
            improvement_rate = improvement / main.previous_cycle_mre if main.previous_cycle_mre > 0 else 0
            print(f"    â””â”€ ç›¸æ¯”ä¸Šå‘¨æœŸæ”¹å–„: {improvement:.6f} ({improvement_rate:.2%})")
            
            # è¯„ä¼°æ”¶æ•›é€Ÿåº¦
            if improvement_rate > 0.1:
                print(f"    ğŸš€ å¿«é€Ÿæ”¶æ•›! æ”¹å–„ç‡ > 10%")
            elif improvement_rate > 0.05:
                print(f"    ğŸ“ˆ è‰¯å¥½æ”¶æ•›! æ”¹å–„ç‡ > 5%")
            elif improvement_rate > 0:
                print(f"    ğŸ“Š ç¼“æ…¢æ”¹å–„")
            else:
                print(f"    âš ï¸  æ€§èƒ½ä¸‹é™æˆ–åœæ»")
        
        # ä¿å­˜å½“å‰MREä¾›ä¸‹ä¸€å‘¨æœŸæ¯”è¾ƒ
        if not hasattr(main, 'previous_cycle_mre'):
            main.previous_cycle_mre = current_mre
        else:
            main.previous_cycle_mre = current_mre

        # å¦‚æœå·²ç»è®­ç»ƒå¤Ÿäº†ï¼Œå°±æå‰ç»“æŸä¸»å¾ªç¯
        if total_epochs_trained >= TOTAL_EPOCHS:
            print("\nINFO: æ€»è®­ç»ƒè½®æ•°å·²è¾¾åˆ°ç›®æ ‡ï¼Œç»“æŸè‡ªé€‚åº”è®­ç»ƒã€‚")
            break

        # 2. ğŸ”§ æ”¹ç”¨å‘¨æœŸè®¡æ•°å™¨è¿›è¡Œå¯é çš„å‘¨æœŸæ€§å¹²é¢„è§¦å‘
        should_trigger_intervention = cycle_counter > 0  # æ¯ä¸ªå‘¨æœŸéƒ½æ£€æŸ¥æ˜¯å¦éœ€è¦å¹²é¢„
        print(f"ğŸ” å¹²é¢„è§¦å‘æ£€æŸ¥: å‘¨æœŸ {cycle_counter} å®Œæˆï¼Œåº”è¯¥è§¦å‘å¹²é¢„ â†’ {should_trigger_intervention}")
        
        if should_trigger_intervention:
            print("\n" + "!"*60)
            
            # æ ¹æ®å¯ç”¨çš„ç­–ç•¥ç¡®å®šå¹²é¢„ç±»å‹æè¿°
            intervention_types = []
            if ENABLE_DATA_INJECTION:
                intervention_types.append("æ•°æ®æ³¨å…¥")
            if ENABLE_KRIGING:
                intervention_types.append("å…‹é‡Œé‡‘é‡é‡‡æ ·")
            
            if intervention_types:
                intervention_desc = " + ".join(intervention_types)
                print(f"!! è®­ç»ƒè¾¾åˆ° {total_epochs_trained} è½®ï¼Œè§¦å‘å‘¨æœŸæ€§å¹²é¢„: {intervention_desc} !!")
            else:
                print(f"!! è®­ç»ƒè¾¾åˆ° {total_epochs_trained} è½®ï¼Œè§¦å‘å‘¨æœŸæ€§é‡å¯ (æ— è‡ªé€‚åº”ç­–ç•¥) !!")
            print("!"*60)

            # --- å¹²é¢„æªæ–½ 1: æ³¨å…¥æ–°æ•°æ® (å¦‚æœå¯ç”¨ä¸”è¿˜æœ‰æ•°æ®) ---
            if ENABLE_DATA_INJECTION:
                if reserve_data_pools:
                    print("\nPHASE A: æ³¨å…¥æ–°çš„å‚¨å¤‡è®­ç»ƒæ•°æ®...")
                    data_injection_epoch = pinn.model.train_state.step or 0
                    data_to_inject = reserve_data_pools.pop(0)
                    pinn.inject_new_data(data_to_inject)
                    print("PHASE A: âœ… æ–°æ•°æ®æ³¨å…¥å®Œæˆã€‚")
                    
                    # [æ–°å¢] è®°å½•æ•°æ®æ³¨å…¥äº‹ä»¶  
                    important_events.append((
                        data_injection_epoch, 
                        'data_injection', 
                        f'å‘¨æœŸæ€§æ•°æ®æ³¨å…¥ (+{len(data_to_inject)}ç‚¹, ç¬¬{cycle_counter}æ¬¡)'
                    ))
                else:
                    print("\nWARNING: æ•°æ®æ³¨å…¥å·²å¯ç”¨ï¼Œä½†å·²æ— æ›´å¤šå‚¨å¤‡æ•°æ®å¯æ³¨å…¥ã€‚")
            else:
                print("\nPHASE A: æ•°æ®æ³¨å…¥å·²ç¦ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µã€‚")

            # --- å¹²é¢„æªæ–½ 2: å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ · (å¦‚æœå¯ç”¨) ---
            if ENABLE_KRIGING:
                print("\nPHASE B: å¼€å§‹å…‹é‡Œé‡‘å¼•å¯¼çš„è‡ªé€‚åº”é‡‡æ ·...")
                
                # æ®‹å·®"ä¾¦å¯Ÿ"
                scout_points = (np.random.rand(NUM_RESIDUAL_SCOUT_POINTS, 3) *
                                (world_max - world_min) + world_min)
                true_residuals = pinn.compute_pde_residual(scout_points)
                print(f"    - çœŸå®PDEæ®‹å·®ç»Ÿè®¡ (åœ¨ {len(scout_points)} ä¸ªä¾¦å¯Ÿç‚¹ä¸Š):")
                print(f"      - Max={np.max(true_residuals):.4e}, "
                      f"Min={np.min(true_residuals):.4e}, "
                      f"Mean={np.mean(true_residuals):.4e}, "
                      f"Std={np.std(true_residuals):.4e}")
                
                # ğŸ” æ®‹å·®è´¨é‡åˆ†æ
                high_residual_ratio = np.mean(true_residuals > np.mean(true_residuals) * 2)
                print(f"      - é«˜æ®‹å·®ç‚¹æ¯”ä¾‹: {high_residual_ratio:.1%} (æ®‹å·®>2å€å‡å€¼)")
                
                # å…‹é‡Œé‡‘ä»£ç†å»ºæ¨¡
                print("    ğŸ”§ å¼€å§‹è®­ç»ƒå…‹é‡Œé‡‘ä»£ç†æ¨¡å‹...")
                kriging.fit(scout_points, true_residuals)

                # è‡ªé€‚åº”é‡‡æ ·
                kriging_epoch = pinn.model.train_state.step or 0
                num_collocation_to_generate = pinn.data.num_domain
                print(f"INFO: Dynamically calculated {num_collocation_to_generate} collocation points to generate.")

                # ğŸ”§ ä½¿ç”¨å‘¨æœŸè®¡æ•°å™¨ä½œä¸ºå‘¨æœŸç¼–å·
                current_collocation_points, used_exploration_ratio = sampler.generate_new_collocation_points(
                    kriging_model=kriging,
                    num_points_to_sample=num_collocation_to_generate,
                    cycle_number=cycle_counter
                )
                print("PHASE B: âœ… æ–°çš„è‡ªé€‚åº”é…ç½®ç‚¹å·²ç”Ÿæˆã€‚")
                
                # ğŸ” æ–°å¢ï¼šè¯„ä¼°æ–°é…ç½®ç‚¹çš„é¢„æœŸæ®‹å·®è´¨é‡
                predicted_residuals_new = kriging.predict(current_collocation_points)
                old_residuals_sample = pinn.compute_pde_residual(current_collocation_points[:100])  # é‡‡æ ·100ä¸ªç‚¹è¯„ä¼°
                print(f"    ğŸ“Š æ–°é…ç½®ç‚¹è´¨é‡è¯„ä¼°:")
                print(f"      - å…‹é‡Œé‡‘é¢„æµ‹æ®‹å·®: Mean={np.mean(predicted_residuals_new):.4e}, Max={np.max(predicted_residuals_new):.4e}")
                print(f"      - å®é™…æ®‹å·®(é‡‡æ ·): Mean={np.mean(old_residuals_sample):.4e}, Max={np.max(old_residuals_sample):.4e}")
                residual_prediction_accuracy = np.corrcoef(
                    predicted_residuals_new[:100], old_residuals_sample
                )[0,1] if len(old_residuals_sample) == 100 else 0
                print(f"      - å…‹é‡Œé‡‘é¢„æµ‹å‡†ç¡®åº¦: {residual_prediction_accuracy:.3f} (ç›¸å…³ç³»æ•°)")
                
                # [æ–°å¢] è®°å½•å‘¨æœŸæ€§å…‹é‡Œé‡‘åº”ç”¨äº‹ä»¶
                important_events.append((
                    kriging_epoch, 
                    'kriging_resampling', 
                    f'å‘¨æœŸæ€§å…‹é‡Œé‡‘é‡é‡‡æ · (ç¬¬{cycle_counter}æ¬¡, æ¢ç´¢ç‡:{used_exploration_ratio:.1%})'
                ))
            else:
                print("\nPHASE B: å…‹é‡Œé‡‘é‡é‡‡æ ·å·²ç¦ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µã€‚")
                print("INFO: é…ç½®ç‚¹ä¿æŒä¸å˜ï¼Œä»…ä¾é æ•°æ®æ³¨å…¥ç­–ç•¥ã€‚")

            # --- å‘¨æœŸæ€§å¹²é¢„å®Œæˆ ---
            # ç¡®å®šå¹²é¢„ç±»å‹æè¿°
            if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
                intervention_desc = "æ•°æ®æ³¨å…¥ + å…‹é‡Œé‡‘é‡é‡‡æ ·"
            elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
                intervention_desc = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·"
            elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
                intervention_desc = "ä»…æ•°æ®æ³¨å…¥"
            else:
                intervention_desc = "ä»…å‘¨æœŸæ€§é‡å¯"
            
            print(f"\nINFO: ç¬¬ {cycle_counter} æ¬¡å‘¨æœŸæ€§å¹²é¢„å®Œæˆ ({intervention_desc})ã€‚")
            print("-" * 60)
        
        else:
            # ğŸ”§ è¿™ä¸ªåˆ†æ”¯ç°åœ¨åº”è¯¥ä¸ä¼šè¢«æ‰§è¡Œï¼Œå› ä¸ºæ¯ä¸ªå‘¨æœŸéƒ½ä¼šè§¦å‘å¹²é¢„æ£€æŸ¥
            print(f"\nâš ï¸  æœªé¢„æœŸçš„æƒ…å†µï¼šå‘¨æœŸ {cycle_counter} ä¸åº”è¯¥è·³è¿‡å¹²é¢„æ£€æŸ¥ï¼")
            pass

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

    # --- 4. è®­ç»ƒåŸå§‹PINNä½œä¸ºå¯¹æ¯”æ¨¡å‹ ---
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒåŸå§‹PINNä½œä¸ºå¯¹æ¯”åŸºçº¿")
    print("="*60 + "\n")

    # [ä¿®æ­£] ä¸ºåŸºçº¿æ¨¡å‹å‡†å¤‡ä¸è‡ªé€‚åº”PINNç›¸åŒçš„è®­ç»ƒæ•°æ®
    # è·å–è‡ªé€‚åº”PINNå®é™…ä½¿ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”
    adaptive_training_data = pinn.data.bcs[0].points  # è‡ªé€‚åº”PINNçš„å®é™…è®­ç»ƒç‚¹åæ ‡
    adaptive_training_values = pinn.data.bcs[0].values.cpu().numpy()  # å¯¹åº”çš„å€¼ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    
    # å°†å¯¹æ•°å°ºåº¦çš„å€¼è½¬æ¢å›çº¿æ€§å°ºåº¦ï¼Œç„¶ååˆå¹¶ä¸º [x,y,z,value] æ ¼å¼
    adaptive_training_linear_values = np.exp(adaptive_training_values)
    full_training_data = np.hstack([adaptive_training_data, adaptive_training_linear_values])
    
    print(f"INFO: åŸºçº¿PINNå°†ä½¿ç”¨ä¸è‡ªé€‚åº”PINNç›¸åŒçš„ {len(full_training_data)} ä¸ªè®­ç»ƒç‚¹")

    pinn_baseline = PINNModel(
        dose_data=dose_data, 
        training_data=full_training_data, # [æ–°] ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
        test_data=test_data,
        num_collocation_points=NUM_COLLOCATION_POINTS
    )
    # ä¸ºåŸå§‹PINNç”Ÿæˆä¸€æ¬¡æ€§çš„ã€å›ºå®šçš„é…ç½®ç‚¹
    baseline_collocation_points = (np.random.rand(NUM_COLLOCATION_POINTS, 3) * 
                                   (world_max - world_min) + world_min)

    # åŸå§‹PINNä½¿ç”¨å›ºå®šçš„è®­ç»ƒå‘¨æœŸ
    print("INFO: (Baseline PINN) Setting collocation points...")
    # [ä¿®æ­£] å°†ç”Ÿæˆçš„é…ç½®ç‚¹æ‰‹åŠ¨è®¾ç½®åˆ°æ¨¡å‹ä¸­
    num_bc_points_base = pinn_baseline.data.bcs[0].points.shape[0]
    if pinn_baseline.model.train_state.X_train is None:
        pinn_baseline.model.train(iterations=0)
    start_index_base = num_bc_points_base
    end_index_base = len(pinn_baseline.model.train_state.X_train) - len(pinn_baseline.data.anchors)
    pinn_baseline.model.train_state.X_train[start_index_base:end_index_base] = baseline_collocation_points

    print("INFO: (Baseline PINN) Starting training...")
    pinn_baseline.model.train(iterations=TOTAL_EPOCHS, display_every=5)
    
    print("\n" + "="*60)
    print("ğŸ‰ åŸå§‹PINNè®­ç»ƒå®Œæˆ!")
    print("="*60 + "\n")

    # --- 5. æœ€ç»ˆç»“æœè¯„ä¼°ä¸å¯¹æ¯” ---
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»“æœè¯„ä¼°ä¸å¯¹æ¯”")
    print("="*60 + "\n")

    test_points = test_data[:, :3]
    true_values = test_data[:, 3]

    print("æ­£åœ¨è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½...")
    adaptive_preds = pinn.predict(test_points)
    baseline_preds = pinn_baseline.predict(test_points)

    def calculate_mre(y_true, y_pred):
        return np.mean(np.abs(y_true - y_pred) / (y_true + 1e-10))

    mre_adaptive = calculate_mre(true_values, adaptive_preds)
    mre_baseline = calculate_mre(true_values, baseline_preds)
    
    # [æ–°å¢] è·å–ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒç‚¹æ•°
    adaptive_train_points = pinn.data.bcs[0].points.shape[0]
    baseline_train_points = pinn_baseline.data.bcs[0].points.shape[0]

    # åŠ¨æ€æ¨¡å‹åç§°
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        adaptive_model_name = "å®Œæ•´è‡ªé€‚åº”PINN"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        adaptive_model_name = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·PINN"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        adaptive_model_name = "ä»…æ•°æ®æ³¨å…¥PINN"
    else:
        adaptive_model_name = "å‘¨æœŸæ€§é‡å¯PINN"
    
    print(f"\n{'æ¨¡å‹':<32} | {'å¹³å‡ç›¸å¯¹è¯¯å·® (MRE)':<20} | {'è®­ç»ƒç‚¹æ•°':<12}")
    print("-" * 74)
    print(f"{adaptive_model_name:<32} | {mre_adaptive:<20.6%} | {adaptive_train_points:<12d}")
    print(f"{'åŸå§‹PINN (å›ºå®šé‡‡æ ·)':<32} | {mre_baseline:<20.6%} | {baseline_train_points:<12d}")
    print("-" * 74)
    
    # [æ–°å¢] è®­ç»ƒæ•ˆç‡å¯¹æ¯”
    print(f"\nğŸ“Š è®­ç»ƒæ•ˆç‡åˆ†æ:")
    print(f"   è‡ªé€‚åº”PINNä½¿ç”¨äº† {adaptive_train_points} ä¸ªè®­ç»ƒç‚¹ï¼Œè¾¾åˆ° MRE = {mre_adaptive:.6%}")
    print(f"   åŸºçº¿PINNä½¿ç”¨äº† {baseline_train_points} ä¸ªè®­ç»ƒç‚¹ï¼Œè¾¾åˆ° MRE = {mre_baseline:.6%}")
    if adaptive_train_points != baseline_train_points:
        efficiency_ratio = baseline_train_points / adaptive_train_points
        print(f"   è®­ç»ƒç‚¹æ•ˆç‡æ¯”: {efficiency_ratio:.2f}x (è‡ªé€‚åº”PINN vs åŸºçº¿PINN)")
    print(f"   ç‹¬ç«‹æµ‹è¯•é›†å¤§å°: {len(test_data)} ç‚¹")
    
    # ğŸ” æ–°å¢ï¼šæ”¶æ•›æ•ˆç‡åˆ†æ
    print(f"\nâš¡ æ”¶æ•›æ•ˆç‡å¯¹æ¯”:")
    if mre_adaptive < mre_baseline:
        improvement = (mre_baseline - mre_adaptive) / mre_baseline
        print(f"   ğŸ¯ è‡ªé€‚åº”PINNè¡¨ç°æ›´ä¼˜: ç›¸å¯¹æ”¹å–„ {improvement:.2%}")
        print(f"   ğŸ’¡ å…‹é‡Œé‡‘å¼•å¯¼ç­–ç•¥æœ‰æ•ˆ!")
    elif mre_adaptive > mre_baseline:
        degradation = (mre_adaptive - mre_baseline) / mre_baseline  
        print(f"   âš ï¸  è‡ªé€‚åº”PINNè¡¨ç°ç•¥å·®: ç›¸å¯¹ä¸‹é™ {degradation:.2%}")
        print(f"   ğŸ”§ å»ºè®®è°ƒæ•´æ¢ç´¢ç‡ç­–ç•¥æˆ–å¢åŠ è®­ç»ƒè½®æ•°")
    else:
        print(f"   ğŸ“Š ä¸¤ç§æ–¹æ³•æ€§èƒ½ç›¸å½“")
    
    # è®¡ç®—æ”¶æ•›é€Ÿåº¦æŒ‡æ ‡
    adaptive_epochs_to_convergence = len(pinn.mre_history)
    baseline_epochs_to_convergence = len(pinn_baseline.mre_history)
    
    print(f"\nğŸƒâ€â™‚ï¸ æ”¶æ•›é€Ÿåº¦åˆ†æ:")
    print(f"   è‡ªé€‚åº”PINN: {adaptive_epochs_to_convergence} æ¬¡è¯„ä¼°åˆ°è¾¾ MRE={mre_adaptive:.6%}")
    print(f"   åŸºçº¿PINN: {baseline_epochs_to_convergence} æ¬¡è¯„ä¼°åˆ°è¾¾ MRE={mre_baseline:.6%}")
    
    if adaptive_epochs_to_convergence < baseline_epochs_to_convergence:
        speed_improvement = (baseline_epochs_to_convergence - adaptive_epochs_to_convergence) / baseline_epochs_to_convergence
        print(f"   ğŸš€ è‡ªé€‚åº”PINNæ”¶æ•›æ›´å¿«: å‡å°‘ {speed_improvement:.1%} çš„è¯„ä¼°æ¬¡æ•°")
    else:
        print(f"   ğŸ“Š æ”¶æ•›é€Ÿåº¦ç›¸å½“æˆ–éœ€è¦æ›´å¤šè¯„ä¼°")

    # è¾“å‡ºé‡è¦äº‹ä»¶æ‘˜è¦
    if important_events:
        print(f"\nğŸ“‹ è®­ç»ƒè¿‡ç¨‹é‡è¦äº‹ä»¶æ‘˜è¦:")
        print("-" * 50)
        for epoch, event_type, description in important_events:
            event_name = "ğŸ”„ å…‹é‡Œé‡‘é‡é‡‡æ ·" if event_type == 'kriging_resampling' else "ğŸ“Š æ•°æ®æ³¨å…¥"
            print(f"  Epoch {epoch:4d}: {event_name} - {description}")
        print("-" * 50)

    # --- 6. ç»˜åˆ¶å¯¹æ¯”å›¾ ---
    print("\n" + "="*60)
    print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹MREå¯¹æ¯”å›¾")
    print("="*60 + "\n")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # æ‰“å°å°†è¦æ ‡æ³¨çš„äº‹ä»¶
    if important_events:
        print(f"INFO: å‡†å¤‡åœ¨å›¾è¡¨ä¸­æ ‡æ³¨ {len(important_events)} ä¸ªé‡è¦æ—¶é—´ç‚¹:")
        for epoch, event_type, description in important_events:
            print(f"  - Epoch {epoch}: {description}")
    
    # ç»˜åˆ¶è‡ªé€‚åº”PINNçš„MREå†å²
    if pinn.epoch_history and pinn.mre_history:
        # ä½¿ç”¨ä¸å‰é¢ä¸€è‡´çš„æ¨¡å‹åç§°
        if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
            adaptive_label = "å®Œæ•´è‡ªé€‚åº”PINN"
        elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
            adaptive_label = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·PINN"
        elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
            adaptive_label = "ä»…æ•°æ®æ³¨å…¥PINN"
        else:
            adaptive_label = "å‘¨æœŸæ€§é‡å¯PINN"
            
        ax.plot(pinn.epoch_history, pinn.mre_history, 
                label=adaptive_label, linewidth=2, alpha=0.8, color='blue')
    
    # ç»˜åˆ¶åŸºçº¿PINNçš„MREå†å²
    if pinn_baseline.epoch_history and pinn_baseline.mre_history:
        ax.plot(pinn_baseline.epoch_history, pinn_baseline.mre_history, 
                label='åŸå§‹PINN (å›ºå®šé‡‡æ ·)', linewidth=2, alpha=0.8, color='red')
    
    # æ·»åŠ é‡è¦äº‹ä»¶æ ‡æ³¨
    if important_events:
        print(f"INFO: æ ‡æ³¨ {len(important_events)} ä¸ªé‡è¦æ—¶é—´ç‚¹...")
        
        # å®šä¹‰äº‹ä»¶ç±»å‹çš„é¢œè‰²å’Œæ ·å¼
        event_styles = {
            'data_injection': {'color': 'green', 'linestyle': '--', 'alpha': 0.7},
            'kriging_resampling': {'color': 'orange', 'linestyle': '-.', 'alpha': 0.7}
        }
        
        for i, (epoch, event_type, description) in enumerate(important_events):
            style = event_styles.get(event_type, {'color': 'gray', 'linestyle': '-', 'alpha': 0.5})
            
            # ç»˜åˆ¶å‚ç›´çº¿
            ax.axvline(x=epoch, **style, linewidth=2)
            
            # è·å–å½“å‰yè½´èŒƒå›´æ¥å®šä½æ–‡æœ¬
            y_min, y_max = ax.get_ylim()
            y_pos = y_max * (0.8 - (i % 3) * 0.15)  # é”™å¼€æ ‡æ³¨ä½ç½®é¿å…é‡å 
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
            ax.annotate(
                f'{description}\n(Epoch {epoch})',
                xy=(epoch, y_pos),
                xytext=(10, 10),
                textcoords='offset points',
                bbox=dict(boxstyle='round,pad=0.3', facecolor=style['color'], alpha=0.3),
                fontsize=9,
                ha='left'
            )
        
        # æ·»åŠ å›¾ä¾‹è¯´æ˜
        from matplotlib.lines import Line2D
        legend_elements = [
            Line2D([0], [0], color='green', linestyle='--', label='æ•°æ®æ³¨å…¥'),
            Line2D([0], [0], color='orange', linestyle='-.', label='å…‹é‡Œé‡‘é‡é‡‡æ ·')
        ]
        
        # åˆ›å»ºç¬¬äºŒä¸ªå›¾ä¾‹
        second_legend = ax.legend(handles=legend_elements, loc='upper right', 
                                 fontsize=10, title='é‡è¦äº‹ä»¶', title_fontsize=11)
        ax.add_artist(second_legend)  # ä¿æŒåŸæœ‰å›¾ä¾‹
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('è®­ç»ƒè½®æ•° (Epochs)', fontsize=12)
    ax.set_ylabel('å¹³å‡ç›¸å¯¹è¯¯å·® (MRE)', fontsize=12)
    
    # åŠ¨æ€å›¾è¡¨æ ‡é¢˜
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        title_suffix = "å®Œæ•´è‡ªé€‚åº”"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        title_suffix = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        title_suffix = "ä»…æ•°æ®æ³¨å…¥"
    else:
        title_suffix = "å‘¨æœŸæ€§é‡å¯"
    
    ax.set_title(f'{title_suffix}PINN vs åŸºçº¿PINN: è®­ç»ƒè¿‡ç¨‹MREå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(loc='center right', fontsize=11)  # è°ƒæ•´åŸå›¾ä¾‹ä½ç½®
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡æ›´å¥½åœ°æ˜¾ç¤ºè¯¯å·®å˜åŒ–
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path(__file__).parent / "results"
    output_dir.mkdir(exist_ok=True)
    
    # åŠ¨æ€æ–‡ä»¶å
    if ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        file_suffix = "full_adaptive"
        config_desc = "å®Œæ•´è‡ªé€‚åº”PINN (æ•°æ®æ³¨å…¥+å…‹é‡Œé‡‘)"
    elif ENABLE_KRIGING and not ENABLE_DATA_INJECTION:
        file_suffix = "kriging_only"
        config_desc = "ä»…å…‹é‡Œé‡‘é‡é‡‡æ ·ç­–ç•¥"
    elif not ENABLE_KRIGING and ENABLE_DATA_INJECTION:
        file_suffix = "data_injection_only"
        config_desc = "ä»…æ•°æ®æ³¨å…¥ç­–ç•¥"
    else:
        file_suffix = "periodic_restart"
        config_desc = "ä»…å‘¨æœŸæ€§é‡å¯ç­–ç•¥"
    
    png_filename = f"mre_comparison_{file_suffix}.png"
    pdf_filename = f"mre_comparison_{file_suffix}.pdf"
    
    plt.savefig(output_dir / png_filename, dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / pdf_filename, bbox_inches='tight')
    
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   - PNGæ ¼å¼: {output_dir / png_filename}")
    print(f"   - PDFæ ¼å¼: {output_dir / pdf_filename}")
    print(f"   - å®éªŒé…ç½®: {config_desc}")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()

if __name__ == "__main__":
    main() 